{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hanna/openfn/ai_experiments/apollo/services\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(\"../\")\n",
    "print(parent_dir)\n",
    "sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embed_docsite.github_utils import get_docs\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:GitHubUtils:Fetched 59 URLs from GitHub for https://api.github.com/repos/OpenFn/docs/contents/adaptors\n",
      "INFO:GitHubUtils:Downloaded and processed 59 files from GitHub\n",
      "INFO:GitHubUtils:{'name': 'asana.md', 'docs': '---\\ntitle: Asana Adaptor\\n---\\n\\n## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks. \\n\\n## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enables external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [developer documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.\\n\\n## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method that is supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor\\'s [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\\n\\n```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```\\n\\n### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)\\n\\n### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)\\n\\n\\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "docs = get_docs(\"adaptor_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/hanna/openfn/ai_experiments/data/adaptor_docs.pkl', 'wb') as file:\n",
    "    pickle.dump(docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\ntitle: Asana Adaptor\\n---',\n",
       " '## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.',\n",
       " '## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enables external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [developer documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.',\n",
       " \"## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method that is supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\",\n",
       " '```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```',\n",
       " '### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)',\n",
       " '### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = clean_html(docs[0][\"docs\"])\n",
    "d = split_by_headers(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_html(text):\n",
    "    \"\"\"Remove HTML tags while preserving essential formatting.\"\"\"\n",
    "    text = re.sub(r'<\\/?p>', '\\n', text)  # Convert <p> to newlines\n",
    "    text = re.sub(r'<\\/?code>', '`', text)  # Convert <code> to backticks\n",
    "    text = re.sub(r'<\\/?strong>', '**', text)  # Convert <strong> to bold\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove other HTML tags\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def split_by_headers(text):\n",
    "    \"\"\"Split text into chunks based on Markdown headers (# and ##) and code blocks.\"\"\"\n",
    "    sections = re.split(r'(?=^#+\\s.*$|^```(?:.*\\n[\\s\\S]*?^```))', text, flags=re.MULTILINE)\n",
    "\n",
    "    return [chunk.strip() for chunk in sections if chunk.strip()]\n",
    "\n",
    "def get_overview(json_data):\n",
    "    for item in json_data:\n",
    "        if isinstance(item, dict) and \"docs\" in item and \"name\" in item:\n",
    "            \n",
    "            docs = item[\"docs\"]\n",
    "            name = item[\"name\"]\n",
    "\n",
    "            # Decode JSON string\n",
    "            try:\n",
    "                docs = json.loads(docs)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            \n",
    "            docs = clean_html(docs)\n",
    "\n",
    "            # Save all fields for adding to metadata later\n",
    "            item[\"docs\"] = docs # replace docs with cleaned text\n",
    "            metadata_dict[name] = item\n",
    "\n",
    "            # Split by headers, and where needed, sentences\n",
    "            splits = split_by_headers(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile list of adaptors with descriptions (offline)\n",
    "\n",
    "describe_adaptor_system_prompt = \"\"\"\n",
    "You are an assistant for writing brief descriptions of adaptors offered by OpenFn, a workflow generation platform.\n",
    "The summary will be used to help select appropriate adaptors for clients' desriptions of their automation tasks.\n",
    "Relevant information might therefore include e.g. the purpose of the adaptor and the data formats it uses.\n",
    "You will be given the name of an adaptor and the overview section from its documentation.\n",
    "Answer with nothing but the name of the adaptor followed by a colon and a 2-3 sentence description.\n",
    "\"\"\"\n",
    "\n",
    "describe_adaptor_user_prompt = \"\"\"The adaptor to describe: \"{adaptor_name}\" \\n The adaptor documentation: {documentation} \"\"\"\n",
    "\n",
    "def describe_adaptor(user_question):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\", \n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=describe_adaptor_system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": describe_adaptor_user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor_summaries = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor_summaries = \"\"\"\n",
    "@openfn/language-asana@latest: For interacting with Asana project management platform\n",
    "@openfn/language-azure-storage@latest: For Azure Storage operations\n",
    "@openfn/language-beyonic@latest: For mobile money payments with Beyonic\n",
    "@openfn/language-bigquery@latest: For Google BigQuery database operations\n",
    "@openfn/language-cartodb@latest: For CartoDB spatial database operations\n",
    "@openfn/language-chatgpt@latest: For interacting with ChatGPT API\n",
    "@openfn/language-cht@latest: For Community Health Toolkit integration\n",
    "@openfn/language-claude@latest: For interacting with Claude AI\n",
    "@openfn/language-collections@latest: For working with data collections\n",
    "@openfn/language-commcare@latest: For interacting with CommCare\n",
    "@openfn/language-common@latest: For basic data transformation operations\n",
    "@openfn/language-dhis2@latest: For DHIS2 health information systems\n",
    "@openfn/language-divoc@latest: For Digital Infrastructure for Vaccination Open Credentialing\n",
    "@openfn/language-dynamics@latest: For Microsoft Dynamics 365 operations\n",
    "@openfn/language-facebook@latest: For Facebook platform integration\n",
    "@openfn/language-fhir@latest: For Fast Healthcare Interoperability Resources\n",
    "@openfn/language-fhir-4@latest: For FHIR version 4 specific operations\n",
    "@openfn/language-fhir-fr@latest: For French implementation of FHIR\n",
    "@openfn/language-fhir-jembi@latest: For Jembi Health Systems FHIR implementation\n",
    "@openfn/language-fhir-ndr-et@latest: For Ethiopia National Data Repository FHIR implementation\n",
    "@openfn/language-ghana-bdr@latest: For Ghana Birth and Death Registry\n",
    "@openfn/language-ghana-nia@latest: For Ghana National Identification Authority\n",
    "@openfn/language-gmail@latest: For Gmail email operations\n",
    "@openfn/language-godata@latest: For WHO Go.Data outbreak management\n",
    "@openfn/language-googledrive@latest: For Google Drive operations\n",
    "@openfn/language-googlehealthcare@latest: For Google Healthcare API\n",
    "@openfn/language-googlesheets@latest: For Google Sheets operations\n",
    "@openfn/language-hive@latest: For Apache Hive data warehouse\n",
    "@openfn/language-http@latest: For making HTTP requests\n",
    "@openfn/language-hubtel@latest: For Hubtel messaging platform\n",
    "@openfn/language-intuit@latest: For QuickBooks and Intuit services\n",
    "@openfn/language-khanacademy@latest: For Khan Academy integration\n",
    "@openfn/language-kobotoolbox@latest: For KoboToolbox data collection\n",
    "@openfn/language-magpi@latest: For Magpi mobile data collection\n",
    "@openfn/language-mailchimp@latest: For MailChimp email marketing\n",
    "@openfn/language-mailgun@latest: For Mailgun email services\n",
    "@openfn/language-maximo@latest: For IBM Maximo asset management\n",
    "@openfn/language-medicmobile@latest: For Medic Mobile health platform\n",
    "@openfn/language-mogli@latest: For Mogli SMS Salesforce app\n",
    "@openfn/language-mojatax@latest: For digital tax platforms\n",
    "@openfn/language-mongodb@latest: For MongoDB database operations\n",
    "@openfn/language-mpesa@latest: For M-Pesa mobile payment service\n",
    "@openfn/language-msgraph@latest: For Microsoft Graph API\n",
    "@openfn/language-mssql@latest: For Microsoft SQL Server operations\n",
    "@openfn/language-msupply@latest: For mSupply inventory management\n",
    "@openfn/language-mysql@latest: For MySQL database operations\n",
    "@openfn/language-nexmo@latest: For Nexmo/Vonage communications API\n",
    "@openfn/language-ocl@latest: For Open Concept Lab terminology services\n",
    "@openfn/language-odk@latest: For Open Data Kit data collection\n",
    "@openfn/language-odoo@latest: For Odoo ERP system\n",
    "@openfn/language-openboxes@latest: For OpenBoxes supply chain management\n",
    "@openfn/language-openfn@latest: For OpenFn platform operations\n",
    "@openfn/language-openhim@latest: For OpenHIM interoperability layer\n",
    "@openfn/language-openimis@latest: For OpenIMIS insurance management\n",
    "@openfn/language-openlmis@latest: For OpenLMIS logistics management\n",
    "@openfn/language-openmrs@latest: For OpenMRS medical record system\n",
    "@openfn/language-openspp@latest: For Open Social Protection Platform\n",
    "@openfn/language-pesapal@latest: For PesaPal payment gateway\n",
    "@openfn/language-postgresql@latest: For PostgreSQL database operations\n",
    "@openfn/language-primero@latest: For Primero child protection information management\n",
    "@openfn/language-progres@latest: For UNHCR ProGres refugee management\n",
    "@openfn/language-rapidpro@latest: For RapidPro messaging platform\n",
    "@openfn/language-redis@latest: For Redis database operations\n",
    "@openfn/language-resourcemap@latest: For Resource Map geospatial platform\n",
    "@openfn/language-salesforce@latest: For Salesforce CRM operations\n",
    "@openfn/language-satusehat@latest: For Indonesia's SatuSehat health platform\n",
    "@openfn/language-senaite@latest: For SENAITE laboratory information management\n",
    "@openfn/language-sftp@latest: For secure file transfer protocol operations\n",
    "@openfn/language-smpp@latest: For Short Message Peer-to-Peer protocol\n",
    "@openfn/language-surveycto@latest: For SurveyCTO data collection\n",
    "@openfn/language-telerivet@latest: For Telerivet SMS platform\n",
    "@openfn/language-template@latest: For template operations\n",
    "@openfn/language-testing@latest: For testing OpenFn adaptors\n",
    "@openfn/language-twilio@latest: For Twilio communications API\n",
    "@openfn/language-varo@latest: For Varo financial services\n",
    "@openfn/language-vtiger@latest: For VTiger CRM operations\n",
    "@openfn/language-wigal-sms@latest: For Wigal SMS gateway\n",
    "@openfn/language-zoho@latest: For Zoho CRM and business applications\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in one prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all in one go - ask for info and gen in one prompt\n",
    "\n",
    "get_info_gen_yaml_system_prompt = \"\"\"\n",
    "You are an expert assistant for the OpenFn workflow automation platform.\n",
    "Your task is to talk to a client with the goal of converting their description of a workflow into an OpenFn workflow YAML.\n",
    "You should produce properly structured YAML files that define workflow jobs, triggers, and connections.\n",
    "This might be an iterative process, where you adjust a previous YAML according to the user's instructions.\n",
    "If necessary, you can ask the user for clarification instead of producing a YAML. You should ask for more details if it is not possible to determine what kind of data and databases/services they are using.\n",
    "Do not produce a YAML unnecessarily; if the user does not otherwise appear to want a new YAML (and is instead e.g. asking for a clarification or hit send too early),\n",
    "do not produce a YAML in your answer.\n",
    "Be as brief as possible in your answers.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Given a text description of a workflow process, you will:\n",
    "1. Identify distinct jobs/steps in the workflow\n",
    "2. Determine appropriate adaptors for each job\n",
    "3. Set up proper trigger mechanisms (webhook or cron)\n",
    "4. Create the connections (edges) between jobs\n",
    "5. Generate a valid project.yaml file that follows OpenFn's structure\n",
    "\n",
    "## OpenFn Project.yaml Structure\n",
    "\n",
    "A valid project.yaml must be enclosed in ``` and follow this structure:\n",
    "```\n",
    "name: open-project\n",
    "jobs:\n",
    "  job-one:\n",
    "    name: First Job\n",
    "    adaptor: \"@openfn/language-common@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  job-two:\n",
    "    name: Second Job\n",
    "    adaptor: \"@openfn/language-http@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "triggers:\n",
    "  # Choose one trigger type and remove the other\n",
    "  cron:  # For scheduled jobs\n",
    "    type: cron\n",
    "    cron_expression: 0 0 * * *  # Format: minute hour day month weekday\n",
    "    enabled: false\n",
    "  # OR\n",
    "  webhook:  # For event-based jobs\n",
    "    type: webhook\n",
    "    enabled: false\n",
    "edges:\n",
    "  daily-trigger->job-one:\n",
    "    source_trigger: daily-trigger\n",
    "    target_job: job-one\n",
    "    condition_type: always\n",
    "    enabled: true\n",
    "  job-one->job-two:\n",
    "    source_job: job-one\n",
    "    target_job: job-two\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "```\n",
    "\n",
    "## Adaptor Knowledge\n",
    "\n",
    "Here is a list of available OpenFn adaptors:\n",
    "{adaptor_summaries}\n",
    "\n",
    "## Trigger Types\n",
    "\n",
    "- **Webhook**: Use for event-based triggers (default if not specified)\n",
    "- **Cron**: Use for time-based schedules\n",
    "The trigger should be set to enabled: false by default.\n",
    "\n",
    "## Rules for Job Identification\n",
    "\n",
    "1. Each distinct action should become its own job\n",
    "2. Jobs should have clear, descriptive names\n",
    "3. Jobs should be connected in a logical sequence\n",
    "4. Choose the most specific adaptor available for each operation\n",
    "5. When in doubt about an adaptor, use `@openfn/language-common@latest`\n",
    "6. Job IDs should be derived from their names, replacing spaces with hyphens\n",
    "\n",
    "## Rules for Edge Creation\n",
    "\n",
    "1. The first job should always connect to the trigger\n",
    "2. Each subsequent job should connect to the previous job with one condition_type: on_job_success, on_job_failure, always or js_expression (for the latter, also add a condition_expression in quotes e.g. \"!state.error\")\n",
    "3. For branching workflows, create conditional edges as appropriate\n",
    "4. Edges should be enabled by default\n",
    "\n",
    "## Example Conversion\n",
    "\n",
    "For the input:\n",
    "\"Fetch visits from commare once a day. For each visitor with an IHS number, create a FHIR Encounter in Satusehat. Otherwise, lookup the number in satusehat and then create an encounter\"\n",
    "\n",
    "The output should be:\n",
    "Your reasoning (max ~4 sentences).\n",
    "\n",
    "```\n",
    "name: Daily CommCare to Satusehat Encounter Sync\n",
    "jobs:\n",
    "  Fetch-visits-from-CommCare:\n",
    "    name: Fetch visits from CommCare\n",
    "    adaptor: \"@openfn/language-commcare@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  Create-FHIR-Encounter-for-visitors-with-IHS-number:\n",
    "    name: Create FHIR Encounter for visitors with IHS number\n",
    "    adaptor: \"@openfn/language-satusehat@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  Lookup-IHS-number-in-Satusehat:\n",
    "    name: Lookup IHS number in Satusehat\n",
    "    adaptor: \"@openfn/language-satusehat@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  Create-FHIR-Encounter-after-IHS-lookup:\n",
    "    name: Create FHIR Encounter after IHS lookup\n",
    "    adaptor: \"@openfn/language-satusehat@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "triggers:\n",
    "  cron:\n",
    "    type: cron\n",
    "    cron_expression: 0 0 * * *\n",
    "    enabled: false\n",
    "edges:\n",
    "  cron->Fetch-visits-from-CommCare:\n",
    "    source_trigger: cron\n",
    "    target_job: Fetch-visits-from-CommCare\n",
    "    condition_type: always\n",
    "    enabled: true\n",
    "  Fetch-visits-from-CommCare->Create-FHIR-Encounter-for-visitors-with-IHS-number:\n",
    "    source_job: Fetch-visits-from-CommCare\n",
    "    target_job: Create-FHIR-Encounter-for-visitors-with-IHS-number\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "  Fetch-visits-from-CommCare->Lookup-IHS-number-in-Satusehat:\n",
    "    source_job: Fetch-visits-from-CommCare\n",
    "    target_job: Lookup-IHS-number-in-Satusehat\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "  Lookup-IHS-number-in-Satusehat->Create-FHIR-Encounter-after-IHS-lookup:\n",
    "    source_job: Lookup-IHS-number-in-Satusehat\n",
    "    target_job: Create-FHIR-Encounter-after-IHS-lookup\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "```\n",
    "\n",
    "## Output Format\n",
    "\n",
    "A) \n",
    "A conversational turn responding to the user (2-4 sentences).\n",
    "\n",
    "or\n",
    "\n",
    "B)\n",
    "In a few sentences (max. as many sentences as there are jobs in the workflow), explain your reasoning and, if relevant, aspects of the workflow that should be reviewed (e.g. to consider alternative approaches).\n",
    "After a blank line, provide the output as a proper YAML file that follows the structure above.\n",
    "\"\"\"\n",
    "\n",
    "get_info_gen_yaml_system_prompt_formatted = get_info_gen_yaml_system_prompt.format(adaptor_summaries=adaptor_summaries)\n",
    "\n",
    "get_info_gen_yaml_user_prompt = \"\"\"The user's automation task is as follows: \"{user_question}\" \"\"\"\n",
    "\n",
    "def get_info_and_gen_yaml(user_question):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=get_info_gen_yaml_system_prompt_formatted,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": get_info_gen_yaml_user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This workflow involves receiving fridge statistics via webhook, processing the data, and storing it in Redis. I\\'ll use language-http for receiving data, language-common for processing, and language-redis for storage.\\n\\n```\\nname: Fridge-Statistics-Processing\\njobs:\\n  receive-fridge-statistics:\\n    name: Receive Fridge Statistics\\n    adaptor: \"@openfn/language-http@latest\"\\n    body: \"| // Parse incoming webhook data\"\\n  process-and-aggregate:\\n    name: Process and Aggregate Data\\n    adaptor: \"@openfn/language-common@latest\"\\n    body: \"| // Aggregate and transform the fridge statistics\"\\n  upload-to-redis:\\n    name: Upload to Redis Collection\\n    adaptor: \"@openfn/language-redis@latest\"\\n    body: \"| // Store the processed data in Redis collection\"\\ntriggers:\\n  webhook:\\n    type: webhook\\n    enabled: false\\nedges:\\n  webhook->receive-fridge-statistics:\\n    source_trigger: webhook\\n    target_job: receive-fridge-statistics\\n    condition_type: always\\n    enabled: true\\n  receive-fridge-statistics->process-and-aggregate:\\n    source_job: receive-fridge-statistics\\n    target_job: process-and-aggregate\\n    condition_type: on_job_success\\n    enabled: true\\n  process-and-aggregate->upload-to-redis:\\n    source_job: process-and-aggregate\\n    target_job: upload-to-redis\\n    condition_type: on_job_success\\n    enabled: true\\n```'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Whenever fridge statistics are send to you, parse and aggregate the data and upload to a collection in redis.\"\n",
    "# user_question = \"i have fridge stats that should be analysed and then uploaded\"\n",
    "\n",
    "answer = get_info_and_gen_yaml(user_question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_and_yaml(s):\n",
    "#     if \"```\" in s:\n",
    "#         parts = s.split(\"```\")\n",
    "#         text = parts[0].strip()\n",
    "#         yaml = parts[1].strip()\n",
    "#         return text, yaml\n",
    "#     return s.strip(), None\n",
    "\n",
    "def split_format_yaml(response):\n",
    "    \"\"\"Split text and YAML in response and format the YAML.\"\"\"\n",
    "    try:\n",
    "        if \"```\" in response:\n",
    "            parts = response.split(\"```\")\n",
    "            output_text = parts[0].strip()\n",
    "            output_yaml = parts[1].strip()\n",
    "            # Decode the escaped newlines into actual newlines\n",
    "            output_yaml = output_yaml.encode().decode('unicode_escape')\n",
    "            output_yaml = yaml.safe_load(output_yaml)\n",
    "            # Convert back to YAML string\n",
    "            output_yaml = yaml.dump(output_yaml, sort_keys=False)\n",
    "            return output_text, output_yaml\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return response.strip(), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This workflow involves receiving fridge statistics via webhook, processing the data, and storing it in Redis. I'll use language-http for receiving data, language-common for processing, and language-redis for storage.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'name: Fridge-Statistics-Processing\\njobs:\\n  receive-fridge-statistics:\\n    name: Receive Fridge Statistics\\n    adaptor: \"@openfn/language-http@latest\"\\n    body: \"| // Parse incoming webhook data\"\\n  process-and-aggregate:\\n    name: Process and Aggregate Data\\n    adaptor: \"@openfn/language-common@latest\"\\n    body: \"| // Aggregate and transform the fridge statistics\"\\n  upload-to-redis:\\n    name: Upload to Redis Collection\\n    adaptor: \"@openfn/language-redis@latest\"\\n    body: \"| // Store the processed data in Redis collection\"\\ntriggers:\\n  webhook:\\n    type: webhook\\n    enabled: false\\nedges:\\n  webhook->receive-fridge-statistics:\\n    source_trigger: webhook\\n    target_job: receive-fridge-statistics\\n    condition_type: always\\n    enabled: true\\n  receive-fridge-statistics->process-and-aggregate:\\n    source_job: receive-fridge-statistics\\n    target_job: process-and-aggregate\\n    condition_type: on_job_success\\n    enabled: true\\n  process-and-aggregate->upload-to-redis:\\n    source_job: process-and-aggregate\\n    target_job: upload-to-redis\\n    condition_type: on_job_success\\n    enabled: true'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text, output_yaml = extract_text_and_yaml(answer)\n",
    "print(output_text)\n",
    "output_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Fridge-Statistics-Processing\n",
      "jobs:\n",
      "  receive-fridge-statistics:\n",
      "    name: Receive Fridge Statistics\n",
      "    adaptor: '@openfn/language-http@latest'\n",
      "    body: '| // Parse incoming webhook data'\n",
      "  process-and-aggregate:\n",
      "    name: Process and Aggregate Data\n",
      "    adaptor: '@openfn/language-common@latest'\n",
      "    body: '| // Aggregate and transform the fridge statistics'\n",
      "  upload-to-redis:\n",
      "    name: Upload to Redis Collection\n",
      "    adaptor: '@openfn/language-redis@latest'\n",
      "    body: '| // Store the processed data in Redis collection'\n",
      "triggers:\n",
      "  webhook:\n",
      "    type: webhook\n",
      "    enabled: false\n",
      "edges:\n",
      "  webhook->receive-fridge-statistics:\n",
      "    source_trigger: webhook\n",
      "    target_job: receive-fridge-statistics\n",
      "    condition_type: always\n",
      "    enabled: true\n",
      "  receive-fridge-statistics->process-and-aggregate:\n",
      "    source_job: receive-fridge-statistics\n",
      "    target_job: process-and-aggregate\n",
      "    condition_type: on_job_success\n",
      "    enabled: true\n",
      "  process-and-aggregate->upload-to-redis:\n",
      "    source_job: process-and-aggregate\n",
      "    target_job: upload-to-redis\n",
      "    condition_type: on_job_success\n",
      "    enabled: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the escaped newlines into actual newlines\n",
    "formatted_str = output_yaml.encode().decode('unicode_escape')\n",
    "\n",
    "try:\n",
    "    yaml_data = yaml.safe_load(formatted_str)\n",
    "    # Convert back to pretty YAML string\n",
    "    pretty_yaml = yaml.dump(yaml_data, sort_keys=False)\n",
    "except yaml.YAMLError as e:\n",
    "    raise ValueError(\"Invalid YAML: \", e)\n",
    "print(pretty_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'i have fridge stats that should be analysed and then uploaded'\n",
    "--> 'I need more details to create an effective workflow. What system are the fridge stats coming from? What kind of analysis needs to be performed? Where should the results be uploaded to? With this information, I can design an appropriate workflow with the right adaptors and connections.'\n",
    "\n",
    "'Whenever fridge statistics are send to you, parse and aggregate the data and upload to a collection in redis.'\n",
    "--> 'This workflow involves receiving fridge statistics via webhook, processing the data, and storing it in Redis. I\\'ll create a two-job workflow with appropriate adaptors.\\n\\n```yaml\\nname: Fridge-Statistics-Processing\\njobs:\\n  parse-and-aggregate-fridge-data:\\n    name: Parse and Aggregate Fridge Data\\n    adaptor: \"@openfn/language-common@latest\"\\n    body: \"| // Add operations to parse and aggregate the fridge statistics\"\\n  upload-to-redis-collection:\\n    name: Upload to Redis Collection\\n    adaptor: \"@openfn/language-redis@latest\"\\n    body: \"| // Add operations to store the processed data in Redis\"\\ntriggers:\\n  webhook:\\n    type: webhook\\n    enabled: false\\nedges:\\n  webhook->parse-and-aggregate-fridge-data:\\n    source_trigger: webhook\\n    target_job: parse-and-aggregate-fridge-data\\n    condition_type: always\\n    enabled: true\\n  parse-and-aggregate-fridge-data->upload-to-redis-collection:\\n    source_job: parse-and-aggregate-fridge-data\\n    target_job: upload-to-redis-collection\\n    condition_type: on_job_success\\n    enabled: true\\n```'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate convo + yaml calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have two slightly different system prompts for first time generation (explain your reasonign in 2 sentences) and subsequent (explain what you changed and why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_yaml_system_prompt = \"\"\"\n",
    "You are an expert assistant for the OpenFn workflow automation platform. Your task is to convert natural language descriptions of workflows into properly structured YAML files that define workflow jobs, triggers, and connections.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Given a text description of a workflow process, you will:\n",
    "1. Identify distinct jobs/steps in the workflow\n",
    "2. Determine appropriate adaptors for each job\n",
    "3. Set up proper trigger mechanisms (webhook or cron)\n",
    "4. Create the connections (edges) between jobs\n",
    "5. Generate a valid project.yaml file that follows OpenFn's structure\n",
    "\n",
    "## OpenFn Project.yaml Structure\n",
    "\n",
    "A valid project.yaml must follow this structure:\n",
    "```yaml\n",
    "name: open-project\n",
    "jobs:\n",
    "  job-one:\n",
    "    name: First Job\n",
    "    adaptor: \"@openfn/language-common@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  job-two:\n",
    "    name: Second Job\n",
    "    adaptor: \"@openfn/language-http@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "triggers:\n",
    "  # Choose one trigger type and remove the other\n",
    "  cron:  # For scheduled jobs\n",
    "    type: cron\n",
    "    cron_expression: 0 0 * * *  # Format: minute hour day month weekday\n",
    "    enabled: false\n",
    "  # OR\n",
    "  webhook:  # For event-based jobs\n",
    "    type: webhook\n",
    "    enabled: false\n",
    "edges:\n",
    "  daily-trigger->job-one:\n",
    "    source_trigger: daily-trigger\n",
    "    target_job: job-one\n",
    "    condition_type: always\n",
    "    enabled: true\n",
    "  job-one->job-two:\n",
    "    source_job: job-one\n",
    "    target_job: job-two\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "```\n",
    "\n",
    "## Adaptor Knowledge\n",
    "\n",
    "Here is a list of available OpenFn adaptors:\n",
    "{adaptor_summaries}\n",
    "\n",
    "## Trigger Types\n",
    "\n",
    "- **Webhook**: Use for event-based triggers (default if not specified)\n",
    "- **Cron**: Use for time-based schedules\n",
    "The trigger should be set to enabled: false by default.\n",
    "\n",
    "## Rules for Job Identification\n",
    "\n",
    "1. Each distinct action should become its own job\n",
    "2. Jobs should have clear, descriptive names\n",
    "3. Jobs should be connected in a logical sequence\n",
    "4. Choose the most specific adaptor available for each operation\n",
    "5. When in doubt about an adaptor, use `@openfn/language-common@latest`\n",
    "6. Job IDs should be derived from their names, replacing spaces with hyphens\n",
    "\n",
    "## Rules for Edge Creation\n",
    "\n",
    "1. The first job should always connect to the trigger\n",
    "2. Each subsequent job should connect to the previous job with one condition_type: on_job_success, on_job_failure, always or js_expression (for the latter, also add a condition_expression in quotes e.g. \"!state.error\")\n",
    "3. For branching workflows, create conditional edges as appropriate\n",
    "4. Edges should be enabled by default\n",
    "\n",
    "## Example Conversion\n",
    "\n",
    "For the input:\n",
    "\"Fetch visits from commare once a day. For each visitor with an IHS number, create a FHIR Encounter in Satusehat. Otherwise, lookup the number in satusehat and then create an encounter\"\n",
    "\n",
    "The output should be:\n",
    "Your reasoning (2-5 sentences).\n",
    "\n",
    "```yaml\n",
    "name: Daily CommCare to Satusehat Encounter Sync\n",
    "jobs:\n",
    "  Fetch-visits-from-CommCare:\n",
    "    name: Fetch visits from CommCare\n",
    "    adaptor: \"@openfn/language-commcare@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  Create-FHIR-Encounter-for-visitors-with-IHS-number:\n",
    "    name: Create FHIR Encounter for visitors with IHS number\n",
    "    adaptor: \"@openfn/language-satusehat@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  Lookup-IHS-number-in-Satusehat:\n",
    "    name: Lookup IHS number in Satusehat\n",
    "    adaptor: \"@openfn/language-satusehat@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "  Create-FHIR-Encounter-after-IHS-lookup:\n",
    "    name: Create FHIR Encounter after IHS lookup\n",
    "    adaptor: \"@openfn/language-satusehat@latest\"\n",
    "    body: \"| // Add operations here\"\n",
    "triggers:\n",
    "  cron:\n",
    "    type: cron\n",
    "    cron_expression: 0 0 * * *\n",
    "    enabled: false\n",
    "edges:\n",
    "  cron->Fetch-visits-from-CommCare:\n",
    "    source_trigger: cron\n",
    "    target_job: Fetch-visits-from-CommCare\n",
    "    condition_type: always\n",
    "    enabled: true\n",
    "  Fetch-visits-from-CommCare->Create-FHIR-Encounter-for-visitors-with-IHS-number:\n",
    "    source_job: Fetch-visits-from-CommCare\n",
    "    target_job: Create-FHIR-Encounter-for-visitors-with-IHS-number\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "  Fetch-visits-from-CommCare->Lookup-IHS-number-in-Satusehat:\n",
    "    source_job: Fetch-visits-from-CommCare\n",
    "    target_job: Lookup-IHS-number-in-Satusehat\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "  Lookup-IHS-number-in-Satusehat->Create-FHIR-Encounter-after-IHS-lookup:\n",
    "    source_job: Lookup-IHS-number-in-Satusehat\n",
    "    target_job: Create-FHIR-Encounter-after-IHS-lookup\n",
    "    condition_type: on_job_success\n",
    "    enabled: true\n",
    "```\n",
    "\n",
    "## Output Format\n",
    "\n",
    "In 2-5 sentences, explain your reasoning and, if relevant, aspects of the workflow that should be reviewed (e.g. to consider alternative approaches).\n",
    "After a blank line, provide the output as a proper YAML file that follows the structure above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_yaml_system_prompt_formatted = gen_yaml_system_prompt.format(adaptor_summaries=adaptor_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_yaml_user_prompt = \"\"\"The user's automation task is as follows: \"{user_question}\" \"\"\"\n",
    "\n",
    "def generate_yaml(user_question):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=gen_yaml_system_prompt_formatted,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": generate_yaml_user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'ll create a workflow that handles incoming fridge statistics via a webhook trigger, processes the data, and stores it in Redis. This is a straightforward two-step process: first receiving and processing the data, then storing it in Redis. I\\'m using a webhook trigger since the description mentions \"whenever\" data is sent, suggesting an event-based rather than scheduled approach.\\n\\n```yaml\\nname: Fridge-Statistics-Processing\\njobs:\\n  parse-and-aggregate-fridge-data:\\n    name: Parse and Aggregate Fridge Data\\n    adaptor: \"@openfn/language-common@latest\"\\n    body: \"| // Add data parsing and aggregation operations here\"\\n  upload-to-redis-collection:\\n    name: Upload to Redis Collection\\n    adaptor: \"@openfn/language-redis@latest\"\\n    body: \"| // Add Redis collection upload operations here\"\\ntriggers:\\n  webhook:\\n    type: webhook\\n    enabled: false\\nedges:\\n  webhook->parse-and-aggregate-fridge-data:\\n    source_trigger: webhook\\n    target_job: parse-and-aggregate-fridge-data\\n    condition_type: always\\n    enabled: true\\n  parse-and-aggregate-fridge-data->upload-to-redis-collection:\\n    source_job: parse-and-aggregate-fridge-data\\n    target_job: upload-to-redis-collection\\n    condition_type: on_job_success\\n    enabled: true\\n```'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"Whenever fridge statistics are send to you, parse and aggregate the data and upload to a collection in redis.\"\n",
    "\n",
    "answer = generate_yaml(q)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decide_gen_user_prompt = \"\"\"The user's automation task is as follows: \"{user_question}\" \"\"\"\n",
    "\n",
    "def generate_yaml(user_question):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=gen_yaml_system_prompt_formatted,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": generate_yaml_user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
