{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hanna/openfn/ai_experiments/apollo/services\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(\"../\")\n",
    "print(parent_dir)\n",
    "sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanna/opt/anaconda3/envs/openfn/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from search_docsite import DocsiteSearch\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "from embed_docsite import github_utils\n",
    "\n",
    "load_dotenv()\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/hanna/opt/anaconda3/envs/openfn/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(text='---\\ntitle: Asana Adaptor\\n---## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\", metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='Then, we load\\n(send it to the destination).## Integration platform\\n\\nAn integration platform (e.g., OpenFn) is an application (or set of\\napplications) that help organizations set up, run, and maintain/manage the\\nintegrations between all of their various systems.### iPaaS\\n\\nYou may also see the acronym \"iPaaS\". This stands for integration platform as a\\nservice and is a type of \"software as a service\" (or \"SaaS\"). SaaS is a software\\npurchasing model in which software is paid for only as it is used (often\\nmonth-to-month), rather than purchased up front or given away for free.## Metadata\\n\\nThis is data that tells us about our data. In a table, for example, that\\'s the\\nname of the columns, the number of rows, etc. Metadata is often brought up in\\nconversations about privacy—e.g., regulators might want to ensure that _only\\nmetadata_ is moved from Ministry A to Ministry B, as opposed to personally\\nidentifiable information (PII) about individuals themselves.', metadata={'doc_title': 'glossary', 'docs_type': 'general_docs'}, score=None),\n",
       " SearchResult(text='Otherwise, read on!## Project\\n\\nA Project is an administrative grouping in OpenFn like a \"workspace\".\\n\\nOn the platform (OpenFn/lightning), Projects define who can access your OpenFn\\nworkflow configuration & history. Projects have an owner and one or more\\nCollaborators.\\n\\nIn local deployment and development, Project also corresponds to a\\n[`project.yaml`](/documentation/deploy/portability-versions#v2) file, which\\ndefines a project\\' configuration.\\n\\nIn either case, a Project contains Workflows, Triggers, Credentials, and\\neverything you need to automate and integrate with OpenFn.## Workflow\\n\\n:::tip\\n\\nWorkflows are the **\"what to do\"** part of automation!\\n\\n:::\\n\\nA Workflow is a collection of a Trigger, Steps, Paths, and custom logic\\nconnected together to automate a specific business process or task. A Workflow\\nis configured via the Canvas in the web app, or locally (via code).\\n\\nOpenFn automation centers around [Workflows](/documentation/build/workflows),\\nwhich may have one or multiple Steps. Workflows can be run in real-time (based\\non an event -e.g., new patient registration), on a scheduled basis (e.g., every\\nday at 8am), or manually on-demand.\\n\\nThink of workflow as a set of instructions you might give a staff member (e.g.,\\nplease create a new Patient record in OpenMRS when a form containing a newly\\nregistered client is received from CommCare, export data to DHIS2 every week on\\nFriday 11pm, send SMS with payment confirmation number when payment confirmation\\nmessage is received etc.).\\n\\nCommon Workflows automate:\\n', metadata={'doc_title': 'terminology', 'docs_type': 'general_docs'}, score=None),\n",
       " SearchResult(text='We are committed to constantly evaluating\\nwhat our user base needs most and spending the few resources we have to deliver\\nvalue to them—we simply can\\'t guarantee that what sounds like the \"7th most\\nimportant feature to build\" now will still be on our list in 6 months.At the same time, we strive to be as transparent and inclusive as we can in our\\nplanning processes. We have a big backlog of feature requests and GitHub issues\\n(bug reports, stubs, even partially shaped epics or projects) that are getting\\nvoted up, commented on, and used as inspiration when we\\'re deciding what to\\nprioritize next.\\n\\nRead on to learn more about how we work, how you can see what\\'s coming, and how\\nyou can get involved!\\n\\n:::## What are we currently working on?\\n\\nAll of our team\\'s work is tracked publicly using a GitHub Project. Three key\\nviews give you up-to-the-minute insights on what we\\'re doing and what\\'s on our\\nimmediate roadmap.', metadata={'doc_title': 'openfn-roadmap', 'docs_type': 'general_docs'}, score=None),\n",
       " SearchResult(text='If you capture these 3 elements, user stories can b ean effective way of\\ndetailing integration requirements and starting discussions at your organization\\nabout which requirements are priority.### Example user stories:\\n\\n- **Case Referrals:** As a caseworker, I want to automatically send referral\\n  requests to my partner agency using another case management system, so that I\\n  can securely share case information and quickly notify them when their\\n  services are needed in a crisis situation.\\n- **EMR - HIS:** As a clinic manager, I would like to integrate patient data\\n  from the district clinic electronic medical record system with the national\\n  DHIS2 health information system, so that I can securely and automatically\\n  report on health outcomes for key indicators in my district.\\n- **Kobo Toolbox - MSSQL Database:** As a M&E manager, I want to monitor Kobo\\n  Toolbox survey responses in a central database in real-time, so that I can\\n  better understand data collection activities and program performance across my\\n  research partner sites.', metadata={'doc_title': 'overview', 'docs_type': 'general_docs'}, score=None),\n",
       " SearchResult(text=\"Learn more about how data is structured in Salesforce at the links below:\\n\\n- How data is organized in Salesforce:\\n  https://help.salesforce.com/s/articleView?id=sf.basics_organize_data.htm&type=5\\n- Salesforce objects: https://www.salesforcetutorial.com/salesforce-objects/## Integration Use Cases\\n\\nSalesforce may be used to manage an organization's programs, operations,\\nfundraising, and more. Therefore integrations with 3rd-party apps is a common\\nrequirement.\\n\\nExample user stories:\\n\\n- As a program or M&E manager working in areas with limited internet\\n  connectivity, I would like to collect data on a mobile applicaiton that\\n  supports offline data capture, but then sync the data collected to Salesforce\\n  so that I can centrally monitor field activities and analyze data collected to\\n  evaluate program impact.\\n- As a fundraiser at an NGO, I would like to see all donor information tracked\\n  in Salesforce so that I can better monitor fundraising activities, nurture\\n  relationships with all funders, and manage campaigns.\", metadata={'doc_title': 'salesforce', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='This versatile platform caters\\nto a diverse array of needs, serving researchers, NGOs, government agencies, and\\nother entities seeking reliable data collection solutions.## Integration Use Cases:\\n\\nAs SurveyCTO is primarily a mobile data collection tool, often that data needs\\nto be extracted, managed, summarized, and analysed in another system (e.g.,\\ndatabase/data warehouse, analytics tools, and \"MIS\"/data management software\\nlike like DHIS2, Salesforce, etc.).\\n\\nExample user story:\\n\\n- As a program or M&E manager, I would like my field officers to collect data in\\n  places with limited connection availability using SurveyCTO. I want the data\\n  collected to automatically sync to my Tabluea dashboard for visualization and\\n  further analysis.', metadata={'doc_title': 'surveycto', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=':::## About the Assistant\\n\\nThe AI Assistant is built on the Claude 3.5 Sonnet model from\\n[Anthropic](https://www.anthropic.com/) and is trained with OpenFn documentation\\nand example job code.\\n\\nAt the moment, the Assistant is only configured to help with job writing. Later\\nversions of the assistant may be rolled out to other pages.\\n\\nAll chat sessions are shared between all users of the project. You can start a\\nnew chat session at any time, or open an old one.\\n\\nInput data and run-time job logs not sent to the model. But we do include\\nyour step code so that the Assistant can provide a contextually relevant answer.## A Note on Responsible AI Usage\\n\\nThe AI assistant is built on emerging Large Language Model (terminology). Like\\nother LLMS and chatbots, its capabilities are impressive, but imperfect.\\n\\nRemember that ultimately, all responses are generated by an algorithm and YOU,\\nthe human in charge, are responsible for how its output is used. You should\\nconsider all responses critically and verify the output where possible.\\n\\nYou can read more about our approach to AI in our\\n[Responsible AI Policy](https://www.openfn.org/ai).', metadata={'doc_title': 'ai-assistant', 'docs_type': 'general_docs'}, score=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsite_search = DocsiteSearch(collection_name=\"docsite-20250225\")\n",
    "results = docsite_search.search(\"what's asana\", top_k=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_questions = [\n",
    "    \"How do I modify a contact on Salesforce\"\n",
    "    'Write a job for me that creates new DataValueSets under the \"Approved School CHP\" organization unit.',\n",
    "    \"How to get data from Asana in my job?\",\n",
    "    \"What's the difference between a Trigger and a Step in a workflow?\",\n",
    "    \"Can I write a step that allows for creation of a JSON file of the transformed records that can be sent to an email or Google Drive?\",\n",
    "    \"Why does this part of the code only POST the first submission in the submissions[] array? I want to POST each submission. each( '$.submissions[*]', post(URL, { body: () => {} ))\",\n",
    "    \"Write a job to fetch for an attribute in the input and map it.\",\n",
    "    \"Can I set up a workflow to trigger based on changes in a database using a webhook? How?\",  \n",
    "    \"How can I securely manage API keys and other sensitive information using environment variables in OpenFn?\",  \n",
    "    \"Does OpenFn offer any built-in support for encrypting sensitive data in transit or at rest?\",  \n",
    "    \"How can I customize the logging level and output format for my workflows?\",  \n",
    "    \"What's the recommended workflow for using Git to manage and deploy changes to my OpenFn workflows?\",  \n",
    "    \"How can I write unit tests for my OpenFn jobs to ensure they are working correctly?\",  \n",
    "    \"How can I set up alerts to notify me when a workflow fails or exceeds a certain execution time?\",\n",
    "    \"How can I use the post() function with the HTTP adapter to send transformed data to another system?\",\n",
    "    \"What's the syntax for using the create() function with the DHIS2 adapter to create new data elements?\",\n",
    "    \"How do I use the alterState() function to modify the state object between operations?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/hanna/opt/anaconda3/envs/openfn/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 20\n",
      "INFO:DocsiteSearch:Filtered to 20 results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(text='ng data from CRM systems into Google Sheets for sales\\nanalysis. - Updating inventory levels in a Google Sheets spreadsheet from an\\ne-commerce platform. - Automatically populating project management data from\\ntask tracking systems into Google Sheets for reporting purposes.\\n\\nA step by step guide is found\\n[in this tutorial](https://docs.openfn.org/documentation/tutorials/http-to-googlesheets)\\nthat shows us how to get data via a REST API and push it to Google Sheet.### Helpful Resources\\n\\n- [Google Sheets API Overview](https://developers.google.com/sheets/api/guides/concepts)\\n- OpenFn Workflow Tutorial:\\n  [HTTP-to-GoogleSheets](https://docs.openfn.org/documentation/tutorials/http-to-googlesheets)', metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=0.79843092),\n",
       " SearchResult(text='- You have access to an OpenFn project (either on a locally installed\\n  [OpenFn v2 app](https://github.com/OpenFn/lightning) or on\\n  [app.openfn.org](https://app.openfn.org)).## Getting started\\n\\nIn this walkthrough, we will configure a Workflow to **automatically sync `user`\\ndata from a web REST API, and import this data to a GoogleSheet**.\\n\\n**This integration can be broken up into two parts:**\\n\\n1. Get data from the REST API (the \"source\" app)\\n2. Transforming & importing this data to a table in your GoogleSheet (the\\n   \"destination\" app)\\n\\nLet’s get started!## 1: Create a new Workflow\\n\\nTo create a new Workflow in your Project:\\n\\n1. Go to the `project dashboard` page.\\n2. Click `Create new workflow` button.\\n3. Give your Workflow a descriptive `Name` (e.g., `Sync Users List`).\\n4. Choose your [Trigger](../build/triggers.md)\\n5. Edit your first [Step](../build/steps/steps.md)', metadata={'doc_title': 'http-to-googlesheets', 'docs_type': 'general_docs'}, score=0.788091719),\n",
       " SearchResult(text=\"Then, when building your workflow to implement this globals mapping table\\nspecification, your job expression might looks something like the below code\\nsnippet.```js\\n//Workflow step 1\\n//First we use fn() to transform, map & clean our data\\nfn(state => {\\n\\n    //Global mapping rules you want to implement in your workflow\\n    const locationMap = {\\n        //location_id from source app: location value in destination app\\n        01: 'Western Cape',\\n        02: 'Eastern Cape',\\n        03: 'Gauteng'\\n    }\\n\\n    // Here we build the payload of our http request body...\\n    // We assume the input is an array of records\\n    const payload = state.data.map(record => ({\\n        location: locationMap[record.location_id] //translate location_id to the mapped value\\n        external_id: record.case_id\\n    }));\\n\\n    return {...state, payload};\\n});\\n\\n//Workflow step 2\\n//Then we post the payload built in the prior operation to create a record\\npost('/api/myEndpoint', {\\n  headers: {\\n    'Content-Type': 'application/json',\\n  },\\n  body: (state) => state.payload\\n});\\n```\", metadata={'doc_title': 'mapping-specs', 'docs_type': 'general_docs'}, score=0.786613047),\n",
       " SearchResult(text=\":::**Once you are finished configuring and writing your Step, save and run it!**\\n\\n- See the [Workflows section](../build/workflows.md) for more guidance on\\n  building & running Workflows.\\n\\n**Check out the `Output & Log` panel to see if your run succeeded.** If it\\nsucceeded, you should see:\\n\\n- Status `success`\\n- Log tab end with `Run complete with status: success`\\n- Input tab has `{}`\\n- Output tab has `{ data: [ {...}]}`## 3. Configure another Step to transform the data & import your GoogleSheet\\n\\nCreate a new Googlesheet `Credential` using your Google account's email. (Make\\nsure this Google user has edit access to the GoogleSheet you want to integrate\\nwith.)\\n\\nFor this demo, we have configured the Googlesheet\\n[like this](https://docs.google.com/spreadsheets/d/1gT4cpHSDQp8A_JIX_5lqTLTwV0xBo_u8u3ZNWALmCLc/edit?usp=sharing)\\nto store the `users` data.\\n\\nCreate a new step with the `googlesheet` adaptor for loading the users data into\\nyour destination GoogleSheet. Configure the step with the following options\\n\\n- Name `Sync Users`\\n- Adaptor `googlesheets`\\n- Version: `2.2.2`\\n- Credentials: Create new `GoogleSheet OAuth` Credential and save it\\n- Step operations: For this job we will use the `appendValues()` operation to\\n  add an array of rows to the spreadsheet. Make sure you update the\\n  `spreadsheetId` to match the Id of your spreadsheet\\n\\n  ```js\\n  // Prepare array of users data\\n  fn(state => {\", metadata={'doc_title': 'http-to-googlesheets', 'docs_type': 'general_docs'}, score=0.785333931),\n",
       " SearchResult(text=\"Click the code button `` displayed on the configuration panel\\n\\nFor more on how to write custom business logic and data transformation rules in\\nthe `Editor`, see the docs on\\n[writing Jobs](/documentation/jobs/job-writing-guide) and check out the below\\nvideo.## Run & Test Steps\\n\\nWhen running Steps to test the configuration, every Run will have an initial\\nstate (which may contain an `Input`) and results in a final state that will\\ninclude `Logs` and an `Output`.\\n\\n- `Input` - Data (JSON) that is used as the starting Input for a Step to utilise\\n  in its run. An Input can exist for a Work Order and individual Steps within a\\n  Run, though it is possible for either to exist without an Input.\\n- `Output` - Data (JSON) that is created as the Output of a Step's execution. An\\n  Output can exist for a Work Order and individual jobs within a run, and\\n  typically contains the data sent to the target app.\\n- `Logs` - A record generated by the workflow execution engine that details the\\n  activities performed when running a Workflow or individual Step.\\n\\nSee [Writing Jobs docs](/documentation/jobs/job-writing-guide) for more on\\nwriting custom logic, and see [this article](/documentation/jobs/state) for more\", metadata={'doc_title': 'step-editor', 'docs_type': 'general_docs'}, score=0.782249093),\n",
       " SearchResult(text='Deciding on which option to go with depends on your business requirements.### Set up a workflow using Option 1\\n\\n1. **Open up an existing project and create a new workflow**\\n\\n![create_new_workflow](/img/create-new-workflow.gif)\\n\\n2. **Create a new “Webhook” trigger to schedule this extract job.**\\n\\n![create_trigger](/img/create_trigger.gif)\\n\\nMake sure you have copied the webhook URL from your OpenFn workflow into CommCare. Each form submitted in CommCare will be automatically sent to OpenFn and will trigger your new workflow.## Transforming and loading CommCare data to a PostgreSQL database\\n\\n1. **You should have a database configured and a username provided for OpenFn to\\n   read and write data in your target DB tables.** For this demo, we have\\n   configured the database\\n   [like this](https://docs.google.com/spreadsheets/d/1pi_oxImakhtaCCCIENkjTPZeuyWhpFEcNmH7hfvTBgo/edit?usp=sharing)\\n   to capture the CommCare form data. Check out the\\n   [this page](../design/mapping-specs)\\n   for how to create your own `mapping specification document` to map data\\n   elements to be exchanged.\\n\\n![db_config](/img/db_config.png)\\n\\n\\n\\n2. **Create a new step with the `postgresql` adaptor for loading the CommCare\\n   data into your destination database.**\\n\\n![configure_job_postgres](/img/create-job.gif)\\n\\n3. **Create a PostgreSQL credential which will be used by the step to\\n   authenticate with the database.**\\n\\n![add_credential_postgres](/img/postgresql-cred.gif)\\n', metadata={'doc_title': 'commcare-to-db', 'docs_type': 'general_docs'}, score=0.780842483),\n",
       " SearchResult(text='Our team currently spends 3 hours a week manually exporting the Kobo Data and\\n   entering it into DHIS2.3. This automation will eliminate the risk of human error in manually entering\\n   the data, save us money and time, and enable us to work with more patients.## Workflow Requirements Gathering\\n\\nUse the questions below to determine the specific workflow steps. The output of\\nthese questions should be a draft workflow or business process diagram (consider\\ndrafting the diagram in [BPMN](https://www.bpmn.org/) to leverage a standardized\\nnotation).\\n\\n**Questions to ask:**\\n\\n1. What triggers the workflow and how often should it run? (e.g., real-time or\\n   scheduled)\\n   - Is there a user action or system event that should trigger the workflow to\\n     run? (e.g., real-time on form submission, or when record status set to\\n     “closed”)\\n   - Or should this workflow be scheduled at a specific day/time? (e.g., every\\n     day at 12:00)\\n2. Does the workflow require a one-way or two-way data flow?\\n   - E.g., if the workflow is sending one record from System A to System B, does\\n     the data need to flow only one way? Or once the data is synced to System B,\\n     should something be sent back to System A for a bi-directional data flow?', metadata={'doc_title': 'discovery', 'docs_type': 'general_docs'}, score=0.77730161),\n",
       " SearchResult(text=\"See the documentation on\\n[mapping specifications](../design/mapping-specs.md) for more information on\\nglobals.```js\\n//Workflow step 1\\n//First we use fn() to tranform, map & clean our data\\nfn(state => {\\n\\n    //Global mapping rules you want to implement in your workflow\\n    const locationMap = {\\n        //location_id from source app: location value in destination app\\n        01: 'Western Cape',\\n        02: 'Eastern Cape',\\n        03: 'Gauteng'\\n    }\\n\\n    // Here we build the payload of our http request body...\\n    // We assume the input is an array of records\\n    const payload = state.data.map(record => ({\\n        location: locationMap[record.location_id] //translate location_id to the mapped value\\n        external_id: record.case_id\\n    }));\\n\\n    return {...state, payload};\\n});\\n\\n//Workflow step 2\\n//Then we post the payload built in the prior operation to create a record\\npost('/api/myEndpoint', {\\n  headers: {\\n    'Content-Type': 'application/json',\\n  },\\n  body: (state) => state.payload\\n});\\n```\", metadata={'doc_title': 'javascript', 'docs_type': 'general_docs'}, score=0.777159),\n",
       " SearchResult(text='[mapping](/img/mapping_example.png)**To draft a data element mapping specification, you’ll need to…**\\n\\n1. Export the metadata or ask for a list of data elements from the target\\n   systems,\\n2. Procure a sample “input” record from the source system and procure a sample\\n   output record from the destination system. At best, this is an example JSON\\n   payload or a link to example records. At worst, this is a screenshot or a CSV\\n   file with “dummy” data.\\n3. Start “mapping” the data elements and recording transformation rules!\\n\\n|            ![mapping](/img/mapping_process.png)            |\\n| :--------------------------------------------------------: |\\n| _The data mapping process for data integration solutions._ |', metadata={'doc_title': 'mapping-specs', 'docs_type': 'general_docs'}, score=0.77437669),\n",
       " SearchResult(text='> The source code for combine can be found here:\\n> [language-common: combine](https://github.com/OpenFn/language-common/blob/master/src/index.js#L204-L222)**### Create many child records WITHOUT a repeat group in ODK```js\\nbeta.each(\\n  \\'$.data.data[*]\\',\\n  upsert(\\n    \\'Outlet__c\\',\\n    \\'Outlet_Code__c\\',\\n    fields(\\n      field(\\'Outlet_Code__c\\', dataValue(\\'outlet_code\\')),\\n      field(\\'Location__Latitude__s\\', dataValue(\\'gps:Latitude\\')),\\n      field(\\'Location__Longitude__s\\', dataValue(\\'gps:Longitude\\'))\\n    )\\n  )\\n),\\n  beta.each(\\n    \\'$.data.data[*]\\',\\n    upsert(\\n      \\'Outlet_Call__c\\',\\n      \\'Invoice_Number__c\\',\\n      fields(\\n        field(\\'Invoice_Number__c\\', dataValue(\\'invoice_number\\')),\\n        relationship(\\'Outlet__r\\', \\'Outlet_Code__c\\', dataValue(\\'outlet_code\\')),\\n        relationship(\\'RecordType\\', \\'name\\', \\'No Call Card\\'),\\n        field(\\'Trip__c\\', \\'a0FN0000008jPue\\'),\\n        relationship(\\n          \\'Sales_Person__r\\',\\n          \\'Sales_Rep_Code__c\\',\\n          dataValue(\\'sales_rep_code\\')\\n        ),\\n        field(\\'Date__c\\', dataValue(\\'date\\')),\\n        field(\\'Comments__c\\', dataValue(\\'comments\\'))\\n      )\\n    )\\n  );```### Salesforce: perform an update```js\\nupdate(\"Patient__c\", fields(\\n  field(\"Id\", dataValue(\"pathToSalesforceId\")),\\n  field(\"Name__c\", dataValue(\"patient.first_name\")),\\n  field(...)\\n));```### Salesforce: Set record type using \\'relationship(...)\\'```js\\ncreate(\\n  \\'custom_obj__c\\',\\n  fields(\\n    relationship(\\n      \\'RecordType\\',\\n      \\'name\\',\\n      dataValue(\\'submission_type\\'),\\n      field(\\'name\\', dataValue(\\'Name\\'))\\n    )\\n  )\\n);```### Salesforce: Set record type using record Type ID```js\\neach(\\n  \\'$.data.data[*]\\',\\n  create(\\n    \\'fancy_object__c\\',\\n    fields(\\n      field(\\'RecordTypeId\\', \\'012110000008s19\\'),\\n      field(\\'site_size\\', dataValue(\\'size\\'))\\n    )\\n  )\\n);```### Telerivet: Send SMS based on Salesforce workflow alert```js\\nsend(\\n  fields(\\n    field(\\n      \\'to_number\\',\\n      dataValue(\\n        \\'Envelope.Body.notifications.Notification.sObject.phone_number__c\\'\\n      )\\n    ),\\n    field(\\'message_type\\', \\'sms\\'),\\n    field(\\'route_id\\', \\'\\'),\\n    field(\\'content\\', function (state) {\\n      return \\'Hey there. Your name is \\'.concat(\\n        dataValue(\\'Envelope.Body.notifications.Notification.sObject.name__c\\')(\\n          state\\n        ),\\n        \\'.\\'\\n      );\\n    })\\n  )\\n);', metadata={'doc_title': 'job-examples', 'docs_type': 'general_docs'}, score=0.773873508),\n",
       " SearchResult(text='ists using a unique identifier…\\n    - If yes, update the record.\\n    - If not, insert a new record.\\n4. __Don’t forget to consider data volumes. Depending on whether you need to handle 1, 10000, or 1M+ records, your workflow approach may need change.__\\n    - Estimate the file size of the data to be extracted \\n    - Consider API limits (records returned per page, request rate limits)    - Consider bulk operations & batching requests\\n\\n\\nCheck out the technical workflow diagram below for syncing forms submissions from KoboToolBox to DHIS2. The original functional diagram can be found [here](/documentation/design/discovery#workflow-requirements-gathering). \\n\\n![Workflow](/img/technical_example.png)', metadata={'doc_title': 'api-discovery', 'docs_type': 'general_docs'}, score=0.773809433),\n",
       " SearchResult(text='-\\nImplementing batch processing for efficiency and resource optimization.#### 3. Pushing Data to Google Sheets via OpenFn\\n\\nThe Google Sheets adaptor can also be used to push data to Google Sheets from\\nother systems via OpenFn. This allows for seamless integration between external\\napplications and Google Sheets, leveraging OpenFn as the integrator.\\n\\n**Use Cases:** - Importing data from CRM systems into Google Sheets for sales\\nanalysis. - Updating inventory levels in a Google Sheets spreadsheet from an\\ne-commerce platform. - Automatically populating project management data from\\ntask tracking systems into Google Sheets for reporting purposes.\\n\\nA step by step guide is found\\n[in this tutorial](https://docs.openfn.org/documentation/tutorials/http-to-googlesheets)\\nthat shows us how to get data via a REST API and push it to Google Sheet.', metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=0.773271739),\n",
       " SearchResult(text='With Collections, you can save each incoming event onto OpenFn, then run a\\nWorkflow on a Cron trigger to process a batch of events in one go, and send on\\naggregated, filtered or transformed results to the next system.### Mapping Structures\\n\\nA typical use-case is data integrations is to store large mapping objects. These\\nobjects themselves are key-value pairs which map strings from one system into\\nmatching strings from another system. For example, mapping medical codes into\\nSNOMED, or mapping city codes into human-readable strings, or mappings some\\ninput string to a DHIS2 attribute code.\\n\\nThese objects are often very large and hard to maintain, and can bloat job code.\\n\\nInstead, the mappings can be saved to a GitHub repository as a JSON object, and\\nuploaded to a collection using the CLI.', metadata={'doc_title': 'collections', 'docs_type': 'general_docs'}, score=0.772206843),\n",
       " SearchResult(text='It is\\ndefined as a JSON object that consists of the following properties:```json\\n{\\n  \"options\": {\\n    \"start\": \"a\" // optionally specify the start node (defaults to steps[0])\\n  },\\n  \"workflow\": {\\n    \"steps\": [\\n      {\\n        \"id\": \"a\",\\n        \"expression\": \"fn((state) => state)\", // code or a path\\n        \"adaptor\": \"@openfn/language-common@1.75\", // specifiy the adaptor to use (version optional)\\n        \"state\": {\\n          \"data\": {} // optionally pre-populate the data object (this will be overriden by keys in in previous state)\\n        },\\n        \"configuration\": {}, // Use this to pass credentials\\n        \"next\": {\\n          // This object defines which steps to call next\\n          // All edges returning true will run\\n          // If there are no next edges, the workflow will end\\n          \"b\": true,\\n          \"c\": {\\n            \"condition\": \"!state.error\" // Note that this is an expression, not a function\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}```###### Example of a workflow\\n\\n\\nHere\\'s an example of a simple workflow that consists of three steps:```json title=\"workflow.json\"\\n{\\n  \"options\": {\\n    \"start\": \"getPatients\"\\n  },\\n  \"workflow\": {\\n    \"steps\": [\\n      {\\n        \"id\": \"getPatients\",\\n        \"adaptor\": \"http\",\\n        \"expression\": \"getPatients.js\",\\n        \"configuration\": \"tmp/http-creds.json\",\\n        \"next\": {\\n          \"getGlobalOrgUnits\": true\\n        }\\n      },\\n      {\\n        \"id\": \"getGlobalOrgUnits\",\\n        \"adaptor\": \"common\",\\n        \"expression\": \"getGlobalOrgUnits.js\",\\n        \"next\": {\\n          \"createTEIs\": true\\n        }\\n      },\\n      {\\n        \"id\": \"createTEIs\",\\n        \"adaptor\": \"dhis2\",\\n        \"expression\": \"createTEIs.js\",\\n        \"configuration\": \"tmp/dhis2-creds.json\"\\n      }\\n    ]\\n  }\\n}```\\n\\n\\n\\n\\n  tmp/http-creds.json```json title=\"tmp/http-creds.json\"\\n{\\n  \"baseUrl\": \"https://jsonplaceholder.typicode.com\"\\n}```\\n\\n\\n\\n\\n  tmp/dhis2-creds.json```json title=\"tmp/dhis2-creds.json\"\\n{\\n  \"hostUrl\": \"https://play.im.dhis2.org/dev\",\\n  \"password\": \"district\",\\n  \"username\": \"admin\"\\n}```\\n\\n\\n\\n\\n  getPatients.js```js title=\"getPatients.js\"\\n// Get users from jsonplaceholder\\nget(\\'users\\');\\n\\n// Prepare new users as new patients\\nfn(state => {\\n  const newPatients = state.data;\\n  return { ...state, newPatients };\\n});```\\n\\n\\n\\n\\n  getGlobalOrgUnits.js```js title=\"getGlobalOrgUnits.js\"\\n// Globals: orgUnits\\nfn(state => {\\n  const globalOrgUnits = [\\n    {\\n      label: \\'Njandama MCHP\\',\\n      id: \\'g8upMTyEZGZ\\',\\n      source: \\'Gwenborough\\',\\n    },\\n    {\\n      label: \\'Njandama MCHP\\',\\n      id: \\'g8upMTyEZGZ\\',\\n      source: \\'Wisokyburgh\\',\\n    },\\n    {\\n      label: \\'Njandama MCHP\\',\\n      id: \\'g8upMTyEZGZ\\',\\n      source: \\'McKenziehaven\\',\\n    },\\n    {\\n      label: \\'Njandama MCHP\\',\\n      id: \\'g8upMTyEZGZ\\',\\n      source: \\'South Elvis\\',\\n    },\\n    {\\n      label: \\'Ngelehun CHC\\',\\n      id: \\'IpHINAT79UW\\',\\n      source: \\'Roscoeview\\',\\n    },\\n    {\\n      label: \\'Ngelehun CHC\\',\\n      id: \\'IpHINAT79UW\\',\\n      source: \\'South Christy\\',\\n    },\\n    {\\n      label: \\'Ngelehun CHC\\',\\n      id: \\'IpHINAT79UW\\',\\n      source: \\'Howemouth\\',\\n    },\\n    {\\n      label: \\'Ngelehun CHC\\',\\n      id: \\'IpHINAT79UW\\',\\n      source: \\'Aliyaview\\',\\n    },\\n    {\\n      label: \\'Baoma Station CHP\\',\\n      id: \\'jNb63DIHuwU\\',\\n      source: \\'Bartholomebury\\',\\n    },\\n    {      label: \\'Baoma Station CHP\\',\\n      id: \\'jNb63DIHuwU\\',\\n      source: \\'Lebsackbury\\',\\n    },\\n  ];\\n\\n  return { ...state, globalOrgUnits };\\n});```\\n\\n\\n\\n\\n  createTEIs.js```js title=\"createTEIs.js\"\\nfn(state => {\\n  const { newPatients, globalOrgUnits } = state;\\n\\n  const getOrgUnit = city =>\\n    globalOrgUnits.find(orgUnit => orgUnit.source === city).id;\\n\\n  const mappedEntities = newPatients.map(patient => {\\n    const [firstName = \\'Patient\\', lastName = \\'Test\\'] = (\\n      patient.name || \\'\\'\\n    ).split(\\' \\');\\n\\n    const orgUnit = getOrgUnit(patient.address.city);\\n\\n    const attributes = [\\n      { attribute: \\'w75KJ2mc4zz\\', value: firstName },\\n      { attribute: \\'zDhUuAYrxNC\\', value: lastName },\\n      { attribute: \\'cejWyOfXge6\\', value: \\'Male\\' },\\n    ];\\n\\n    return { ...patient, attributes: attributes, orgUnit: orgUnit };\\n  });\\n\\n  return { ...state, mappedEntities };\\n});\\n\\neach(\\n  \\'mappedEntities[*]\\',\\n  create(\\'trackedEntityInstances\\', {\\n    orgUnit: dataValue(\\'orgUnit\\'),\\n    trackedEntityType: \\'nEenWmSyUEp\\',\\n    attributes: dataValue(\\'attributes\\'),\\n  })\\n);', metadata={'doc_title': 'cli-walkthrough', 'docs_type': 'general_docs'}, score=0.770675063),\n",
       " SearchResult(text=\"---\\nsidebar_label: HTTP to GoogleSheets\\ntitle: HTTP to GoogleSheets Workflow\\n---# Create a Workflow connecting a REST API to Google Sheets\\n\\nIn this tutorial, we are going to walk through how to create a simple OpenFn\\nWorkflow that automates syncing data between a REST API and Google Sheets, using\\nthe `http` and `GoogleSheets` [Adaptors](/adaptors).## Video Walkthrough\\n\\nWatch the video and follow along with the steps below.## Before you start\\n\\nHere are some we assume you've looked over before you begin this process.\\n\\n- You have checked out our glossary and have an understanding of basic OpenFn &\\n  API concepts. Check out the pages below to get started\\n  - [OpenFn Concepts](../get-started/terminology.md)\\n  - [A glossary for data integration](../get-started/glossary.md)\\n- You have a Google Account. We will use it to create a credential to authorize\\n  with Google Sheets.\\n- You have access to an OpenFn project (either on a locally installed\\n  [OpenFn v2 app](https://github.com/OpenFn/lightning) or on\\n  [app.openfn.org](https://app.openfn.org)).\", metadata={'doc_title': 'http-to-googlesheets', 'docs_type': 'general_docs'}, score=0.770275354),\n",
       " SearchResult(text=\"[image](https://github.com/OpenFn/docs/assets/167166847/4680c12b-ad57-497e-9073-37e287624f42)```js\\nfunction sendToOpenFn(data) {\\n  var payload = JSON.stringify(data);\\n  var url = 'https://app.openfn.org/i/your-workorder-id';\\n  var options = {\\n    method: 'post',\\n    contentType: 'application/json',\\n    payload: payload,\\n  };\\n  var response = UrlFetchApp.fetch(url, options);\\n  Logger.log(response);\\n}\\n\\nfunction onFormSubmit(e) {\\n  var headers = Object.keys(e.namedValues); // Get column headers from form responses\\n\\n  // Create the survey form entry object dynamically\\n  var surveyFormEntry = { formId: 'surveyForm', data: {} };\\n\\n  // Iterate over each row of form responses\\n  for (var i = 1; i  0) {\\n      surveyFormEntry.data = formData;\\n      // Send the survey form entry to OpenFn and log the workorder id back on Google sheet\\n      sendToOpenFn(surveyFormEntry); // Sending data to OpenFn and getting the response\\n    }\\n  }\\n}\\n```\", metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=0.769161046),\n",
       " SearchResult(text=\"[add_credential_postgres](/img/postgresql-cred.gif)4. **Writing the step:** For this step we will use the upsert operation to\\ninsert/update records in the destination `patient` table and use `patient_id` as\\nthe primary key. An `upsert` will update an existing row if a specified value\\nalready exists in a table, and insert a new row if the specified value doesn't\\nalready exist.```js\\nupsert('patient', 'ON CONSTRAINT patient_pk', {\\n  patient_id: dataValue('data.patient_name'),\\n  patient_name: dataValue('data.patient_name'),\\n  village_name: dataValue('data.village_name'),\\n  last_menstrual_period: dataValue('data.last_menstrual_period'),\\n  expected_delivery_date: dataValue('data.expected_delivery_date'),\\n  children_alive: dataValue('data.children_alive'),\\n  living_children: dataValue('data.living_children'),\\n  feeling_sick: dataValue('data.feeling_sick'),\\n  total_children: dataValue('data.Total_children'),\\n  risk_level: dataValue('data.Risk_level'),\\n});\\n```\\n\\nFeel free to modify the code above to reflect your CommCare and database\\nconfiguration according to your mapping specifications. \\n\\n![create-job](/img/create_job_db.gif)\", metadata={'doc_title': 'commcare-to-db', 'docs_type': 'general_docs'}, score=0.769138),\n",
       " SearchResult(text='> The source code for combine can be found here:\\n> [language-common: combine](https://github.com/OpenFn/language-common/blob/master/src/index.js#L204-L222)**### Create many child records WITHOUT a repeat group in ODK```js\\nbeta.each(\\n  \\'$.data.data[*]\\',\\n  upsert(\\n    \\'Outlet__c\\',\\n    \\'Outlet_Code__c\\',\\n    fields(\\n      field(\\'Outlet_Code__c\\', dataValue(\\'outlet_code\\')),\\n      field(\\'Location__Latitude__s\\', dataValue(\\'gps:Latitude\\')),\\n      field(\\'Location__Longitude__s\\', dataValue(\\'gps:Longitude\\'))\\n    )\\n  )\\n),\\n  beta.each(\\n    \\'$.data.data[*]\\',\\n    upsert(\\n      \\'Outlet_Call__c\\',\\n      \\'Invoice_Number__c\\',\\n      fields(\\n        field(\\'Invoice_Number__c\\', dataValue(\\'invoice_number\\')),\\n        relationship(\\'Outlet__r\\', \\'Outlet_Code__c\\', dataValue(\\'outlet_code\\')),\\n        relationship(\\'RecordType\\', \\'name\\', \\'No Call Card\\'),\\n        field(\\'Trip__c\\', \\'a0FN0000008jPue\\'),\\n        relationship(\\n          \\'Sales_Person__r\\',\\n          \\'Sales_Rep_Code__c\\',\\n          dataValue(\\'sales_rep_code\\')\\n        ),\\n        field(\\'Date__c\\', dataValue(\\'date\\')),\\n        field(\\'Comments__c\\', dataValue(\\'comments\\'))\\n      )\\n    )\\n  );```### Salesforce: perform an update```js\\nupdate(\"Patient__c\", fields(\\n  field(\"Id\", dataValue(\"pathToSalesforceId\")),\\n  field(\"Name__c\", dataValue(\"patient.first_name\")),\\n  field(...)\\n));```### Salesforce: Set record type using \\'relationship(...)\\'```js\\ncreate(\\n  \\'custom_obj__c\\',\\n  fields(\\n    relationship(\\n      \\'RecordType\\',\\n      \\'name\\',\\n      dataValue(\\'submission_type\\'),\\n      field(\\'name\\', dataValue(\\'Name\\'))\\n    )\\n  )\\n);```### Salesforce: Set record type using record Type ID```js\\neach(\\n  \\'$.data.data[*]\\',\\n  create(\\n    \\'fancy_object__c\\',\\n    fields(\\n      field(\\'RecordTypeId\\', \\'012110000008s19\\'),\\n      field(\\'site_size\\', dataValue(\\'size\\'))\\n    )\\n  )\\n);```### Telerivet: Send SMS based on Salesforce workflow alert', metadata={'doc_title': 'job-examples', 'docs_type': 'general_docs'}, score=0.768580854),\n",
       " SearchResult(text=\"s structured data is an\\neasier starting point because we know what to expect from a column with a name,\\ndata type, field size, and so on.## Workflow\\n\\nThe set of instructions that determine how to solve a problem or accomplish a\\ntask. They are often broken down into smaller, independent tasks.## Workflow automation\\n\\nThe use of software to perform tasks or a business process independently, in\\naccordance with predefined business rules, and without the need for human input.## Writeback\\n\\nRefers to a destination system making a change in a data source. When my\\ndestination application receives information from a data source and wants to do\\nsomething back to the source in response, that's writeback.\", metadata={'doc_title': 'glossary', 'docs_type': 'general_docs'}, score=0.768536925),\n",
       " SearchResult(text=\"Then, when building your workflow to implement this globals mapping table\\nspecification, your job expression might looks something like the below code\\nsnippet.```js\\n//Workflow step 1\\n//First we use fn() to transform, map & clean our data\\nfn(state => {\\n\\n    //Global mapping rules you want to implement in your workflow\\n    const locationMap = {\\n        //location_id from source app: location value in destination app\\n        01: 'Western Cape',\\n        02: 'Eastern Cape',\\n        03: 'Gauteng'\\n    }\\n\\n    // Here we build the payload of our http request body...\\n    // We assume the input is an array of records\\n    const payload = state.data.map(record => ({\\n        location: locationMap[record.location_id] //translate location_id to the mapped value\\n        external_id: record.case_id\\n    }));\\n\\n    return {...state, payload};\\n});\\n\\n//Workflow step 2\\n//Then we post the payload built in the prior operation to create a record\\npost('/api/myEndpoint', {\\n  headers: {\\n    'Content-Type': 'application/json',\\n  },\\n  body: (state) => state.payload\\n});\\n```#### Managing global variables & mappings outside of OpenFn\\n\\nWhile the OpenFn XLS-based mapping template is helpful for collaborating with\\nother stakeholders on defining the mapping requirements, once these\\nspecifications are set, you might consider capturing any `globals` mapping rules\\nin an outside application, rather than hard-coding these in your job code\\ndirectly (as in the example above).\\n\\nInstead, you might store these global variables and mapping rules in a separate\\ndatabase table or in an application like\\n[Open Concept Lab](https://openconceptlab.org/), which has a user-friendly web\\napp for recording data dictionaries & mapping rules, and REST API support. This\\nwould then allow you to dynamically query these mapping rules using OpenFn, to\\nensure your integration is utilizing the latest and greatest specifications.\\n\\nIn this case, your workflow configuration might look like the below, where the\\nsecond step in the workflow is dedicated to querying this list of global\", metadata={'doc_title': 'mapping-specs', 'docs_type': 'general_docs'}, score=0.768085837)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testx threshold\n",
    "# estimate a threshold to use in sim search\n",
    "\n",
    "docsite_search = DocsiteSearch(collection_name=\"docsite-20250225\")\n",
    "results = docsite_search.search(\"hCan I write a step that allows for creation of an JSON file of the transformed records that can be sent to an email or Google Drive\", top_k=20) #threshold=0.73\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a threshold to use in sim search\n",
    "def determine_threshold(user_questions):\n",
    "    docsite_search = DocsiteSearch(collection_name=\"docsite-20250225\")\n",
    "    \n",
    "    results_dict = {}\n",
    "    for question in user_questions:\n",
    "        results = docsite_search.search(question, top_k=30)\n",
    "        results_dict[question] = results\n",
    "    \n",
    "    threshold_prompt = \"\"\"\n",
    "    You are an expert in determining relevant document retrieval thresholds.\n",
    "    Given a user question, a retrieved search result, and its relevance score,\n",
    "    determine a reasonable threshold score for filtering relevant results.\n",
    "    \n",
    "    The input data follows this format:\n",
    "    - User Question: {user_question}\n",
    "    - Search Result: {search_result}\n",
    "    - Relevance Score: {score}\n",
    "    \n",
    "    Provide 2–3-sentence reasoning and a single numerical threshold value as output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def llm_determine_threshold(user_question, results):\n",
    "        prompt_content = \"\\n\".join([\n",
    "            threshold_prompt.format(\n",
    "                user_question=user_question,\n",
    "                search_result=r.text,\n",
    "                score=r.score\n",
    "            ) for r in results\n",
    "        ])\n",
    "        \n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  \n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "            system=\"Decide an appropriate threshold for filtering relevant search results.\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_content}]}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return message.content[0].text.strip()\n",
    "    \n",
    "    thresholds = {}\n",
    "    for question, results in results_dict.items():\n",
    "        thresholds[question] = llm_determine_threshold(question, results)\n",
    "        \n",
    "    return thresholds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/hanna/opt/anaconda3/envs/openfn/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 30 results\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'How do I modify a contact on SalesforceWrite a job for me that creates new DataValueSets under the \"Approved School CHP\" organization unit.': \"Looking at the search results and their relevance scores, I notice that the scores cluster between 0.75-0.79, with the most relevant result having a score of 0.789996564. The first result contains directly relevant DHIS2 DataValueSet creation code, while results below 0.78 tend to be less directly relevant to the user's question about creating DataValueSets. Based on this pattern, I would recommend a threshold of 0.78 to filter for the most relevant results while excluding less relevant examples.\\n\\nThreshold value: 0.78\",\n",
       " 'How to get data from Asana in my job?': 'Based on the search results shown, I can see that the most relevant result has a score of 0.814727 and contains direct information about Asana integration options, while results below 0.80 start to become less relevant to the specific question about getting data from Asana. Looking at the distribution of scores, there appears to be a natural break point around 0.80 where relevance begins to drop more significantly.\\n\\nRecommended threshold: 0.80\\n\\nThis threshold would ensure that only highly relevant results about Asana data integration are included while filtering out less relevant content about other systems and general workflow concepts.',\n",
       " \"What's the difference between a Trigger and a Step in a workflow?\": 'Based on analyzing the search results and their relevance scores, I recommend a threshold of 0.80 for filtering relevant results for this question about the difference between Triggers and Steps in workflows. This threshold is appropriate because the most relevant results (scoring >0.80) contain direct explanations of both concepts, while results below 0.80 tend to be more tangential or focus on only one aspect. The highest scoring results clearly define Triggers as \"when to do it\" (determining how/when workflows execute) and Steps as \"what to do\" (specific tasks/activities within the workflow).\\n\\nThreshold value: 0.80',\n",
       " 'Can I write a step that allows for creation of a JSON file of the transformed records that can be sent to an email or Google Drive?': \"Based on the search results and relevance scores shown, I recommend a threshold of 0.78 for filtering relevant results. The highest scoring results (0.797) contain directly relevant information about connecting to Google services and file handling, while results below 0.78 become increasingly tangential to the user's question about JSON file creation and sending to email/Drive. This threshold provides a good balance between including helpful implementation details while filtering out less relevant technical documentation.\",\n",
       " \"Why does this part of the code only POST the first submission in the submissions[] array? I want to POST each submission. each( '$.submissions[*]', post(URL, { body: () => {} ))\": \"Based on the search results and relevance scores, I notice that the most relevant result has a score of 0.808573, while less relevant results drop below 0.76. The first result directly addresses the user's question about POSTing submissions with detailed code examples and explanations, while lower-scoring results contain more tangential information about general API usage and configuration.\\n\\nGiven this pattern, I recommend a threshold of 0.78 to filter relevant results. This would retain the most pertinent results that specifically address array iteration and POST operations while filtering out less relevant documentation about general setup and configuration.\",\n",
       " 'Write a job to fetch for an attribute in the input and map it.': \"Looking at the search results and relevance scores, I notice that the most relevant results cluster around 0.75-0.77, with the first result scoring 0.766. The results show code examples and documentation for fetching and mapping data, which directly relates to the user's question about writing a job to fetch and map attributes. Given this pattern and the clear relevance of results above 0.74, I recommend a threshold of 0.74 to capture the meaningful results while filtering out less relevant content.\\n\\nThreshold value: 0.74\",\n",
       " 'Can I set up a workflow to trigger based on changes in a database using a webhook? How?': \"Based on the search results shown, I recommend a threshold of 0.80 for the following reasons:\\n\\n1. The results with scores above 0.80 contain directly relevant information about setting up webhooks and workflows, including specific implementation details and technical steps.\\n\\n2. Results below 0.80 tend to be more tangentially related, covering general concepts or peripheral topics that don't directly address the core question about webhook-triggered workflows.\\n\\nThreshold value: 0.80\",\n",
       " 'How can I securely manage API keys and other sensitive information using environment variables in OpenFn?': \"Based on the search results shown, I recommend a threshold of 0.82 for filtering relevant results. The most relevant documents (scoring above 0.82) contain specific information about securely managing API keys and credentials in OpenFn, including examples of using environment variables and state objects. Results below this threshold tend to contain more general or tangential information about OpenFn that doesn't directly address credential security management.\",\n",
       " 'Does OpenFn offer any built-in support for encrypting sensitive data in transit or at rest?': \"Based on the search results shown, I notice that the highest relevance scores are around 0.83 and the lowest are around 0.78, with most relevant results clustering above 0.80. The most relevant results contain important security-related information about OpenFn's data protection capabilities, while results below 0.80 tend to be less directly related to the encryption question. Therefore, I recommend setting a threshold of 0.80 to capture the most pertinent security-focused content while filtering out less relevant results.\\n\\nThreshold value: 0.80\",\n",
       " 'How can I customize the logging level and output format for my workflows?': 'Based on the search results shown, I can provide a clear threshold recommendation. The most relevant results (scoring around 0.82) contain direct information about log level customization, while results below 0.79 become increasingly less relevant to the specific question about customizing logging levels and output formats. Therefore, I recommend a threshold of 0.79 to ensure high-quality, relevant results while filtering out less pertinent information.\\n\\nThreshold: 0.79',\n",
       " \"What's the recommended workflow for using Git to manage and deploy changes to my OpenFn workflows?\": 'Based on the search results shown, I can see that most highly relevant results (scoring above 0.84) contain detailed information about Git workflows for OpenFn, including staging/production setups and version control procedures. The relevance scores show a natural break point around 0.84, with less relevant results falling below this threshold. Therefore, I recommend setting the threshold at 0.84 to capture the most pertinent information while filtering out less relevant content.\\n\\nThreshold value: 0.84',\n",
       " 'How can I write unit tests for my OpenFn jobs to ensure they are working correctly?': 'Looking at the relevance scores across these search results, I notice they range from ~0.79 to ~0.82, with most relevant results clustering around 0.81-0.82. Given that the question specifically asks about unit testing OpenFn jobs, and considering that even the most relevant results (scoring ~0.82) contain only partially relevant information, I would recommend setting the threshold at 0.81 to capture the most pertinent content while filtering out less relevant results that score below this mark.\\n\\nThreshold value: 0.81',\n",
       " 'How can I set up alerts to notify me when a workflow fails or exceeds a certain execution time?': 'Based on the search results, I can see that the most relevant result has a score of 0.844 and directly addresses setting up failure notifications, while results below 0.80 become less focused on the core question about alerts and notifications. The scores show a clear drop-off in relevance below 0.80, with many results containing only tangentially related information about workflows and monitoring. Therefore, I recommend a threshold of 0.80 to ensure retrieved results remain highly relevant to questions about workflow notifications and alerts while filtering out less relevant content.\\n\\nThreshold value: 0.80',\n",
       " 'How can I use the post() function with the HTTP adapter to send transformed data to another system?': 'Based on the search results and relevance scores, I recommend a threshold of 0.78 for filtering relevant results. This recommendation is based on observing that results above 0.78 contain specific, actionable information about using the post() function with the HTTP adapter, including code examples and implementation details, while results below this threshold tend to be more tangential or contain less directly relevant information. The highest scoring results (0.80+) contain the most pertinent information about HTTP POST requests and data transformation.',\n",
       " \"What's the syntax for using the create() function with the DHIS2 adapter to create new data elements?\": 'Based on the search results, I recommend a threshold score of 0.82 for filtering relevant results. The highest scoring result (0.823) contains directly relevant examples of using the create() function with DHIS2 for data elements, while results below 0.82 tend to contain either partial matches or examples for other types of resources. This threshold provides a good balance between including highly relevant results while filtering out less relevant content that may confuse users.',\n",
       " 'How do I use the alterState() function to modify the state object between operations?': \"Based on analyzing the search results and their relevance scores, I notice that the most relevant results cluster around 0.79-0.80, with a clear dropoff in relevance below 0.79. The second result with score 0.798 contains a direct example of using alterState() between operations, making it highly relevant to the user's question. Therefore, I recommend setting the threshold at 0.79 to capture the most pertinent results while filtering out less relevant content.\\n\\nThreshold value: 0.79\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = determine_threshold(user_questions)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>consult_doc_label</th>\n",
       "      <th>consult_adaptor_docs_label</th>\n",
       "      <th>consult_general_docs_label</th>\n",
       "      <th>consult_doc_answer_v1</th>\n",
       "      <th>consult_adaptor_docs_answer_v1</th>\n",
       "      <th>consult_general_docs_answer_v1</th>\n",
       "      <th>search_query_answer_v1</th>\n",
       "      <th>search_results_answer_v1</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>evaluations_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there a way to handle pagination when pulli...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'pagination memory large dataset', ...</td>\n",
       "      <td>[[SearchResult(text='This typically won\\'t hap...</td>\n",
       "      <td>True\\n\\nThe second snippet discusses cursor-ba...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When authenticating with API tokens instead of...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'DHIS2', 'doc_type': 'adaptor_docs'...</td>\n",
       "      <td>[[SearchResult(text=\"# DHIS2## App Overview\\n\\...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to update a Contact record in Salesforc...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'salesforce', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: Salesforce\\n-...</td>\n",
       "      <td>True\\n\\nThe retrieved documentation includes a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I fetch submissions from a specific fo...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'kobotoolbox', 'doc_type': 'adaptor...</td>\n",
       "      <td>[[SearchResult(text=\"---\\ntitle: Kobo Toolbox\\...</td>\n",
       "      <td>True\\n\\nThe documentation snippets show how to...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to write data from a workflow to a Goog...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'google_sheets', 'doc_type': 'adapt...</td>\n",
       "      <td>[[SearchResult(text='ng data from CRM systems ...</td>\n",
       "      <td>False\\n\\nWhile the documentation snippets cont...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>In PostgreSQL, how do I write a query to updat...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>What's the best way to handle NULL values in a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nWhile the documentation contains info...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>How do I write a SQL query to pivot rows into ...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'sql pivot dynamic columns', 'doc_t...</td>\n",
       "      <td>[[SearchResult(text=')### Sample Job Expressio...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In PostgreSQL, how can I use the JSONB data ty...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What's the most efficient way to bulk insert a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Is there a way to handle pagination when pulli...   \n",
       "1    When authenticating with API tokens instead of...   \n",
       "2    I need to update a Contact record in Salesforc...   \n",
       "3    How can I fetch submissions from a specific fo...   \n",
       "4    I want to write data from a workflow to a Goog...   \n",
       "..                                                 ...   \n",
       "105  In PostgreSQL, how do I write a query to updat...   \n",
       "106  What's the best way to handle NULL values in a...   \n",
       "107  How do I write a SQL query to pivot rows into ...   \n",
       "108  In PostgreSQL, how can I use the JSONB data ty...   \n",
       "109  What's the most efficient way to bulk insert a...   \n",
       "\n",
       "                question_type  consult_doc_label  consult_adaptor_docs_label  \\\n",
       "0           adaptor_questions               True                        True   \n",
       "1           adaptor_questions               True                        True   \n",
       "2           adaptor_questions               True                        True   \n",
       "3           adaptor_questions               True                        True   \n",
       "4           adaptor_questions               True                        True   \n",
       "..                        ...                ...                         ...   \n",
       "105  general_coding_questions              False                       False   \n",
       "106  general_coding_questions              False                       False   \n",
       "107  general_coding_questions              False                       False   \n",
       "108  general_coding_questions              False                       False   \n",
       "109  general_coding_questions              False                       False   \n",
       "\n",
       "     consult_general_docs_label  consult_doc_answer_v1  \\\n",
       "0                         False                  False   \n",
       "1                         False                   True   \n",
       "2                         False                   True   \n",
       "3                         False                   True   \n",
       "4                         False                   True   \n",
       "..                          ...                    ...   \n",
       "105                       False                  False   \n",
       "106                       False                  False   \n",
       "107                       False                  False   \n",
       "108                       False                  False   \n",
       "109                       False                  False   \n",
       "\n",
       "     consult_adaptor_docs_answer_v1  consult_general_docs_answer_v1  \\\n",
       "0                             False                           False   \n",
       "1                             False                           False   \n",
       "2                             False                           False   \n",
       "3                             False                           False   \n",
       "4                             False                           False   \n",
       "..                              ...                             ...   \n",
       "105                           False                           False   \n",
       "106                           False                           False   \n",
       "107                           False                           False   \n",
       "108                           False                           False   \n",
       "109                           False                           False   \n",
       "\n",
       "                                search_query_answer_v1  \\\n",
       "0    [{'query': 'pagination memory large dataset', ...   \n",
       "1    [{'query': 'DHIS2', 'doc_type': 'adaptor_docs'...   \n",
       "2    [{'query': 'salesforce', 'doc_type': 'adaptor_...   \n",
       "3    [{'query': 'kobotoolbox', 'doc_type': 'adaptor...   \n",
       "4    [{'query': 'google_sheets', 'doc_type': 'adapt...   \n",
       "..                                                 ...   \n",
       "105  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "106  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "107  [{'query': 'sql pivot dynamic columns', 'doc_t...   \n",
       "108  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "109  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "\n",
       "                              search_results_answer_v1  \\\n",
       "0    [[SearchResult(text='This typically won\\'t hap...   \n",
       "1    [[SearchResult(text=\"# DHIS2## App Overview\\n\\...   \n",
       "2    [[SearchResult(text='---\\ntitle: Salesforce\\n-...   \n",
       "3    [[SearchResult(text=\"---\\ntitle: Kobo Toolbox\\...   \n",
       "4    [[SearchResult(text='ng data from CRM systems ...   \n",
       "..                                                 ...   \n",
       "105  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "106  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "107  [[SearchResult(text=')### Sample Job Expressio...   \n",
       "108  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "109  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "\n",
       "                                            evaluation  evaluations_bin  \n",
       "0    True\\n\\nThe second snippet discusses cursor-ba...             True  \n",
       "1    False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "2    True\\n\\nThe retrieved documentation includes a...             True  \n",
       "3    True\\n\\nThe documentation snippets show how to...             True  \n",
       "4    False\\n\\nWhile the documentation snippets cont...            False  \n",
       "..                                                 ...              ...  \n",
       "105  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "106  False\\n\\nWhile the documentation contains info...            False  \n",
       "107  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "108  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "109  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "\n",
       "[110 rows x 12 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"results_v1_top3.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 7 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 12 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 10 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 7 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 16 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 23 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 5 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 7 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 15 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 4 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 17 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 4 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 6 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 4 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 29 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 13 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 5 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 7 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 4 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 5 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 15 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 14 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 6 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 6 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 13 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 11 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 7 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 8 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 2 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 12 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 3 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 25 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 4 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 0 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retrieved: 30\n",
      "INFO:DocsiteSearch:Filtered to 1 results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.109090909090909"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many are returned at a threshold\n",
    "threshold = 0.79\n",
    "results = []\n",
    "\n",
    "for user_question, queries, eval in zip(df.question.to_list(), df.search_query_answer_v1.to_list(), df.evaluation.to_list()):\n",
    "    queries = queries.replace(\"None\", \"null\")\n",
    "    queries = queries.replace(\"'\", '\"')\n",
    "    queries = json.loads(queries)\n",
    "    docs = []\n",
    "    for q in queries:\n",
    "        result = docsite_search.search(q.get(\"query\"), top_k=30, threshold=threshold, docs_type=q.get(\"doc_type\"))\n",
    "        docs.extend(result)\n",
    "    results.append([user_question, queries, docs, len(docs), len(queries), threshold, eval])\n",
    "\n",
    "thre_df = pd.DataFrame(results, columns=[\"question\", \"queries\", \"docs_retrieved\", \"num_docs\", \"num_queries\", \"threshold\", \"orig_eval\"])\n",
    "thre_df.to_csv(f\"thre_eval_{threshold}.csv\", index=False)\n",
    "thre_df.num_docs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.94 mean at 0.78\n",
    "4 mean at 0.79\n",
    "2.218 mean at 0.8\n",
    "0.736 mean at 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre_df = pd.read_csv(\"thre_eval_0.78.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max docs retrieved:  34\n",
      "avg docs per useful=FALSE 5.5813953488372094\n",
      "avg docs per useful=TRUE 7.82089552238806\n"
     ]
    }
   ],
   "source": [
    "print(\"max docs retrieved: \", thre_df.num_docs.max())\n",
    "thre_df_f = thre_df[thre_df[\"orig_eval\"].str.startswith(\"False\")]\n",
    "thre_df_t = thre_df[thre_df[\"orig_eval\"].str.startswith(\"True\")]\n",
    "print(\"avg docs per useful=FALSE\", thre_df_f.num_docs.mean())\n",
    "print(\"avg docs per useful=TRUE\", thre_df_t.num_docs.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orig_eval shows if we origianlly retrieved useful docs at a limit of n=3 (usu max total 6)\n",
    "hard to set a threshold to capture those but filter others further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "pc_db = PineconeVectorStore(index_name=\"docsite\", namespace=\"docsite-20250225\", embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what's asana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='56cc39d7-0f22-4f6c-85ac-882f6f28c219', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, page_content='---\\ntitle: Asana Adaptor\\n---## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.'),\n",
       " Document(id='7198fce5-63f8-4818-bbe5-e45d8595dcae', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, page_content='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)'),\n",
       " Document(id='49e47230-001d-4b06-9ba3-9caffee802bc', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, page_content=\"Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\"),\n",
       " Document(id='553835b6-04c2-48eb-b13b-5d0c83944ff1', metadata={'doc_title': 'glossary', 'docs_type': 'general_docs'}, page_content='Then, we load\\n(send it to the destination).## Integration platform\\n\\nAn integration platform (e.g., OpenFn) is an application (or set of\\napplications) that help organizations set up, run, and maintain/manage the\\nintegrations between all of their various systems.### iPaaS\\n\\nYou may also see the acronym \"iPaaS\". This stands for integration platform as a\\nservice and is a type of \"software as a service\" (or \"SaaS\"). SaaS is a software\\npurchasing model in which software is paid for only as it is used (often\\nmonth-to-month), rather than purchased up front or given away for free.## Metadata\\n\\nThis is data that tells us about our data. In a table, for example, that\\'s the\\nname of the columns, the number of rows, etc. Metadata is often brought up in\\nconversations about privacy—e.g., regulators might want to ensure that _only\\nmetadata_ is moved from Ministry A to Ministry B, as opposed to personally\\nidentifiable information (PII) about individuals themselves.'),\n",
       " Document(id='601804ca-83b6-47fa-8ff5-ef9659844778', metadata={'doc_title': 'terminology', 'docs_type': 'general_docs'}, page_content='Otherwise, read on!## Project\\n\\nA Project is an administrative grouping in OpenFn like a \"workspace\".\\n\\nOn the platform (OpenFn/lightning), Projects define who can access your OpenFn\\nworkflow configuration & history. Projects have an owner and one or more\\nCollaborators.\\n\\nIn local deployment and development, Project also corresponds to a\\n[`project.yaml`](/documentation/deploy/portability-versions#v2) file, which\\ndefines a project\\' configuration.\\n\\nIn either case, a Project contains Workflows, Triggers, Credentials, and\\neverything you need to automate and integrate with OpenFn.## Workflow\\n\\n:::tip\\n\\nWorkflows are the **\"what to do\"** part of automation!\\n\\n:::\\n\\nA Workflow is a collection of a Trigger, Steps, Paths, and custom logic\\nconnected together to automate a specific business process or task. A Workflow\\nis configured via the Canvas in the web app, or locally (via code).\\n\\nOpenFn automation centers around [Workflows](/documentation/build/workflows),\\nwhich may have one or multiple Steps. Workflows can be run in real-time (based\\non an event -e.g., new patient registration), on a scheduled basis (e.g., every\\nday at 8am), or manually on-demand.\\n\\nThink of workflow as a set of instructions you might give a staff member (e.g.,\\nplease create a new Patient record in OpenMRS when a form containing a newly\\nregistered client is received from CommCare, export data to DHIS2 every week on\\nFriday 11pm, send SMS with payment confirmation number when payment confirmation\\nmessage is received etc.).\\n\\nCommon Workflows automate:\\n'),\n",
       " Document(id='ec616bad-b3a4-45b1-840b-f8052879114b', metadata={'doc_title': 'openfn-roadmap', 'docs_type': 'general_docs'}, page_content='We are committed to constantly evaluating\\nwhat our user base needs most and spending the few resources we have to deliver\\nvalue to them—we simply can\\'t guarantee that what sounds like the \"7th most\\nimportant feature to build\" now will still be on our list in 6 months.At the same time, we strive to be as transparent and inclusive as we can in our\\nplanning processes. We have a big backlog of feature requests and GitHub issues\\n(bug reports, stubs, even partially shaped epics or projects) that are getting\\nvoted up, commented on, and used as inspiration when we\\'re deciding what to\\nprioritize next.\\n\\nRead on to learn more about how we work, how you can see what\\'s coming, and how\\nyou can get involved!\\n\\n:::## What are we currently working on?\\n\\nAll of our team\\'s work is tracked publicly using a GitHub Project. Three key\\nviews give you up-to-the-minute insights on what we\\'re doing and what\\'s on our\\nimmediate roadmap.'),\n",
       " Document(id='24cfd388-36b9-47cc-8af3-c354407fac4f', metadata={'doc_title': 'overview', 'docs_type': 'general_docs'}, page_content='If you capture these 3 elements, user stories can b ean effective way of\\ndetailing integration requirements and starting discussions at your organization\\nabout which requirements are priority.### Example user stories:\\n\\n- **Case Referrals:** As a caseworker, I want to automatically send referral\\n  requests to my partner agency using another case management system, so that I\\n  can securely share case information and quickly notify them when their\\n  services are needed in a crisis situation.\\n- **EMR - HIS:** As a clinic manager, I would like to integrate patient data\\n  from the district clinic electronic medical record system with the national\\n  DHIS2 health information system, so that I can securely and automatically\\n  report on health outcomes for key indicators in my district.\\n- **Kobo Toolbox - MSSQL Database:** As a M&E manager, I want to monitor Kobo\\n  Toolbox survey responses in a central database in real-time, so that I can\\n  better understand data collection activities and program performance across my\\n  research partner sites.'),\n",
       " Document(id='21f25edf-2f75-44dc-8533-17b7df30267b', metadata={'doc_title': 'salesforce', 'docs_type': 'adaptor_docs'}, page_content=\"Learn more about how data is structured in Salesforce at the links below:\\n\\n- How data is organized in Salesforce:\\n  https://help.salesforce.com/s/articleView?id=sf.basics_organize_data.htm&type=5\\n- Salesforce objects: https://www.salesforcetutorial.com/salesforce-objects/## Integration Use Cases\\n\\nSalesforce may be used to manage an organization's programs, operations,\\nfundraising, and more. Therefore integrations with 3rd-party apps is a common\\nrequirement.\\n\\nExample user stories:\\n\\n- As a program or M&E manager working in areas with limited internet\\n  connectivity, I would like to collect data on a mobile applicaiton that\\n  supports offline data capture, but then sync the data collected to Salesforce\\n  so that I can centrally monitor field activities and analyze data collected to\\n  evaluate program impact.\\n- As a fundraiser at an NGO, I would like to see all donor information tracked\\n  in Salesforce so that I can better monitor fundraising activities, nurture\\n  relationships with all funders, and manage campaigns.\"),\n",
       " Document(id='3dcb2d90-d35d-43e7-a6c8-65958cdde4c4', metadata={'doc_title': 'surveycto', 'docs_type': 'adaptor_docs'}, page_content='This versatile platform caters\\nto a diverse array of needs, serving researchers, NGOs, government agencies, and\\nother entities seeking reliable data collection solutions.## Integration Use Cases:\\n\\nAs SurveyCTO is primarily a mobile data collection tool, often that data needs\\nto be extracted, managed, summarized, and analysed in another system (e.g.,\\ndatabase/data warehouse, analytics tools, and \"MIS\"/data management software\\nlike like DHIS2, Salesforce, etc.).\\n\\nExample user story:\\n\\n- As a program or M&E manager, I would like my field officers to collect data in\\n  places with limited connection availability using SurveyCTO. I want the data\\n  collected to automatically sync to my Tabluea dashboard for visualization and\\n  further analysis.'),\n",
       " Document(id='0156ea8b-ff01-438a-89ad-1fcf3436437d', metadata={'doc_title': 'ai-assistant', 'docs_type': 'general_docs'}, page_content=':::## About the Assistant\\n\\nThe AI Assistant is built on the Claude 3.5 Sonnet model from\\n[Anthropic](https://www.anthropic.com/) and is trained with OpenFn documentation\\nand example job code.\\n\\nAt the moment, the Assistant is only configured to help with job writing. Later\\nversions of the assistant may be rolled out to other pages.\\n\\nAll chat sessions are shared between all users of the project. You can start a\\nnew chat session at any time, or open an old one.\\n\\nInput data and run-time job logs not sent to the model. But we do include\\nyour step code so that the Assistant can provide a contextually relevant answer.## A Note on Responsible AI Usage\\n\\nThe AI assistant is built on emerging Large Language Model (terminology). Like\\nother LLMS and chatbots, its capabilities are impressive, but imperfect.\\n\\nRemember that ultimately, all responses are generated by an algorithm and YOU,\\nthe human in charge, are responsible for how its output is used. You should\\nconsider all responses critically and verify the output where possible.\\n\\nYou can read more about our approach to AI in our\\n[Responsible AI Policy](https://www.openfn.org/ai).')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_db.similarity_search(query=query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/hanna/opt/anaconda3/envs/openfn/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(id='56cc39d7-0f22-4f6c-85ac-882f6f28c219', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, page_content='---\\ntitle: Asana Adaptor\\n---## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.'),\n",
       "  0.782627642),\n",
       " (Document(id='7198fce5-63f8-4818-bbe5-e45d8595dcae', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, page_content='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)'),\n",
       "  0.756549),\n",
       " (Document(id='49e47230-001d-4b06-9ba3-9caffee802bc', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, page_content=\"Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\"),\n",
       "  0.751750648),\n",
       " (Document(id='553835b6-04c2-48eb-b13b-5d0c83944ff1', metadata={'doc_title': 'glossary', 'docs_type': 'general_docs'}, page_content='Then, we load\\n(send it to the destination).## Integration platform\\n\\nAn integration platform (e.g., OpenFn) is an application (or set of\\napplications) that help organizations set up, run, and maintain/manage the\\nintegrations between all of their various systems.### iPaaS\\n\\nYou may also see the acronym \"iPaaS\". This stands for integration platform as a\\nservice and is a type of \"software as a service\" (or \"SaaS\"). SaaS is a software\\npurchasing model in which software is paid for only as it is used (often\\nmonth-to-month), rather than purchased up front or given away for free.## Metadata\\n\\nThis is data that tells us about our data. In a table, for example, that\\'s the\\nname of the columns, the number of rows, etc. Metadata is often brought up in\\nconversations about privacy—e.g., regulators might want to ensure that _only\\nmetadata_ is moved from Ministry A to Ministry B, as opposed to personally\\nidentifiable information (PII) about individuals themselves.'),\n",
       "  0.72738862),\n",
       " (Document(id='601804ca-83b6-47fa-8ff5-ef9659844778', metadata={'doc_title': 'terminology', 'docs_type': 'general_docs'}, page_content='Otherwise, read on!## Project\\n\\nA Project is an administrative grouping in OpenFn like a \"workspace\".\\n\\nOn the platform (OpenFn/lightning), Projects define who can access your OpenFn\\nworkflow configuration & history. Projects have an owner and one or more\\nCollaborators.\\n\\nIn local deployment and development, Project also corresponds to a\\n[`project.yaml`](/documentation/deploy/portability-versions#v2) file, which\\ndefines a project\\' configuration.\\n\\nIn either case, a Project contains Workflows, Triggers, Credentials, and\\neverything you need to automate and integrate with OpenFn.## Workflow\\n\\n:::tip\\n\\nWorkflows are the **\"what to do\"** part of automation!\\n\\n:::\\n\\nA Workflow is a collection of a Trigger, Steps, Paths, and custom logic\\nconnected together to automate a specific business process or task. A Workflow\\nis configured via the Canvas in the web app, or locally (via code).\\n\\nOpenFn automation centers around [Workflows](/documentation/build/workflows),\\nwhich may have one or multiple Steps. Workflows can be run in real-time (based\\non an event -e.g., new patient registration), on a scheduled basis (e.g., every\\nday at 8am), or manually on-demand.\\n\\nThink of workflow as a set of instructions you might give a staff member (e.g.,\\nplease create a new Patient record in OpenMRS when a form containing a newly\\nregistered client is received from CommCare, export data to DHIS2 every week on\\nFriday 11pm, send SMS with payment confirmation number when payment confirmation\\nmessage is received etc.).\\n\\nCommon Workflows automate:\\n'),\n",
       "  0.723739266),\n",
       " (Document(id='ec616bad-b3a4-45b1-840b-f8052879114b', metadata={'doc_title': 'openfn-roadmap', 'docs_type': 'general_docs'}, page_content='We are committed to constantly evaluating\\nwhat our user base needs most and spending the few resources we have to deliver\\nvalue to them—we simply can\\'t guarantee that what sounds like the \"7th most\\nimportant feature to build\" now will still be on our list in 6 months.At the same time, we strive to be as transparent and inclusive as we can in our\\nplanning processes. We have a big backlog of feature requests and GitHub issues\\n(bug reports, stubs, even partially shaped epics or projects) that are getting\\nvoted up, commented on, and used as inspiration when we\\'re deciding what to\\nprioritize next.\\n\\nRead on to learn more about how we work, how you can see what\\'s coming, and how\\nyou can get involved!\\n\\n:::## What are we currently working on?\\n\\nAll of our team\\'s work is tracked publicly using a GitHub Project. Three key\\nviews give you up-to-the-minute insights on what we\\'re doing and what\\'s on our\\nimmediate roadmap.'),\n",
       "  0.721128225),\n",
       " (Document(id='24cfd388-36b9-47cc-8af3-c354407fac4f', metadata={'doc_title': 'overview', 'docs_type': 'general_docs'}, page_content='If you capture these 3 elements, user stories can b ean effective way of\\ndetailing integration requirements and starting discussions at your organization\\nabout which requirements are priority.### Example user stories:\\n\\n- **Case Referrals:** As a caseworker, I want to automatically send referral\\n  requests to my partner agency using another case management system, so that I\\n  can securely share case information and quickly notify them when their\\n  services are needed in a crisis situation.\\n- **EMR - HIS:** As a clinic manager, I would like to integrate patient data\\n  from the district clinic electronic medical record system with the national\\n  DHIS2 health information system, so that I can securely and automatically\\n  report on health outcomes for key indicators in my district.\\n- **Kobo Toolbox - MSSQL Database:** As a M&E manager, I want to monitor Kobo\\n  Toolbox survey responses in a central database in real-time, so that I can\\n  better understand data collection activities and program performance across my\\n  research partner sites.'),\n",
       "  0.720605254),\n",
       " (Document(id='21f25edf-2f75-44dc-8533-17b7df30267b', metadata={'doc_title': 'salesforce', 'docs_type': 'adaptor_docs'}, page_content=\"Learn more about how data is structured in Salesforce at the links below:\\n\\n- How data is organized in Salesforce:\\n  https://help.salesforce.com/s/articleView?id=sf.basics_organize_data.htm&type=5\\n- Salesforce objects: https://www.salesforcetutorial.com/salesforce-objects/## Integration Use Cases\\n\\nSalesforce may be used to manage an organization's programs, operations,\\nfundraising, and more. Therefore integrations with 3rd-party apps is a common\\nrequirement.\\n\\nExample user stories:\\n\\n- As a program or M&E manager working in areas with limited internet\\n  connectivity, I would like to collect data on a mobile applicaiton that\\n  supports offline data capture, but then sync the data collected to Salesforce\\n  so that I can centrally monitor field activities and analyze data collected to\\n  evaluate program impact.\\n- As a fundraiser at an NGO, I would like to see all donor information tracked\\n  in Salesforce so that I can better monitor fundraising activities, nurture\\n  relationships with all funders, and manage campaigns.\"),\n",
       "  0.716697276),\n",
       " (Document(id='3dcb2d90-d35d-43e7-a6c8-65958cdde4c4', metadata={'doc_title': 'surveycto', 'docs_type': 'adaptor_docs'}, page_content='This versatile platform caters\\nto a diverse array of needs, serving researchers, NGOs, government agencies, and\\nother entities seeking reliable data collection solutions.## Integration Use Cases:\\n\\nAs SurveyCTO is primarily a mobile data collection tool, often that data needs\\nto be extracted, managed, summarized, and analysed in another system (e.g.,\\ndatabase/data warehouse, analytics tools, and \"MIS\"/data management software\\nlike like DHIS2, Salesforce, etc.).\\n\\nExample user story:\\n\\n- As a program or M&E manager, I would like my field officers to collect data in\\n  places with limited connection availability using SurveyCTO. I want the data\\n  collected to automatically sync to my Tabluea dashboard for visualization and\\n  further analysis.'),\n",
       "  0.715661168),\n",
       " (Document(id='0156ea8b-ff01-438a-89ad-1fcf3436437d', metadata={'doc_title': 'ai-assistant', 'docs_type': 'general_docs'}, page_content=':::## About the Assistant\\n\\nThe AI Assistant is built on the Claude 3.5 Sonnet model from\\n[Anthropic](https://www.anthropic.com/) and is trained with OpenFn documentation\\nand example job code.\\n\\nAt the moment, the Assistant is only configured to help with job writing. Later\\nversions of the assistant may be rolled out to other pages.\\n\\nAll chat sessions are shared between all users of the project. You can start a\\nnew chat session at any time, or open an old one.\\n\\nInput data and run-time job logs not sent to the model. But we do include\\nyour step code so that the Assistant can provide a contextually relevant answer.## A Note on Responsible AI Usage\\n\\nThe AI assistant is built on emerging Large Language Model (terminology). Like\\nother LLMS and chatbots, its capabilities are impressive, but imperfect.\\n\\nRemember that ultimately, all responses are generated by an algorithm and YOU,\\nthe human in charge, are responsible for how its output is used. You should\\nconsider all responses critically and verify the output where possible.\\n\\nYou can read more about our approach to AI in our\\n[Responsible AI Policy](https://www.openfn.org/ai).'),\n",
       "  0.71508038)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pc_db.similarity_search_with_score(\n",
    "            query=query,\n",
    "            k=10,\n",
    "            # score_threshold=0.5\n",
    "            # search_kwargs={\"score_threshold\": 0.5}\n",
    "        )\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_title': 'asana', 'docs_type': 'adaptor_docs'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(text='---\\ntitle: Asana Adaptor\\n---## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\", metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = docsite_search.search(\"what's asana\", top_k=10, docs_type='adaptor_docs', doc_title='asana')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(text='---\\ntitle: Asana Adaptor\\n---## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\", metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"Learn more about how data is structured in Salesforce at the links below:\\n\\n- How data is organized in Salesforce:\\n  https://help.salesforce.com/s/articleView?id=sf.basics_organize_data.htm&type=5\\n- Salesforce objects: https://www.salesforcetutorial.com/salesforce-objects/## Integration Use Cases\\n\\nSalesforce may be used to manage an organization's programs, operations,\\nfundraising, and more. Therefore integrations with 3rd-party apps is a common\\nrequirement.\\n\\nExample user stories:\\n\\n- As a program or M&E manager working in areas with limited internet\\n  connectivity, I would like to collect data on a mobile applicaiton that\\n  supports offline data capture, but then sync the data collected to Salesforce\\n  so that I can centrally monitor field activities and analyze data collected to\\n  evaluate program impact.\\n- As a fundraiser at an NGO, I would like to see all donor information tracked\\n  in Salesforce so that I can better monitor fundraising activities, nurture\\n  relationships with all funders, and manage campaigns.\", metadata={'doc_title': 'salesforce', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='This versatile platform caters\\nto a diverse array of needs, serving researchers, NGOs, government agencies, and\\nother entities seeking reliable data collection solutions.## Integration Use Cases:\\n\\nAs SurveyCTO is primarily a mobile data collection tool, often that data needs\\nto be extracted, managed, summarized, and analysed in another system (e.g.,\\ndatabase/data warehouse, analytics tools, and \"MIS\"/data management software\\nlike like DHIS2, Salesforce, etc.).\\n\\nExample user story:\\n\\n- As a program or M&E manager, I would like my field officers to collect data in\\n  places with limited connection availability using SurveyCTO. I want the data\\n  collected to automatically sync to my Tabluea dashboard for visualization and\\n  further analysis.', metadata={'doc_title': 'surveycto', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"---\\ntitle: Kobo Toolbox\\n---## App Overview\\n\\n[Kobo Toolbox](https://www.kobotoolbox.org/) is a suite of open source tools for\\nfield data collection for use in challenging environments. If you've worked on\\nODK or ONA, the underlying tech is very similar. They offer free accounts and\\nhosting for humanitarian projects, and the app provides a nice interface for\\nmanaging (and cleaning!) form submissions.\\n\\n:::note\\n\\nTool docs are (1) to ensure all OpenFn can more quickly and easily integrate\\nwith common tools, and (2) to educate any OpenFn user/the wider sector.\\n\\n:::## Data Model\\n\\nKobo data is collected using `projects` or `forms` which are the actual surveys/\\nquestions being adminsitered. Form responses are collected as individual\\n`form submissions` (1 row for every form submitted is logged in the Kobo web\\napp, viewable via the `Data` menu option).\", metadata={'doc_title': 'kobotoolbox', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"- As a fundraiser at an NGO, I would like to see all donor information tracked\\n  in Salesforce so that I can better monitor fundraising activities, nurture\\n  relationships with all funders, and manage campaigns.## APIs & Integration Options\\n\\nSalesforce has a rich ecosystem of developers and ready-made applications. See\\nthe [Salesforce App Exchange](https://appexchange.salesforce.com/) for existing\\napps and integrations custom-made for Salesforce. If existing applications do\\nnot meet your functional or budget requirements, leverage Salesforce's robust\\nAPIs to configure a custom integration.\\n\\n1. **APIs:** Salesforce has a robust set of RESTful APIs that support a wide\\n   range of operations. For connecting with these APIs, including the\\n   `Bulk API`, OpenFn has developed a robust API adaptor for quicker integration\\n   setup - see\\n   [`language-salesforce`](https://github.com/OpenFn/language-salesforce).\\n2. **Webhook:** By configuring\\n   [`Outbound Messages`](https://developer.salesforce.com/docs/atlas.en-us.api.meta/api/sforce_api_om_outboundmessaging_understanding.htm)\\n   that can be sent via criteria-based `Workflow Rules`, Salesforce Admins can\", metadata={'doc_title': 'salesforce', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"---\\ntitle: CKAN\\n---## App Overview\\n\\nCKAN (Comprehensive Knowledge Archive Network) is the world’s leading\\nopen-source data portal platform. CKAN makes it easy to publish, share and work\\nwith data. It's a data management system that provides a powerful platform for\\ncataloging, storing and accessing datasets with a rich front-end, full API (for\\nboth data and catalog), visualization tools and more.\\n\\nCKAN is a tool for making open data websites. It helps you manage and publish\\ncollections of data. It is used by national and local governments, research\\ninstitutions, and other organizations who collect a lot of data. See the public\\ndocumentation for more:\\n\\n- https://ckan.org/\\n- https://docs.ckan.org/en/2.9/user-guide.html#what-is-ckan\", metadata={'doc_title': 'ckan', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='---\\ntitle: Salesforce\\n---## App Overview\\n\\n[Salesforce](https://www.salesforce.com/) is a cloud-based **customer\\nrelationship management (CRM)** platform that hosts applications that customers\\ncan access online. Beyond its core CRM product, Salesforce offers a customizable\\nplatform for configuring relational databases, business automation, web portals,\\nreporting tools, and robust applications for supporting a wide range of use\\ncases.### Data Model\\n\\nAt its core, Salesforce is a relational database. It has some out-of-box or\\n\"standard\" data tables and features, but can be easily extended to include\\n\"custom\" metadata configuration and other app features.\\n\\nSalesforce data is stored in **individual records (rows)** and organized within\\n**objects (tables)**. Record attributes are captured in **fields (columns)**.\\nThe data model is configurable, but there are some standard objects that are\\nprovided. Note that naming conventions for custom and standard metadata may\\ndiffer (e.g., all \"custom\" field names include the suffix `__c` like\\n`CustomField__c`).\\n\\nLearn more about how data is structured in Salesforce at the links below:\\n\\n- How data is organized in Salesforce:\\n  https://help.salesforce.com/s/articleView?id=sf.basics_organize_data.htm&type=5\\n- Salesforce objects: https://www.salesforcetutorial.com/salesforce-objects/', metadata={'doc_title': 'salesforce', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='In the following sections, special systems will be described.#### Example user stories\\n\\n1 Logistics Management(LMIS)\\n\\n- LMIS is an area where a multitude of parallel, overlapping or competing\\n  software solutions can be found in a single country\\n- Although a basic LMIS configuration based on aggregate data can take you very\\n  far, in some cases a transactional LMIS is necessary if you need to track such\\n  detailed operations as returns, transfer between facilities, barcode reading,\\n  batch and expiry managemen\\n- In such a situation...\\n\\n2 Data Sharing for Health and Nutrition, Water Sanitation and Hygiene Projects\\n\\n- Case management sytsems such as CommCare are widely preffered in collecting\\n  case data(or patient level data) due to its dominance in the sector and easy\\n  of adoption. In such scenarios, ...\\n\\n3 DHIS2 Instance Synchronization\\n\\n- Different DHIS2 instances in a given organisation or government ministry may\\n  be deployed on separate servers which places the need for synchronization in', metadata={'doc_title': 'dhis2', 'docs_type': 'adaptor_docs'}, score=None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = docsite_search.search(\"what's asana\", top_k=10, docs_type='adaptor_docs')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['adaptor_functions_chunks', 'adaptor_docs_chunks', 'general_docs_chunks'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "folder = \"/Users/hanna/openfn/ai_experiments/apollo/services/tmp/split_sections\"\n",
    "files = [\"adaptor_functions_chunks.json\", \"adaptor_docs_chunks.json\", \"general_docs_chunks.json\"]\n",
    "doc_dict = {}\n",
    "for file in files:\n",
    "    with open(f\"{folder}/{file}\") as f:\n",
    "        data = json.load(f)\n",
    "        doc_dict[file[:-5]] = data\n",
    "doc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptor_functions_chunks\n",
      "315\n",
      "adaptor_docs_chunks\n",
      "171\n",
      "general_docs_chunks\n",
      "596\n"
     ]
    }
   ],
   "source": [
    "for d in doc_dict.keys():\n",
    "    print(f\"{d}\")\n",
    "    print(len(doc_dict[d]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:GitHubUtils:Fetched 38 URLs from GitHub for https://api.github.com/repos/OpenFn/docs/contents/adaptors\n",
      "INFO:GitHubUtils:Downloaded and processed 38 files from GitHub\n",
      "INFO:GitHubUtils:{'name': 'asana.md', 'docs': '---\\ntitle: Asana Adaptor\\n---\\n\\n## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks. \\n\\n## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.\\n\\n## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor\\'s [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\\n\\n```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```\\n\\n### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)\\n\\n### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)\\n\\n\\n\\n\\n'}\n",
      "INFO:GitHubUtils:Fetched 82 URLs from GitHub for https://api.github.com/repos/OpenFn/docs/contents/docs\n",
      "INFO:GitHubUtils:Downloaded and processed 82 files from GitHub\n",
      "INFO:GitHubUtils:{'name': 'build-compliant-apps.md', 'docs': '---\\ntitle: Developing connected applications\\nsidebar_label: Building compliant APIs\\n---\\n\\nThis section is for you if you are hoping to build or extend an existing\\napplication that can connect to OpenFn. We follow modern web-standard JSON API\\nguidelines.\\n\\nFor your application to a be data provider (or \"source\") for OpenFn\\nintegrations, we highly recommend that you create a \"notifications service\"\\n(sometimes called a \"webhooks service\" or \"event-based push API\"). This is\\npreferable to using a REST api for two reasons: (1) A notifications service will\\ngive your clients the ability to set up real-time integrations, and (2) a\\nnotifications service is more efficient for both your servers and OpenFn—instead\\nof having requests be made and handled every X seconds, your servers and\\nOpenFn\\'s servers will only work when new data is available.\\n\\nFor your application to be a consumer (or \"destination\") for OpenFn, you must\\neither have a standard, JSON-based REST API or create a language-package that\\nmeets your API specifications.\\n\\n## Sending data to OpenFn\\n\\nTo send data to OpenFn, your application must be able to make an HTTPS post to\\nan external URL with a valid JSON object as the post body. See the following\\nexample using cURL:\\n\\n```sh\\ncurl -X POST \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"Cache-Control: no-cache\" \\\\\\n  -d \\'{\"foo\":\"bar\", \"baz\":\"qux\"}\\' \\\\\\n  \"https://app.openfn.org/i/some-secret-inbox-uuid\"\\n```\\n\\nOpenFn will respond with a 200 and an empty JSON object in the event of a\\nsuccessful post. 400s mean that the user\\'s external URL is wrong, and 500s means\\nthat there is an application error on OpenFn. While 500s are rare, they could be\\ndue to invalid JSON in your POST body.\\n\\nIf you cannot notify an external URL when some event takes place, you can still\\nintegrate with OpenFn if you have a JSON-based REST API. OpenFn users can make\\nHTTP GET requests to your application and perform additional actions based on\\nyour response. You should allow either basic or token authentication and\\nresponse to a valid GET with JSON. There is no specific format for your\\nresponse, as users can parse it any way they\\'d like, extracting relevant data\\nand then performing other actions—like loading it into a destination system—with\\nthat data. See [language-http](https://www.github.com/openfn/language-http) for\\ndetails on how users make these generic HTTP requests.\\n\\n### Payload sizing\\n\\nIf you\\'re using the platform, and you\\'re not planning on using an enterprise\\nplan you\\'ll have to consider the size of the data you\\'re sending in each\\npayload. Run `state` is typically limited to `10MB` and you should therefore\\nkeep your payloads well below that limit.\\n\\n## Receiving data from OpenFn\\n\\nTo make it easy for users to connect to your application, it\\'s highly\\nrecommended that you create a language-package with your required authentication\\nand a set of simple, allowable actions nicely abstracted into \"helper\\nfunctions\". See [language-dhis2](https://www.github.com/openfn/language-dhis2)\\nfor an example of a language-package which creates a simpler interface for a\\ntraditional JSON-based REST api. Adaptors are written in Javascript and execute\\nin Node. You can convert OpenFn\\'s JSON into XML, or any other format before\\nsending it to your application and you may make use of any node modules you\\'d\\nlike. See\\n[language-postgresql](https://www.github.com/openfn/language-postgresql) for an\\nexample of an adaptor that connects directly to PostgreSQL databases using a\\npopular NPM module called \"pg\".\\n\\nTo receive data from OpenFn\\'s generic `language-http` adaptor, your application\\nmust allow either basic, token, or digest authenticated POST, PUT, or GET\\nrequests. (Though it is not advisable to create an API that requires GET\\nrequests to create or update data.)\\n'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'build-compliant-apps.md',\n",
       "  'docs': '---\\ntitle: Developing connected applications\\nsidebar_label: Building compliant APIs\\n---\\n\\nThis section is for you if you are hoping to build or extend an existing\\napplication that can connect to OpenFn. We follow modern web-standard JSON API\\nguidelines.\\n\\nFor your application to a be data provider (or \"source\") for OpenFn\\nintegrations, we highly recommend that you create a \"notifications service\"\\n(sometimes called a \"webhooks service\" or \"event-based push API\"). This is\\npreferable to using a REST api for two reasons: (1) A notifications service will\\ngive your clients the ability to set up real-time integrations, and (2) a\\nnotifications service is more efficient for both your servers and OpenFn—instead\\nof having requests be made and handled every X seconds, your servers and\\nOpenFn\\'s servers will only work when new data is available.\\n\\nFor your application to be a consumer (or \"destination\") for OpenFn, you must\\neither have a standard, JSON-based REST API or create a language-package that\\nmeets your API specifications.\\n\\n## Sending data to OpenFn\\n\\nTo send data to OpenFn, your application must be able to make an HTTPS post to\\nan external URL with a valid JSON object as the post body. See the following\\nexample using cURL:\\n\\n```sh\\ncurl -X POST \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"Cache-Control: no-cache\" \\\\\\n  -d \\'{\"foo\":\"bar\", \"baz\":\"qux\"}\\' \\\\\\n  \"https://app.openfn.org/i/some-secret-inbox-uuid\"\\n```\\n\\nOpenFn will respond with a 200 and an empty JSON object in the event of a\\nsuccessful post. 400s mean that the user\\'s external URL is wrong, and 500s means\\nthat there is an application error on OpenFn. While 500s are rare, they could be\\ndue to invalid JSON in your POST body.\\n\\nIf you cannot notify an external URL when some event takes place, you can still\\nintegrate with OpenFn if you have a JSON-based REST API. OpenFn users can make\\nHTTP GET requests to your application and perform additional actions based on\\nyour response. You should allow either basic or token authentication and\\nresponse to a valid GET with JSON. There is no specific format for your\\nresponse, as users can parse it any way they\\'d like, extracting relevant data\\nand then performing other actions—like loading it into a destination system—with\\nthat data. See [language-http](https://www.github.com/openfn/language-http) for\\ndetails on how users make these generic HTTP requests.\\n\\n### Payload sizing\\n\\nIf you\\'re using the platform, and you\\'re not planning on using an enterprise\\nplan you\\'ll have to consider the size of the data you\\'re sending in each\\npayload. Run `state` is typically limited to `10MB` and you should therefore\\nkeep your payloads well below that limit.\\n\\n## Receiving data from OpenFn\\n\\nTo make it easy for users to connect to your application, it\\'s highly\\nrecommended that you create a language-package with your required authentication\\nand a set of simple, allowable actions nicely abstracted into \"helper\\nfunctions\". See [language-dhis2](https://www.github.com/openfn/language-dhis2)\\nfor an example of a language-package which creates a simpler interface for a\\ntraditional JSON-based REST api. Adaptors are written in Javascript and execute\\nin Node. You can convert OpenFn\\'s JSON into XML, or any other format before\\nsending it to your application and you may make use of any node modules you\\'d\\nlike. See\\n[language-postgresql](https://www.github.com/openfn/language-postgresql) for an\\nexample of an adaptor that connects directly to PostgreSQL databases using a\\npopular NPM module called \"pg\".\\n\\nTo receive data from OpenFn\\'s generic `language-http` adaptor, your application\\nmust allow either basic, token, or digest authenticated POST, PUT, or GET\\nrequests. (Though it is not advisable to create an API that requires GET\\nrequests to create or update data.)\\n'},\n",
       " {'name': 'build-with-api.md',\n",
       "  'docs': '---\\nid: build-with-api\\ntitle: Build Projects via the OpenFn API\\nsidebar_label: Build with the API\\nslug: /build-with-api\\n---\\n\\nOpenFn offers the ability to configure projects via HTTP requests sent to the REST API. See the [OpenFn/Lightning GitHub repo](https://openfn.github.io/lightning/provisioning.html) for detailed developer documentation. \\n'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc_types = [\"adaptor_functions\", \"adaptor_docs\", \"general_docs\"]\n",
    "# doc_types = [\"adaptor_functions\"]\n",
    "doc_dict = {}\n",
    "for d in doc_types:\n",
    "    doc_list = github_utils.get_docs(docs_type=d)\n",
    "    doc_dict[d] = doc_list\n",
    "\n",
    "doc_dict[d][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build-compliant-apps',\n",
       " 'build-with-api',\n",
       " 'cli-challenges',\n",
       " 'cli-collections',\n",
       " 'cli-intro']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genral_docs_topics = [d[\"name\"][:-3] for d in doc_dict[\"general_docs\"]]\n",
    "genral_docs_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['asana', 'cht', 'ckan', 'collections', 'commcare']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptor_docs_topics = [d[\"name\"][:-3] for d in doc_dict[\"adaptor_docs\"]]\n",
    "print(len(adaptor_docs_topics))\n",
    "adaptor_docs_topics[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Test SQLite for searching by column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_question = \"How to inverse columns in javascript\"\n",
    "# user_question = \"Given a JSON data, and given a list of columns in an sql table, give me an insert query\"\n",
    "# user_question = \"\"\"write a job for me that creates new datavaluesets under the \"Approved School CHP\" organization unit\"\"\"\n",
    "# user_question = \"\"\"How to get data from asana in my job\"\"\"\n",
    "# user_question = \"What's the difference between a Trigger and a Step in a workflow?\"\n",
    "user_question = \"“Can I write a step that allows for creation of an JSON file of the transformed records that can be sent to an email or Google Drive”\"\n",
    "# user_question = \"Why does this part of the code only POST the first submission in the submissions[] array? I want to POST _each_ submission. each( '$.submissions[*]', post(URL, { body: () => {} ))”\"\n",
    "# user_question = \"write a job to fetch for an attribute in the input and map it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs docs T/F ?\n",
    "\n",
    "needs_docs_system_prompt = \"\"\"\n",
    "You are an assistant for a solutions engineer helping a user write a job for our platform.\n",
    "Our platform is OpenFn (Open Function Group), the world's leading digital public good for workflow automation.\n",
    "Your job is to decide whether the user question requires consulting our documentation. If the question is\n",
    "about general coding advice or other external information, we do not need to consult the documentation. \n",
    "\n",
    "Answer nothing but True or False.\n",
    "\"\"\"\n",
    "\n",
    "needs_docs_user_prompt = \"\"\"The user question is as follows: \"{user_question}\" \"\"\"\n",
    "\n",
    "def needs_docs(user_question):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\", # TODO change to cheaper model\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=needs_docs_system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": needs_docs_user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which search AND how?\n",
    "\n",
    "# system_prompt = \"\"\"\n",
    "# You are an assistant for a solutions engineer helping a user write a job for our platform.\n",
    "# Our platform is OpenFn (Open Function Group), the world's leading digital public good for workflow automation.\n",
    "# Your job is to query the documenation based on the user's question.  \n",
    "\n",
    "# Your answer should detail one or more searches that should be done on the documentation.\n",
    "# The options for search types are: \n",
    "# a) semantic search \n",
    "# b) direct retrieval by section (options: \"adaptor_documentation\", \"general_documentation\")\n",
    "\n",
    "# Answer in nothing but code, using one or both of these functions, one or more times, filling in an appropriate input: \n",
    "\n",
    "# semantic_search(query)\n",
    "# database_search(documentation_section)\n",
    "# \"\"\"\n",
    "#  ------------------------------\n",
    "# system_prompt = \"\"\"\n",
    "# You are an assistant for a solutions engineer helping a user write a job for our platform.\n",
    "# Our platform is OpenFn (Open Function Group), the world's leading digital public good for workflow automation.\n",
    "# Your job is to query the documenation based on the user's question.  \n",
    "\n",
    "# Your answer should detail one or more searches that should be done on the documentation.\n",
    "# The options for search types are: \n",
    "# a) semantic search \n",
    "# b) direct retrieval by section (options: \"adaptor_documentation\", \"general_documentation\")\n",
    "\n",
    "# You can use one or both of these functions, one or more times: \n",
    "\n",
    "# semantic_search(query)\n",
    "# database_search(documentation_section_query)\n",
    "\n",
    "# list which ones you would like to use in dictionaries on new lines:\n",
    "# {\"function\": \"semantic_search\", \"query\": \"your_query\"}\n",
    "# {\"function\":\"database_search\", \"query\": \"adaptor_documentation\"}\n",
    "\n",
    "# Return nothing but this list of dictionaries with \"function\" and \"query\" keys.\n",
    "# \"\"\"\n",
    "#  ------------------------------\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant for a solutions engineer helping a user write a job for our platform.\n",
    "Our platform is Open Function, a platform for workflow automation.\n",
    "Your job is to run search queries on the documentation based on the user's question.  \n",
    "\n",
    "Your answer should detail the searches that should be done on the documentation.\n",
    "Stick to one search query only per topic. If the user question requires information across\n",
    "distinct topics (e.g. two different adaptors; CLI and API instructions), then list additional queries.\n",
    "\n",
    "A search consists of a query string, and an optional filter for doc_type. \n",
    "This can be either adaptor_docs or general_docs.\n",
    "If the filter is adaptor_documentation, then the query should be just the name of the adaptor.\n",
    "\n",
    "Return a JSON array of search queries:\n",
    "[\n",
    "  {\"query\": \"your_query\", \"doc_type\": null},\n",
    "  {\"query\": \"optional_second_query\", \"doc_type\": \"adaptor_docs\"}\n",
    "]\n",
    "\n",
    "Return NOTHING but this JSON array.\n",
    "\"\"\"\n",
    "#  ------------------------------\n",
    "\n",
    "user_prompt = \"\"\"The user question is as follows: \"{user_question}\" \"\"\"\n",
    "\n",
    "# # fucnution plus query\n",
    "# response_schemas = [\n",
    "#     ResponseSchema(name=\"function\", description=\"The function to be called\"),\n",
    "#     ResponseSchema(name=\"query\", description=\"The query for the function\")\n",
    "# ]\n",
    "\n",
    "# parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# # query plus filter\n",
    "# response_schemas = [\n",
    "#     ResponseSchema(name=\"query\", description=\"The function to be called\"),\n",
    "#     ResponseSchema(name=\"doc_type\", description=\"The doc_type filter for the function\")\n",
    "# ]\n",
    "\n",
    "# parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "def search_docs(user_question):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\", # TODO change to cheaper model\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    text = message.content[0].text\n",
    "    # print(\"answer: \", text)\n",
    "    # answer_parsed = [parser.parse(a) for a in text.split(\"\\n\")]\n",
    "    answer_parsed = json.loads(text)\n",
    "\n",
    "    return answer_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{\"function\": \"semantic_search\", \"query\": \"difference between trigger and step in workflow\"}\n",
      "{\"function\": \"database_search\", \"query\": \"general_documentation\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'function': 'semantic_search',\n",
       "  'query': 'difference between trigger and step in workflow'},\n",
       " {'function': 'database_search', 'query': 'general_documentation'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer = search_docs(user_question)\n",
    "# print(answer)\n",
    "# answer_parsed = [parser.parse(a) for a in answer.split(\"\\n\")]\n",
    "# answer_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "answer:  [\n",
      "  {\"query\": \"dhis2\", \"doc_type\": \"adaptor_documentation\"},\n",
      "  {\"query\": \"api token authentication\", \"doc_type\": null}\n",
      "]\n",
      "[{'query': 'dhis2', 'doc_type': 'adaptor_documentation'}, {'query': 'api token authentication', 'doc_type': None}]\n"
     ]
    }
   ],
   "source": [
    "answer = search_docs(user_question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(text='---\\ntitle: Asana Adaptor\\n---## About Asana\\n\\n[Asana](https://app.asana.com/) is a web-based project management tool that helps teams organize, plan, collaborate, and execute tasks.## Integration Options\\n\\nAsana supports 2 primary integration options:\\n\\n1. Rest API: Asana has an available REST API that enable external services like OpenFn to pull data from Asana, or push data from external apps to Asana. This option is suited for scheduled, bulk syncs or workflows that must update data in Asana with external information. See [functions](/adaptors/packages/asana-docs) for more on how to use this adaptor to work with the API.\\n\\n2. Webhook: Asana also has a [Webhook or Data Forwarding](https://developers.asana.com/docs/webhooks-guide) to push data from Asana to external systems. This option is suited for real-time, event-based data integration. Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='If you capture these 3 elements, user stories can b ean effective way of\\ndetailing integration requirements and starting discussions at your organization\\nabout which requirements are priority.### Example user stories:\\n\\n- **Case Referrals:** As a caseworker, I want to automatically send referral\\n  requests to my partner agency using another case management system, so that I\\n  can securely share case information and quickly notify them when their\\n  services are needed in a crisis situation.\\n- **EMR - HIS:** As a clinic manager, I would like to integrate patient data\\n  from the district clinic electronic medical record system with the national\\n  DHIS2 health information system, so that I can securely and automatically\\n  report on health outcomes for key indicators in my district.\\n- **Kobo Toolbox - MSSQL Database:** As a M&E manager, I want to monitor Kobo\\n  Toolbox survey responses in a central database in real-time, so that I can\\n  better understand data collection activities and program performance across my\\n  research partner sites.', metadata={'doc_title': 'overview', 'docs_type': 'general_docs'}, score=None),\n",
       " SearchResult(text='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"Check out the Asana [devloper documentation](/adaptors/packages/asana-docs) to learn how to set up a webhook to push data to OpenFn.## Authentication\\n\\nSee [Asana docs](https://developers.asana.com/docs/authentication) for the latest on supported authentication methods. \\n\\nWhen integrating with Asana via OpenFn, there is one primary authentication method supported: **Personal Access Token (PAT)**. You can generate a personal access token from the Asana [developer console](https://developers.asana.com/docs/personal-access-token).\\n\\nSee this adaptor's [Configuration docs](/adaptors/packages/asana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:\", metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='Then, we load\\n(send it to the destination).## Integration platform\\n\\nAn integration platform (e.g., OpenFn) is an application (or set of\\napplications) that help organizations set up, run, and maintain/manage the\\nintegrations between all of their various systems.### iPaaS\\n\\nYou may also see the acronym \"iPaaS\". This stands for integration platform as a\\nservice and is a type of \"software as a service\" (or \"SaaS\"). SaaS is a software\\npurchasing model in which software is paid for only as it is used (often\\nmonth-to-month), rather than purchased up front or given away for free.## Metadata\\n\\nThis is data that tells us about our data. In a table, for example, that\\'s the\\nname of the columns, the number of rows, etc. Metadata is often brought up in\\nconversations about privacy—e.g., regulators might want to ensure that _only\\nmetadata_ is moved from Ministry A to Ministry B, as opposed to personally\\nidentifiable information (PII) about individuals themselves.', metadata={'doc_title': 'glossary', 'docs_type': 'general_docs'}, score=None)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get adaptor doc by title or rever to semantic search?\n",
    "def get_adaptor_documentation(query):\n",
    "   if query.lower() in adaptor_docs_topics:\n",
    "        target_section = query.lower()\n",
    "   else:\n",
    "    #  search docsite-topics\n",
    "    #  target_section = search_result\n",
    "    # result = fetch_local(doc=target_section)\n",
    "    \n",
    "    return result\n",
    "\n",
    "search_results = []\n",
    "for q in answer:\n",
    "    query_text = q.get(\"query\", \"\")\n",
    "    if query_text:\n",
    "        doc_type = q.get(\"doc_type\", None)\n",
    "        if doc_type == \"adaptor_documentation\":\n",
    "          results = get_adaptor_documentation(query_text)  \n",
    "        # results = docsite_search.search(query_text, top_k=5, docs_type=doc_type)\n",
    "        search_results.extend(results)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs docs?\n",
    "# parallel a: should we fetch one of more of the following docs sections?\n",
    "# parallel b: llm: run semantic search\n",
    "# dedup & combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick from topics\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant for a solutions engineer helping a user write a job for our platform.\n",
    "Our platform is OpenFn (Open Function Group), the world's leading digital public good for workflow automation.\n",
    "Your job is to query the documentation based on the user's question.  \n",
    "\n",
    "Your answer should pick titles to retreive from the list below. Where possible, stick to only one title.\n",
    "\n",
    "Titles to pick from:\n",
    "{document_titles}\n",
    "\n",
    "Answer in nothing but the list of titles on new lines.\n",
    "\"\"\"\n",
    "\n",
    "#  ------------------------------\n",
    "\n",
    "user_prompt = \"\"\"The user question is as follows: \"{user_question}\" \"\"\"\n",
    "\n",
    "def select_title(user_question, document_titles):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\", # TODO change to cheaper model\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt.format(document_titles=document_titles),\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_prompt.format(user_question=user_question)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    text = message.content[0].text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http\\nsftp'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_title(user_question, adaptor_docs_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile test set\n",
    "test_questions = []\n",
    "test_types = [\"platform_questions\", \"specific_coding_questions\", \"workflow_overview_questions\"]\n",
    "for test_type, test in zip(test_types, [platform_questions, specific_coding_questions, workflow_overview_questions]):\n",
    "    questions = [t for t in test.split(\"\\n\")[:10] if t] #TODO limiting size now\n",
    "    for q in questions:\n",
    "        test_questions.append({\"question_type\":test_type, \"question\":q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from anthropic import (\n",
    "    Anthropic,\n",
    "    APIConnectionError,\n",
    "    BadRequestError,\n",
    "    AuthenticationError,\n",
    "    PermissionDeniedError,\n",
    "    NotFoundError,\n",
    "    UnprocessableEntityError,\n",
    "    RateLimitError,\n",
    "    InternalServerError,\n",
    ")\n",
    "from util import ApolloError, create_logger\n",
    "# from .prompt import build_prompt\n",
    "from job_chat.prompt import build_prompt\n",
    "\n",
    "logger = create_logger(\"job_chat\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Payload:\n",
    "    \"\"\"\n",
    "    Data class for validating and storing input parameters.\n",
    "    Required fields will raise TypeError if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    content: str\n",
    "    context: Optional[str] = None\n",
    "    api_key: Optional[str] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, Any]) -> \"Payload\":\n",
    "        \"\"\"\n",
    "        Create a Payload instance from a dictionary, validating required fields.\n",
    "        \"\"\"\n",
    "        if \"content\" not in data:\n",
    "            raise ValueError(\"'content' is required\")\n",
    "\n",
    "        return cls(content=data[\"content\"], context=data.get(\"context\"), api_key=data.get(\"api_key\"))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChatConfig:\n",
    "    model: str = \"claude-3-5-sonnet-20240620\"\n",
    "    max_tokens: int = 1024\n",
    "    api_key: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChatResponse:\n",
    "    content: str\n",
    "    history: List[Dict[str, str]]\n",
    "    usage: Dict[str, Any]\n",
    "\n",
    "\n",
    "class AnthropicClient:\n",
    "    def __init__(self, config: Optional[ChatConfig] = None):\n",
    "        self.config = config or ChatConfig()\n",
    "        self.api_key = self.config.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"API key must be provided\")\n",
    "        self.client = Anthropic(api_key=self.api_key)\n",
    "\n",
    "    def generate(\n",
    "        self, content: str, history: Optional[List[Dict[str, str]]] = None, context: Optional[str] = None\n",
    "    ) -> ChatResponse:\n",
    "        \"\"\"\n",
    "        Generate a response using the Claude API with improved error handling and response processing.\n",
    "        \"\"\"\n",
    "        history = history.copy() if history else []\n",
    "\n",
    "        system_message, prompt = build_prompt(content, history, context)\n",
    "\n",
    "        message = self.client.beta.prompt_caching.messages.create(\n",
    "            max_tokens=self.config.max_tokens, messages=prompt, model=self.config.model, system=system_message\n",
    "        )\n",
    "\n",
    "        if hasattr(message, \"usage\"):\n",
    "            if message.usage.cache_creation_input_tokens:\n",
    "                logger.info(f\"Cache write: {message.usage.cache_creation_input_tokens} tokens\")\n",
    "            if message.usage.cache_read_input_tokens:\n",
    "                logger.info(f\"Cache read: {message.usage.cache_read_input_tokens} tokens\")\n",
    "\n",
    "        response_parts = []\n",
    "        for content_block in message.content:\n",
    "            if content_block.type == \"text\":\n",
    "                response_parts.append(content_block.text)\n",
    "            else:\n",
    "                logger.warning(f\"Unhandled content type: {content_block.type}\")\n",
    "\n",
    "        response = \"\\n\\n\".join(response_parts)\n",
    "\n",
    "        updated_history = history + [\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "            {\"role\": \"assistant\", \"content\": response},\n",
    "        ]\n",
    "\n",
    "        return ChatResponse(\n",
    "            content=response,\n",
    "            history=updated_history,\n",
    "            usage=message.usage.model_dump() if hasattr(message, \"usage\") else {},\n",
    "        )\n",
    "\n",
    "\n",
    "def main(data_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Main entry point with improved error handling and input validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = Payload.from_dict(data_dict)\n",
    "\n",
    "        config = ChatConfig(api_key=data.api_key) if data.api_key else None\n",
    "        client = AnthropicClient(config)\n",
    "\n",
    "        result = client.generate(content=data.content, history=data_dict.get(\"history\", []), context=data.context)\n",
    "\n",
    "        return {\"response\": result.content, \"history\": result.history, \"usage\": result.usage}\n",
    "\n",
    "    except ValueError as e:\n",
    "        raise ApolloError(400, str(e), type=\"BAD_REQUEST\")\n",
    "\n",
    "    except APIConnectionError as e:\n",
    "        raise ApolloError(\n",
    "            503,\n",
    "            \"Unable to reach the Anthropic AI Service\",\n",
    "            type=\"CONNECTION_ERROR\",\n",
    "            details={\"cause\": str(e.__cause__)},\n",
    "        )\n",
    "    except AuthenticationError as e:\n",
    "        raise ApolloError(401, \"Authentication failed\", type=\"AUTH_ERROR\")\n",
    "    except RateLimitError as e:\n",
    "        raise ApolloError(\n",
    "            429, \"Rate limit exceeded, please try again later\", type=\"RATE_LIMIT\", details={\"retry_after\": 60}\n",
    "        )\n",
    "    except BadRequestError as e:\n",
    "        raise ApolloError(400, str(e), type=\"BAD_REQUEST\")\n",
    "    except PermissionDeniedError as e:\n",
    "        raise ApolloError(403, \"Not authorized to perform this action\", type=\"FORBIDDEN\")\n",
    "    except NotFoundError as e:\n",
    "        raise ApolloError(404, \"Resource not found\", type=\"NOT_FOUND\")\n",
    "    except UnprocessableEntityError as e:\n",
    "        raise ApolloError(422, str(e), type=\"INVALID_REQUEST\")\n",
    "    except InternalServerError as e:\n",
    "        raise ApolloError(500, \"The Anthropic AI Service encountered an error\", type=\"PROVIDER_ERROR\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during chat generation: {str(e)}\")\n",
    "        raise ApolloError(500, str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjob_chat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_prompt, Context\n\u001b[0;32m----> 3\u001b[0m apollo_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms asana\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43madaptor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m@openfn/language-http@2.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m apollo_prompt\n",
      "File \u001b[0;32m~/openfn/ai_experiments/apollo/services/job_chat/prompt.py:188\u001b[0m, in \u001b[0;36mbuild_prompt\u001b[0;34m(content, history, context)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_prompt\u001b[39m(content, history, context):\n\u001b[0;32m--> 188\u001b[0m     system_message \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_system_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m     prompt\u001b[38;5;241m.\u001b[39mextend(history)\n",
      "File \u001b[0;32m~/openfn/ai_experiments/apollo/services/job_chat/prompt.py:163\u001b[0m, in \u001b[0;36mgenerate_system_message\u001b[0;34m(context_dict)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m adaptor_docs:\n\u001b[1;32m    162\u001b[0m     adaptor_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypescript definitions for doc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 163\u001b[0m     adaptor_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43madaptor_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    164\u001b[0m adaptor_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</adaptor>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m message\u001b[38;5;241m.\u001b[39mappend(adaptor_string)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "from job_chat.prompt import build_prompt, Context\n",
    "\n",
    "apollo_prompt = build_prompt(content=\"What's asana\", history=None, context=Context(adaptor=\"@openfn/language-http@2.0.0\"))\n",
    "apollo_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_logger, apollo\n",
    "\n",
    "# logger = create_logger(\"job_chat.prompt\")\n",
    "\n",
    "system_role = \"\"\"\n",
    "You are a software engineer helping a non-expert user write a job for our platform.\n",
    "We are OpenFn (Open Function Group) the world's leading digital public good for workflow automation.\n",
    "\n",
    "Where reasonable, assume questions are related to workflow automation, \n",
    "professional platforms or programming. You may provide general information around these topics, \n",
    "e.g. general programming assistance unrelated to job writing.\n",
    "If a question is entirely irrelevant, do not answer it.\n",
    "\n",
    "You MUST keep your responses concise. Do not explain your answers unless\n",
    "the user explicitly asks you to. When generating code, always use the simplest\n",
    "possible code to achieve the task.\n",
    "\n",
    "Do not thank the user or be obsequious. Address the user directly.\n",
    "\n",
    "You are embedded in our app for building workflows. Our app will provide the\n",
    "history of each chat session to you. Our app will send you the user's code and\n",
    "tell you which adaptor (library) is being used. We will not send you the user's \n",
    "input data, output data, or logs, because they might contain sensitive information. \n",
    "Chat sessions are saved to each job, so any user who can see the workflow can see the chat.\n",
    "\n",
    "Your chat panel is embedded in a web based IDE, which lets users build a Workflow with a number\n",
    "of steps (or jobs). There is a code editor next to you, which users can copy and paste code into.\n",
    "Users must set or select an input in the Input tab, and can then run the current job.\n",
    "\n",
    "Users can Flag any answers that are not helpful, which will help us build a better prompt for you.\n",
    "\"\"\"\n",
    "\n",
    "job_writing_summary = \"\"\"\n",
    "<credential management>\n",
    "When writing jobs, users will use their own credentials to access different\n",
    "backend systems. The OpenFn app handles all credential management for them\n",
    "in a secure way.\n",
    "\n",
    "For more help direct them to https://docs.openfn.org/documentation/build/credentials\n",
    "\n",
    "Users must never add credentials into job code directly. If a user gives you an\n",
    "API key, password, access token, or other credential, you must reject it.\n",
    "</credential management>\n",
    "<job writing guide>\n",
    "An OpenFn Job is written in a DSL which is very similar to Javascript.\n",
    "\n",
    "Job code does not use import statements or async/await.\n",
    "\n",
    "Job code must only contain function calls at the top level.\n",
    "\n",
    "If the user is talking about collections, suggest this: \"For working with collections, refer to the official documentation here: https://docs.openfn.org/adaptors/packages/collections-docs.\".\n",
    "Avoid suggesting code to a user enquiring about collections or a single collection.\n",
    "\n",
    "Each job is associated with an adaptor, which provides functions for the job.\n",
    "All jobs have the fn() and each() function, which are very important.\n",
    "\n",
    "DO NOT use the `alterState()` function. Use `fn()` instead.\n",
    "\n",
    "The adaptor API may be attached.\n",
    "\n",
    "The functions provided by an adaptor are called Operations.\n",
    "Know that technically an Operation is a factory function which returns a function that takes state and returns state, like this:\n",
    "```js\n",
    "const myOperation = (arg) => (state) => { /* do something with arg and state */ return state; }\n",
    "```\n",
    "But the DSL presents these operations like simple functions. Users don't know it's a factory, they think it's a regular function.\n",
    "<examples>\n",
    "<example>\n",
    "Here's how we issue a GET request with the http adaptor:\n",
    "```\n",
    "get('/patients');\n",
    "```\n",
    "The first argument to get is the path to request from (the configuration will tell\n",
    "the adaptor what base url to use). In this case we're passing a static string,\n",
    "but we can also pass a value from state:\n",
    "```\n",
    "get(state => state.endpoint);\n",
    "```\n",
    "</example>\n",
    "<example>\n",
    "Example job code with the HTTP adaptor:\n",
    "```\n",
    "get('/patients');\n",
    "fn(state => {\n",
    "  const patients = state.data.map(p => {\n",
    "    return { ...p, enrolled: true }\n",
    "  });\n",
    "\n",
    "  return { ...state, data: { patients } };\n",
    "})\n",
    "post('/patients', dataValue('patients'));\n",
    "</example>\n",
    "<example>\n",
    "```\n",
    "Example job code with the Salesforce adaptor:\n",
    "```\n",
    "each(\n",
    "  '$.form.participants[*]',\n",
    "  upsert('Person__c', 'Participant_PID__c', state => ({\n",
    "    Participant_PID__c: state.pid,\n",
    "    First_Name__c: state.participant_first_name,\n",
    "    Surname__c: state.participant_surname,\n",
    "  }))\n",
    ");\n",
    "```\n",
    "</example>\n",
    "<example>\n",
    "Example job code with the ODK adaptor:\n",
    "```\n",
    "create(\n",
    "  'ODK_Submission__c',\n",
    "  fields(\n",
    "    field('Site_School_ID_Number__c', dataValue('school')),\n",
    "    field('Date_Completed__c', dataValue('date')),\n",
    "    field('comments__c', dataValue('comments')),\n",
    "    field('ODK_Key__c', dataValue('*meta-instance-id*'))\n",
    "  )\n",
    ");\n",
    "```\n",
    "</example>\n",
    "<examples>\n",
    "</job writing guide>\n",
    "<workflow guide>\n",
    "A job is just one step in a workflow (or pipeline). Workflows are used\n",
    "to automate processes and migrate data from system to system.\n",
    "\n",
    "In OpenFn, each step works with a single backend system, or adaptor. Data is shared\n",
    "between steps through the state object.\n",
    "\n",
    "To build a successful workflow, we have to take the user's problem and break it down\n",
    "step by step. Focus on one bit at a time. For example, when uploading from commcare to salesforce, we have to:\n",
    "1. Download our data from commcare in one step\n",
    "2. Transform/map data into salesforce format in another step (with the common adaptor)\n",
    "3. Upload the transformed data into salesforce in the final step\n",
    "</workflow guide>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Context:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def has(self, key):\n",
    "        return hasattr(self, key) and getattr(self, key) is not None\n",
    "\n",
    "\n",
    "def generate_system_message(context_dict):\n",
    "    context = context_dict if isinstance(context_dict, Context) else Context(**context_dict)\n",
    "\n",
    "    message = [system_role]\n",
    "    message.append(f\"<job_writing_guide>{job_writing_summary}</job_writing_guide>\")\n",
    "    message.append({\"type\": \"text\", \"text\": \".\", \"cache_control\": {\"type\": \"ephemeral\"}})\n",
    "\n",
    "    if context.has(\"adaptor\"):\n",
    "        adaptor_string = (\n",
    "            f\"<adaptor>The user is using the OpenFn {context.adaptor} adaptor. Use functions provided by its API.\"\n",
    "        )\n",
    "\n",
    "        adaptor_docs = apollo(\"describe_adaptor\", {\"adaptor\": context.adaptor})\n",
    "\n",
    "        for doc in adaptor_docs:\n",
    "            adaptor_string += f\"Typescript definitions for doc {doc}\"\n",
    "            adaptor_string += adaptor_docs[doc][\"description\"]\n",
    "        adaptor_string += \"</adaptor>\"\n",
    "\n",
    "        message.append(adaptor_string)\n",
    "    else:\n",
    "        message.append(\"The user is using an OpenFn Adaptor to write the job.\")\n",
    "\n",
    "    message.append({\"type\": \"text\", \"text\": \".\", \"cache_control\": {\"type\": \"ephemeral\"}})\n",
    "\n",
    "    if context.has(\"expression\"):\n",
    "        message.append(f\"<user_code>{context.expression}</user_code>\")\n",
    "\n",
    "    if context.has(\"input\"):\n",
    "        message.append(f\"<input>The user's input data is :\\n\\n```{context.input}```</input>\")\n",
    "\n",
    "    if context.has(\"output\"):\n",
    "        message.append(f\"<output>The user's last output data was :\\n\\n```{context.output}```</output>\")\n",
    "\n",
    "    if context.has(\"log\"):\n",
    "        message.append(f\"<log>The user's last log output was :\\n\\n```{context.log}```</log>\")\n",
    "\n",
    "    return list(map(lambda text: text if isinstance(text, dict) else {\"type\": \"text\", \"text\": text}, message))\n",
    "\n",
    "\n",
    "def build_prompt(content, history, context):\n",
    "    system_message = generate_system_message(context)\n",
    "\n",
    "    prompt = []\n",
    "    prompt.extend(history)\n",
    "    prompt.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    return (system_message, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<adaptor>The user is using the OpenFn @openfn/language-googlesheets@3.0.5 adaptor. Use functions provided by its API.Typescript definitions for doc @openfn/language-common/**\\n * Execute a sequence of operations.\\n * Main outer API for executing expressions.\\n * @public\\n * @example\\n *  execute(\\n *    create(\\'foo\\'),\\n *    delete(\\'bar\\')\\n *  )\\n * @private\\n * @param {Operations} operations - Operations to be performed.\\n * @returns {Promise}\\n */\\nexport function execute(...operations: Operations): Promise<any>;\\n/**\\n * alias for \"fn()\"\\n * @function\\n * @param {Function} func is the function\\n * @returns {Operation}\\n */\\nexport function alterState(func: Function): Operation;\\n/**\\n * Creates a custom step (or operation) for more flexible job writing.\\n * @public\\n * @function\\n * @example\\n * fn(state => {\\n *   // do some things to state\\n *   return state;\\n * });\\n * @param {Function} func is the function\\n * @returns {Operation}\\n */\\nexport function fn(func: Function): Operation;\\n/**\\n * A custom operation that will only execute the function if the condition returns true\\n * @public\\n * @function\\n * @example\\n * fnIf((state) => state?.data?.name, get(\"https://example.com\"));\\n * @param {Boolean} condition - The condition that returns true\\n * @param {Operation} operation - The operation needed to be executed.\\n * @returns {Operation}\\n */\\nexport function fnIf(condition: boolean, operation: Operation): Operation;\\n/**\\n * Picks out a single value from a JSON object.\\n * If a JSONPath returns more than one value for the reference, the first\\n * item will be returned.\\n * @public\\n * @function\\n * @example\\n * jsonValue({ a:1 }, \\'a\\')\\n * @param {object} obj - A valid JSON object.\\n * @param {String} path - JSONPath referencing a point in given JSON object.\\n * @returns {Operation}\\n */\\nexport function jsonValue(obj: object, path: string): Operation;\\n/**\\n * Picks out a single value from source data.\\n * If a JSONPath returns more than one value for the reference, the first\\n * item will be returned.\\n * @public\\n * @function\\n * @example\\n * sourceValue(\\'$.key\\')\\n * @param {String} path - JSONPath referencing a point in `state`.\\n * @returns {Operation}\\n */\\nexport function sourceValue(path: string): Operation;\\n/**\\n * Picks out a value from source data.\\n * Will return whatever JSONPath returns, which will always be an array.\\n * If you need a single value use `sourceValue` instead.\\n * @public\\n * @function\\n * @example\\n * source(\\'$.key\\')\\n * @param {String} path - JSONPath referencing a point in `state`.\\n * @returns {Array.<String|Object>}\\n */\\nexport function source(path: string): Array<string | any>;\\n/**\\n * Ensures a path points at the data.\\n * @public\\n * @function\\n * @example\\n * dataPath(\\'key\\')\\n * @param {string} path - JSONPath referencing a point in `data`.\\n * @returns {string}\\n */\\nexport function dataPath(path: string): string;\\n/**\\n * Picks out a single value from the source data object—usually `state.data`.\\n * If a JSONPath returns more than one value for the reference, the first\\n * item will be returned.\\n * @public\\n * @function\\n * @example\\n * dataValue(\\'key\\')\\n * @param {String} path - JSONPath referencing a point in `data`.\\n * @returns {Operation}\\n */\\nexport function dataValue(path: string): Operation;\\n/**\\n * Ensures a path points at references.\\n * @public\\n * @function\\n * @example\\n * referencePath(\\'key\\')\\n * @param {string} path - JSONPath referencing a point in `references`.\\n * @returns {string}\\n */\\nexport function referencePath(path: string): string;\\n/**\\n * Picks out the last reference value from source data.\\n * @public\\n * @function\\n * @example\\n * lastReferenceValue(\\'key\\')\\n * @param {String} path - JSONPath referencing a point in `references`.\\n * @returns {Operation}\\n */\\nexport function lastReferenceValue(path: string): Operation;\\n/**\\n * Simple switcher allowing other expressions to use either a JSONPath or\\n * object literals as a data source.\\n * - JSONPath referencing a point in `state`\\n * - Object Literal of the data itself.\\n * - Function to be called with state.\\n * @public\\n * @function\\n * @example\\n * asData(\\'$.key\\'| key | callback)\\n * @param {String|object|function} data\\n * @param {object} state - The current state.\\n * @returns {array}\\n */\\nexport function asData(data: string | object | Function, state: object): any[];\\n/**\\n * Iterates over an array of items and invokes an operation upon each one, where the state\\n * object is _scoped_ so that state.data is the item under iteration.\\n * The rest of the state object is untouched and can be referenced as usual.\\n * You can pass an array directly, or use lazy state or a JSONPath string to\\n * reference a slice of state.\\n * @public\\n * @function\\n * @example <caption>Using lazy state ($) to iterate over items in state.data and pass each into an \"insert\" operation</caption>\\n * each(\\n *   $.data,\\n *   // Inside the callback operation, `$.data` is scoped to the item under iteration\\n *   insert(\"patient\", {\\n *     patient_name: $.data.properties.case_name,\\n *     patient_id: $.data.case_id,\\n *   })\\n * );\\n * @example <caption>Iterate over items in state.data and pass each one into an \"insert\" operation</caption>\\n * each(\\n *   $.data,\\n *   insert(\"patient\", (state) => ({\\n *     patient_id: state.data.case_id,\\n *     ...state.data\\n *   }))\\n * );\\n * @example <caption>Using JSON path to iterate over items in state.data and pass each one into an \"insert\" operation</caption>\\n * each(\\n *   \"$.data[*]\",\\n *   insert(\"patient\", (state) => ({\\n *     patient_name: state.data.properties.case_name,\\n *     patient_id: state.data.case_id,\\n *   }))\\n * );\\n * @param {DataSource} dataSource - JSONPath referencing a point in `state`.\\n * @param {Operation} operation - The operation needed to be repeated.\\n * @returns {Operation}\\n */\\nexport function each(dataSource: DataSource, operation: Operation): Operation;\\n/**\\n * Combines two operations into one\\n * @public\\n * @function\\n * @example\\n * combine(\\n *   create(\\'foo\\'),\\n *   delete(\\'bar\\')\\n * )\\n * @param {Operations} operations - Operations to be performed.\\n * @returns {Operation}\\n */\\nexport function combine(...operations: Operations): Operation;\\n/**\\n * Adds data from a target object\\n * @public\\n * @function\\n * @example\\n * join(\\'$.key\\',\\'$.data\\',\\'newKey\\')\\n * @param {String} targetPath - Target path\\n * @param {String} sourcePath - Source path\\n * @param {String} targetKey - Target Key\\n * @returns {Operation}\\n */\\nexport function join(targetPath: string, sourcePath: string, targetKey: string): Operation;\\n/**\\n * Recursively resolves objects that have resolvable values (functions).\\n * @public\\n * @function\\n * @param {object} value - data\\n * @param {Function} [skipFilter] - a function which returns true if a value should be skipped\\n * @returns {Operation}\\n */\\nexport function expandReferences(value: object, skipFilter?: Function): Operation;\\n/**\\n * Returns a key, value pair in an array.\\n * @public\\n * @function\\n * @example\\n * field(\\'destination_field_name__c\\', \\'value\\')\\n * @param {string} key - Name of the field\\n * @param {Value} value - The value itself or a sourceable operation.\\n * @returns {Field}\\n */\\nexport function field(key: string, value: Value): Field;\\n/**\\n * Zips key value pairs into an object.\\n * @public\\n * @function\\n * @example\\n *  fields(list_of_fields)\\n * @param {Fields} fields - a list of fields\\n * @returns {Object}\\n */\\nexport function fields(...fields: Fields): any;\\n/**\\n * Merges fields into each item in an array.\\n * @public\\n * @example\\n * merge(\\n *   \"$.books[*]\",\\n *   fields(\\n *     field( \"publisher\", sourceValue(\"$.publisher\") )\\n *   )\\n * )\\n * @function\\n * @public\\n * @param {DataSource} dataSource\\n * @param {Object} fields - Group of fields to merge in.\\n * @returns {DataSource}\\n */\\nexport function merge(dataSource: DataSource, fields: any): DataSource;\\n/**\\n * Groups an array of objects by a specified key path.\\n * @public\\n * @example\\n * const users = [\\n *   { name: \\'Alice\\', age: 25, city: \\'New York\\' },\\n *   { name: \\'Bob\\', age: 30, city: \\'San Francisco\\' },\\n *   { name: \\'Charlie\\', age: 25, city: \\'New York\\' },\\n *   { name: \\'David\\', age: 30, city: \\'San Francisco\\' }\\n * ];\\n * group(users, \\'city\\');\\n * // state is { data: { \\'New York\\': [/Alice, Charlie/], \\'San Francisco\\': [ /Bob, David / ] }\\n * @function\\n * @public\\n * @param {Object[]} arrayOfObjects - The array of objects to be grouped.\\n * @param {string} keyPath - The key path to group by.\\n * @param {function} callback - (Optional) Callback function\\n * @returns {Operation}\\n */\\nexport function group(arrayOfObjects: any[], keyPath: string, callback?: Function): Operation;\\n/**\\n * Returns the index of the current array being iterated.\\n * To be used with `each` as a data source.\\n * @public\\n * @function\\n * @example\\n * index()\\n * @returns {DataSource}\\n */\\nexport function index(): DataSource;\\n/**\\n * Turns an array into a string, separated by X.\\n * @public\\n * @function\\n * @example\\n * field(\"destination_string__c\", function(state) {\\n *   return arrayToString(dataValue(\"path_of_array\")(state), \\', \\')\\n * })\\n * @param {array} arr - Array of toString\\'able primatives.\\n * @param {string} separator - Separator string.\\n * @returns {string}\\n */\\nexport function arrayToString(arr: any[], separator: string): string;\\n/**\\n * Ensures primitive data types are wrapped in an array.\\n * Does not affect array objects.\\n * @public\\n * @function\\n * @example\\n * each(function(state) {\\n *   return toArray( dataValue(\"path_of_array\")(state) )\\n * }, ...)\\n * @param {any} arg - Data required to be in an array\\n * @returns {array}\\n */\\nexport function toArray(arg: any): any[];\\n/**\\n * Prepares next state\\n * @public\\n * @function\\n * @example\\n * composeNextState(state, response)\\n * @param {State} state - state\\n * @param {Object} response - Response to be added\\n * @returns {State}\\n */\\nexport function composeNextState(state: State, response: any): State;\\n/**\\n * Substitutes underscores for spaces and proper-cases a string\\n * @public\\n * @function\\n * @example\\n * field(\"destination_string__c\", humanProper(state.data.path_to_string))\\n * @param {string} str - String that needs converting\\n * @returns {string}\\n */\\nexport function humanProper(str: string): string;\\n/**\\n * Splits an object into two objects based on a list of keys.\\n * The first object contains the keys that are not in the list,\\n * and the second contains the keys that are.\\n * @public\\n * @function\\n * @param {Object} obj - The object to split.\\n * @param {string[]} keys - List of keys to split on.\\n * @returns {Object[]} - Tuple of objects, first object contains keys not in list, second contains keys that are.\\n */\\nexport function splitKeys(obj: any, keys: string[]): any[];\\n/**\\n * Replaces emojis in a string.\\n * @public\\n * @function\\n * @example\\n * scrubEmojis(\\'Dove🕊️⭐ 29\\')\\n * @param {string} text - String that needs to be cleaned\\n * @param {string} replacementChars - Characters that replace the emojis\\n * @returns {string}\\n */\\nexport function scrubEmojis(text: string, replacementChars: string): string;\\n/**\\n * Chunks an array into an array of arrays, each with no more than a certain size.\\n * @public\\n * @function\\n * @example\\n * chunk([1,2,3,4,5], 2)\\n * @param {Object} array - Array to be chunked\\n * @param {Integer} chunkSize - The maxiumum size of each chunks\\n * @returns {Object}\\n */\\nexport function chunk(array: any, chunkSize: Integer): any;\\n/**\\n * Takes a CSV file string or stream and parsing options as input, and returns a promise that\\n * resolves to the parsed CSV data as an array of objects.\\n * Options for `parsingOptions` include:\\n * - `delimiter` {string/Buffer/[string/Buffer]} - Defines the character(s) used to delineate the fields inside a record. Default: `\\',\\'`\\n * - `quote` {string/Buffer/[string/Buffer]} - Defines the characters used to surround a field. Default: `\\'\"\\'`\\n * - `escape` {Buffer/string/null/boolean} - Set the escape character as one character/byte only. Default: `\"`\\n * - `columns` {boolean / array / function} - Generates record in the form of object literals. Default: `true`\\n * - `bom` {boolean} - Strips the {@link https://en.wikipedia.org/wiki/Byte_order_mark byte order mark (BOM)} from the input string or buffer. Default: `true`\\n * - `trim` {boolean} - Ignore whitespace characters immediately around the `delimiter`. Default: `true`\\n * - `ltrim` {boolean} - Ignore whitespace characters from the left side of a CSV field. Default: `true`\\n * - `rtrim` {boolean} - Ignore whitespace characters from the right side of a CSV field. Default: `true`\\n * - `chunkSize` {number} - The size of each chunk of CSV data. Default: `Infinity`\\n * - `skip_empty_lines` {boolean} - Ignore empty lines in the CSV file. Default: `true`\\n * @public\\n * @function\\n * @param {String | Stream} csvData - A CSV string or a readable stream\\n * @param {Object} [parsingOptions] - Optional. Parsing options for converting CSV to JSON.\\n * @param {function} [callback] - (Optional) callback function. If used it will be called state and an array of rows.\\n * @returns {Operation} The function returns a Promise that resolves to the result of parsing a CSV `stringOrStream`.\\n */\\nexport function parseCsv(csvData: string | Stream, parsingOptions?: any, callback?: Function): Operation;\\n/**\\n * Validate against a JSON schema. Any erors are written to an array at `state.validationErrors`.\\n * Schema can be passed directly, loaded as a JSON path from state, or loaded from a URL\\n * Data can be passed directly or loaded as a JSON path from state.\\n * By default, schema is loaded from `state.schema` and data from `state.data`.\\n * @pubic\\n * @function\\n * @param {string|object} schema - The schema, path or URL to validate against\\n * @param {string|object} data - The data or path to validate\\n * @example <caption>Validate `state.data` with `state.schema`</caption>\\n * validate()\\n * @example <caption>Validate form data at `state.form` with a schema from a URL</caption>\\n * validate(\"https://www.example.com/schema/record\", \"form\")\\n * @example <caption>Validate the each item in `state.records` with a schema from a URL</caption>\\n * each(\"records[*]\", validate(\"https://www.example.com/schema/record\"))\\n * @returns {Operation}\\n */\\nexport function validate(schema?: string | object, data?: string | object): Operation;\\n/**\\n * Sets a cursor property on state.\\n * Supports natural language dates like `now`, `today`, `yesterday`, `n hours ago`, `n days ago`, and `start`,\\n * which will be converted relative to the environment (ie, the Lightning or CLI locale). Custom timezones\\n * are not yet supported.\\n * You can provide a formatter to customise the final cursor value, which is useful for normalising\\n * different inputs. The custom formatter runs after natural language date conversion.\\n * See the usage guide at {@link https://docs.openfn.org/documentation/jobs/job-writing-guide#using-cursors}\\n * @public\\n * @function\\n * @example <caption>Use a cursor from state if present, or else use the default value</caption>\\n * cursor($.cursor, { defaultValue: \\'today\\' })\\n * @example <caption>Use a pagination cursor</caption>\\n * cursor(22)\\n * @param {any} value - the cursor value. Usually an ISO date, natural language date, or page number\\n * @param {object} options - options to control the cursor.\\n * @param {string} options.key - set the cursor key. Will persist through the whole run.\\n * @param {any} options.defaultValue - the value to use if value is falsy\\n * @param {Function} options.format - custom formatter for the final cursor value\\n * @returns {Operation}\\n */\\nexport function cursor(value: any, options?: {\\n    key: string;\\n    defaultValue: any;\\n    format: Function;\\n}): Operation;\\n/**\\n * Scopes an array of data based on a JSONPath.\\n * Useful when the source data has `n` items you would like to map to\\n * an operation.\\n * The operation will receive a slice of the data based of each item\\n * of the JSONPath provided.\\n * @public\\n * @function\\n * @example\\n * map(\"$.[*]\",\\n *   create(\"SObject\",\\n *     field(\"FirstName\", sourceValue(\"$.firstName\"))\\n *   )\\n * )\\n * @param {string} path - JSONPath referencing a point in `state.data`.\\n * @param {function} operation - The operation needed to be repeated.\\n * @param {State} state - Runtime state.\\n * @returns {State}\\n */\\nexport const map: any;\\n\\n\\n/**\\n * Scopes an array of data based on a JSONPath.\\n * Useful when the source data has `n` items you would like to map to\\n * an operation.\\n * The operation will receive a slice of the data based of each item\\n * of the JSONPath provided.\\n *\\n * It also ensures the results of an operation make their way back into\\n * the state\\'s references.\\n * @public\\n * @example\\n *  each(\"$.[*]\",\\n *    create(\"SObject\",\\n *    field(\"FirstName\", sourceValue(\"$.firstName\")))\\n *  )\\n * @function\\n * @param {DataSource} dataSource - JSONPath referencing a point in `state`.\\n * @param {Operation} operation - The operation needed to be repeated.\\n * @returns {Operation}\\n */\\nexport function each(dataSource: DataSource, operation: Operation): Operation;\\n\\n\\nexport { parse, format } from \"date-fns\";\\n\\n\\n/**\\n * Builder function to create request options. Returns an object with helpers to\\n * easily add commonly used options. The return object is chainable so you can set\\n * as many options as you want.\\n * Pass an object to set your own options.\\n * @param {CommonRequestOptions} options - options to pass to the request\\n * @returns {OptionsHelpers}\\n * @function\\n * @public\\n * @example <caption>Get with a query an oath token</caption>\\n * get($.data.url, http.options({ query: $.query }).oath($.configuration.access_token)\\n */\\nexport function options(opts?: {}): any;\\n/**\\n * Make a GET request.\\n * @public\\n * @function\\n * @example <caption>Request a resource</caption>\\n * http.get(\\'https://jsonplaceholder.typicode.com/todos\\')\\n * @example <caption>Request a resource with basic auth</caption>\\n * http.get(\\n *  \\'https://jsonplaceholder.typicode.com/todos\\',\\n *  http.options().basic(\\'user\\', \\'pass\\')\\n * )\\n * @example <caption>Request a resource with oauth</caption>\\n * http.get(\\n *  \\'https://jsonplaceholder.typicode.com/todos\\',\\n *  http.options().oauth($.configuration.access_token)\\n * )\\n * @param {string} url - URL to access\\n * @param {CommonRequestOptions} options - Request options\\n * @state {CommonHttpState}\\n * @returns {Operation}\\n */\\nexport function get(url: string, options: CommonRequestOptions): Operation;\\n/**\\n * Make a POST request.\\n * @public\\n * @function\\n * @example <caption>Post a JSON object (setting the content-type header)</caption>\\n *  http.post(\\n *    \\'https://jsonplaceholder.typicode.com/todos\\',\\n *    $.data,\\n *    options().json(),\\n *  })\\n * @param {string} url - URL to access\\n * @param {CommonRequestOptions} options - Request options\\n * @state {CommonHttpState}\\n * @returns {Operation}\\n */\\nexport function post(path: any, data: any, options: CommonRequestOptions): Operation;\\nexport { req as request };\\n/**\\n * Helper functions provided by `http.options`.\\n */\\nexport type OptionsHelpers = any;\\n/**\\n * Options provided to the HTTP request\\n */\\nexport type CommonRequestOptions = {\\n    /**\\n     * - Map of errorCodes -> error messages, ie, `{ 404: \\'Resource not found;\\' }`. Pass `false` to suppress errors.\\n     */\\n    errors: object;\\n    /**\\n     * - Pass a JSON object to be serialised into a multipart HTML form (as FormData) in the body.\\n     */\\n    form: object;\\n    /**\\n     * - An object of query parameters to be encoded into the URL.\\n     */\\n    query: object;\\n    /**\\n     * - An object of headers to append to the request.\\n     */\\n    headers: object;\\n    /**\\n     * - Parse the response body as json, text or stream. By default will use the response headers.\\n     */\\n    parseAs: string;\\n    /**\\n     * - Request timeout in ms. Default: 300 seconds.\\n     */\\n    timeout: number;\\n    /**\\n     * - TLS/SSL authentication options. See https://nodejs.org/api/tls.html#tlscreatesecurecontextoptions\\n     */\\n    tls: object;\\n};\\n/**\\n * State object\\n */\\nexport type CommonHttpState = any;\\n/**\\n * Options provided to the HTTP request\\n * @typedef {Object} CommonRequestOptions\\n * @property {object} errors - Map of errorCodes -> error messages, ie, `{ 404: \\'Resource not found;\\' }`. Pass `false` to suppress errors.\\n * @property {object} form - Pass a JSON object to be serialised into a multipart HTML form (as FormData) in the body.\\n * @property {object} query - An object of query parameters to be encoded into the URL.\\n * @property {object} headers - An object of headers to append to the request.\\n * @property {string} parseAs - Parse the response body as json, text or stream. By default will use the response headers.\\n * @property {number} timeout - Request timeout in ms. Default: 300 seconds.\\n * @property {object} tls - TLS/SSL authentication options. See https://nodejs.org/api/tls.html#tlscreatesecurecontextoptions\\n */\\n/**\\n * State object\\n * @typedef {Object} CommonHttpState\\n * @private\\n * @property data - the parsed response body\\n * @property response - the response from the HTTP server, including headers, statusCode, body, etc\\n * @property references - an array of all previous data objects used in the Job\\n **/\\n/**\\n * Make a HTTP request.\\n * @public\\n * @function\\n * @example\\n * http.request(\\n *   \\'GET\\',\\n *   \\'https://jsonplaceholder.typicode.com/todos\\'\\n * )\\n * @name request\\n * @param {string} method - The HTTP method to use.\\n * @param {string} url - URL to resource.\\n * @param {CommonRequestOptions} options - Request options\\n * @state {CommonHttpState}\\n * @returns {Operation}\\n */\\ndeclare function req(method: string, url: string, options: CommonRequestOptions): Operation;\\n\\n\\nimport * as Adaptor from \\'./Adaptor\\';\\nexport default Adaptor;\\nexport * from \\'./Adaptor\\';\\nexport * as beta from \\'./beta\\';\\nexport * as http from \\'./http\\';\\nexport * as dateFns from \\'./dateFns\\';\\nimport * as metadata from \\'./metadata\\';\\nexport { metadata };\\n\\n\\ndeclare type Entity = {\\n    name: string;\\n    type: string;\\n    label?: string;\\n    datatype?: string;\\n    desc?: string;\\n    children?: Entity[] | Record<string, Entity>;\\n    meta?: Record<string, any>;\\n    addChild: (e: Entity, name?: string) => void;\\n};\\ndeclare type DataType = \\'string\\' | \\'boolean\\' | \\'date\\';\\n\\n\\nexport function encode(data: string): string;\\nexport function decode(base64Data: string): string;\\nexport function uuid(): string;\\n\\n\\n/**\\n * `request` is a helper function that sends HTTP requests and returns the response\\n * body, headers, and status code.\\n * Use the error map to provide custom error messages or get hold of the response in case of errors.\\n * @param method - The HTTP method to use for the request (e.g., \"GET\", \"POST\", \"PUT\", \"DELETE\", etc.).\\n * @param fullUrlOrPath - The full or partial URL for the request.\\n * @param [options] - The `options` parameter is an object that contains additional configuration\\n * options for the request.\\n * @returns an object with the following properties:\\n * - method: the request method\\n * - url: the request url\\n * - code: the status code of the response\\n * - headers: the headers of the response\\n * - body: the body of the response\\n * - message: the status text of the response\\n * - duration: the response time\\n */\\nexport function request(method: any, fullUrlOrPath: any, options?: {}): Promise<{\\n    url: string;\\n    method: any;\\n    statusCode: any;\\n    statusMessage: string;\\n    headers: any;\\n    body: any;\\n    duration: number;\\n}>;\\nexport function makeBasicAuthHeader(username: any, password: any): {\\n    Authorization: string;\\n};\\nexport function logResponse(response: any): any;\\nexport function enableMockClient(baseUrl: any): import(\"undici/types/mock-interceptor\").Interceptable;\\nexport const ERROR_ABSOLUTE_URL: \"Absolute URLs not suppored\";\\nexport function assertRelativeUrl(path: any): void;\\nexport const ERROR_URL_MISMATCH: \"Target origin does not match baseUrl origin\";\\nexport function parseUrl(pathOrUrl: string, baseUrl: any): {\\n    url: string;\\n    baseUrl: string;\\n    path: string;\\n    query: any;\\n};\\nexport function get(url: any, options: any): Promise<{\\n    url: string;\\n    method: any;\\n    statusCode: any;\\n    statusMessage: string;\\n    headers: any;\\n    body: any;\\n    duration: number;\\n}>;\\nexport function post(url: any, body: any, options: any): Promise<{\\n    url: string;\\n    method: any;\\n    statusCode: any;\\n    statusMessage: string;\\n    headers: any;\\n    body: any;\\n    duration: number;\\n}>;\\nexport function put(url: any, body: any, options: any): Promise<{\\n    url: string;\\n    method: any;\\n    statusCode: any;\\n    statusMessage: string;\\n    headers: any;\\n    body: any;\\n    duration: number;\\n}>;\\nexport function del(url: any, body: any, options: any): Promise<{\\n    url: string;\\n    method: any;\\n    statusCode: any;\\n    statusMessage: string;\\n    headers: any;\\n    body: any;\\n    duration: number;\\n}>;\\n\\n\\nexport * from \"./http\";\\nexport * from \"./helpers\";\\nexport * from \"./references\";\\nimport parseDate from \"./parse-date\";\\nimport throwError from \"./throw-error\";\\nexport { parseDate, throwError };\\n\\n\\ndeclare function _default(d: any, startDate: any): any;\\nexport default _default;\\n\\n\\nexport function expandReferences(state: any, ...args: any[]): any[];\\nexport function normalizeOauthConfig(configuration: any): any;\\n\\n\\ndeclare function _default(code: any, { description, fix, ...extras }?: {\\n    description: any;\\n    fix: any;\\n}): never;\\nexport default _default;\\nTypescript definitions for doc @openfn/language-googlesheets/**\\n * Execute a sequence of oper.\\n * Wraps `language-common/execute`, and prepends initial state for http.\\n * @example\\n * execute(\\n *   create(\\'foo\\'),\\n *   delete(\\'bar\\')\\n * )(state)\\n * @private\\n * @param {Operations} operations - Operations to be performed.\\n * @returns {Operation}\\n */\\nexport function execute(...operations: Operations): Operation;\\n/**\\n * Add an array of rows to the spreadsheet.\\n * https://developers.google.com/sheets/api/samples/writing#append_values\\n * @public\\n * @example\\n * appendValues({\\n *   spreadsheetId: \\'1O-a4_RgPF_p8W3I6b5M9wobA3-CBW8hLClZfUik5sos\\',\\n *   range: \\'Sheet1!A1:E1\\',\\n *   values: [\\n *     [\\'From expression\\', \\'$15\\', \\'2\\', \\'3/15/2016\\'],\\n *     [\\'Really now!\\', \\'$100\\', \\'1\\', \\'3/20/2016\\'],\\n *   ],\\n * })\\n * @function\\n * @param {Object} params - Data object to add to the spreadsheet.\\n * @param {string} [params.spreadsheetId] The spreadsheet ID.\\n * @param {string} [params.range] The range of values to update.\\n * @param {array} [params.values] A 2d array of values to update.\\n * @param {function} callback - (Optional) Callback function\\n * @returns {Operation}\\n */\\nexport function appendValues(params: {\\n    spreadsheetId?: string;\\n    range?: string;\\n    values?: any[];\\n}, callback?: Function): Operation;\\n/**\\n * Batch update values in a Spreadsheet.\\n * @example\\n * batchUpdateValues({\\n *   spreadsheetId: \\'1O-a4_RgPF_p8W3I6b5M9wobA3-CBW8hLClZfUik5sos\\',\\n *   range: \\'Sheet1!A1:E1\\',\\n *   values: [\\n *     [\\'From expression\\', \\'$15\\', \\'2\\', \\'3/15/2016\\'],\\n *     [\\'Really now!\\', \\'$100\\', \\'1\\', \\'3/20/2016\\'],\\n *   ],\\n * })\\n * @function\\n * @public\\n * @param {Object} params - Data object to add to the spreadsheet.\\n * @param {string} [params.spreadsheetId] The spreadsheet ID.\\n * @param {string} [params.range] The range of values to update.\\n * @param {string} [params.valueInputOption] (Optional) Value update options. Defaults to \\'USER_ENTERED\\'\\n * @param {array} [params.values] A 2d array of values to update.\\n * @param {function} callback - (Optional) callback function\\n * @returns {Operation} spreadsheet information\\n */\\nexport function batchUpdateValues(params: {\\n    spreadsheetId?: string;\\n    range?: string;\\n    valueInputOption?: string;\\n    values?: any[];\\n}, callback?: Function): Operation;\\n/**\\n * Gets cell values from a Spreadsheet.\\n * @public\\n * @example\\n * getValues(\\'1O-a4_RgPF_p8W3I6b5M9wobA3-CBW8hLClZfUik5sos\\',\\'Sheet1!A1:E1\\')\\n * @function\\n * @param {string} spreadsheetId The spreadsheet ID.\\n * @param {string} range The sheet range.\\n * @param {function} callback - (Optional) callback function\\n * @returns {Operation} spreadsheet information\\n */\\nexport function getValues(spreadsheetId: string, range: string, callback?: Function): Operation;\\nexport { alterState, combine, cursor, dataPath, dataValue, each, field, fields, fn, fnIf, http, lastReferenceValue, merge, sourceValue } from \"@openfn/language-common\";\\n\\n\\nexport default Adaptor;\\nexport * from \"./Adaptor\";\\nimport * as Adaptor from \"./Adaptor\";\\n</adaptor>'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = Context(adaptor=\"@openfn/language-googlesheets@3.0.5\")\n",
    "adaptor_string = (\n",
    "    f\"<adaptor>The user is using the OpenFn {context.adaptor} adaptor. Use functions provided by its API.\"\n",
    ")\n",
    "\n",
    "adaptor_docs = apollo(\"describe_adaptor\", {\"adaptor\": context.adaptor})\n",
    "\n",
    "for doc in adaptor_docs:\n",
    "    adaptor_string += f\"Typescript definitions for doc {doc}\"\n",
    "    adaptor_string += adaptor_docs[doc][\"description\"]\n",
    "adaptor_string += \"</adaptor>\"\n",
    "adaptor_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_questions = [\n",
    "    \"How do I create a new workflow in OpenFn?\",\n",
    "    \"What's the difference between a Trigger and a Step in a workflow?\",\n",
    "    \"Can I merge branches or skip steps in my workflow?\",\n",
    "    \"How do I troubleshoot errors in my workflow?\",\n",
    "    \"What do the different status codes mean for a workflow run?\",\n",
    "    \"How can I optimize my workflow if it's timing out?\",\n",
    "    \"What should I include in my run logs to make troubleshooting easier?\",\n",
    "    \"How do I handle mapping errors between source and destination systems?\",\n",
    "    \"What does the 'DUPLICATE_VALUE' error mean and how do I resolve it?\",\n",
    "    \"Can I use OpenFn to edit source data before retrying a failed attempt?\",\n",
    "    \"How do I set up a request and approval process in my workflow?\",\n",
    "    \"What's the best way to handle data transformation in OpenFn jobs?\",\n",
    "    \"Can OpenFn auto-generate jobs based on Kobo forms?\",\n",
    "    \"How do I access the OpenFn platform to start creating workflows?\",\n",
    "    \"What programming knowledge do I need to write complex jobs in OpenFn?\",\n",
    "    \"How can I design error messages to be more user-friendly and actionable?\",\n",
    "    \"Is it possible to auto-correct minor issues in my workflow?\",\n",
    "    \"How do I manually run a workflow for testing and troubleshooting?\",\n",
    "    \"Does OpenFn support two-way syncing between applications?\",\n",
    "    \"Can I set up time-based triggers for my workflows, like running a job every Friday at 11 PM?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor_questions = [\n",
    "    \"Is there a way to handle pagination when pulling large datasets from an API? I'm running into memory issues using a standard approach.\",\n",
    "    \"When authenticating with API tokens instead of a username/password, what's the best way to configure authentication in an integration with DHIS2?\",\n",
    "    \"I need to update a Contact record in Salesforce. Can you provide a code snippet to update the FirstName and LastName based on data from a previous step?\",\n",
    "    \"How can I fetch submissions from a specific form within a certain date range in KoboToolbox? Can you provide an example?\",\n",
    "    \"I want to write data from a workflow to a Google Sheet. How do I specify the sheet ID and range? What permissions are needed for the service account?\",\n",
    "    \"Is it possible to send an SMS message with variables populated from data in my workflow? What's the correct syntax for handling this dynamically?\",\n",
    "    \"When querying a PostgreSQL database within an OpenFn job, how do I construct the SQL query? Can I use parameters to prevent SQL injection?\",\n",
    "    \"How does OpenMRS handle searching for patients based on multiple identifiers in an integration scenario?\",\n",
    "    \"I'm testing my CommCare integration. How can I verify that a case property has been updated correctly?\",\n",
    "    \"When pushing data to Aggregate in an ODK workflow, how can I confirm successful submission within the OpenFn logs?\",\n",
    "    \"My workflow involves transferring files via SFTP. What are the common error codes, and how should I handle them to ensure resilience?\",\n",
    "    \"I need to connect to an API that doesn’t have a specific adaptor available. How do I send a PATCH request with a custom header?\",\n",
    "    \"How can I trigger a RapidPro flow while passing in custom variables?\",\n",
    "    \"I want to load data into BigQuery. What's the recommended approach for handling large datasets and schema transformations?\",\n",
    "    \"I've upgraded our Dynamics 365 instance. Do I need to update anything in OpenFn to ensure compatibility? How do I check the version and update if necessary?\",\n",
    "    \"I'm running into authentication failures when integrating with Mailchimp\",\n",
    "    \"What are the security considerations when storing sensitive data in Azure Storage? How do I manage access keys securely?\",\n",
    "    \"What are the recommended best practices for ensuring data quality and consistency when syncing data with Primero?\",\n",
    "    \"How does OpenHIM work in conjunction with other integration tools in a complex workflow? Can you provide an example?\",\n",
    "    \"If a record fails to upload to OpenMRS due to a validation error, how can we configure the workflow to retry the operation a certain number of times before moving on?\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_questions = [\n",
    "    # \"How do I configure the HTTP adapter to handle OAuth 2.0 authentication for connecting to a REST API? Can you provide an example of setting up the headers and body for a POST request using the HTTP adapter?\",  \n",
    "    # \"What's the best way to handle upserts in Salesforce using the Salesforce adapter, and how do I manage external IDs effectively?\",  \n",
    "    # \"How can I use the KoboToolbox adapter to trigger a workflow every time a new submission is received on a specific form?\",  \n",
    "    # \"How do I configure the CommCare adapter to pull data based on a specific date range or case property?\",  \n",
    "    # \"Can you walk me through an example of using the DHIS2 adapter to push aggregated data to a specific data element and period?\",  \n",
    "    # \"How do I use the each() function within a job to process an array of data and create multiple records in the destination system?\",  \n",
    "    \"How can I implement custom error handling logic to retry failed operations with exponential backoff?\",  \n",
    "    \"What are some best practices for using the transform function to reshape data before sending it to the destination system?\",  \n",
    "    \"Can I set up a workflow to trigger based on changes in a database using a webhook? How?\",  \n",
    "    \"How can I securely manage API keys and other sensitive information using environment variables in OpenFn?\",  \n",
    "    \"Does OpenFn offer any built-in support for encrypting sensitive data in transit or at rest?\",  \n",
    "    \"How can I customize the logging level and output format for my workflows?\",  \n",
    "    \"What's the recommended workflow for using Git to manage and deploy changes to my OpenFn workflows?\",  \n",
    "    \"How can I write unit tests for my OpenFn jobs to ensure they are working correctly?\",  \n",
    "    \"How can I set up alerts to notify me when a workflow fails or exceeds a certain execution time?\",  \n",
    "    \"What's the most efficient way to process large datasets (e.g., millions of records) using OpenFn?\",  \n",
    "    \"Can I import and use custom JavaScript libraries in my OpenFn jobs? How?\",  \n",
    "    \"How do I handle nested JSON structures when mapping data between systems?\",  \n",
    "    \"How can I implement rate limiting in my workflow to avoid exceeding the API limits of the destination system?\",  \n",
    "    \"How can I use the OpenFn CLI to deploy changes to my workflows and manage my OpenFn instance?\"  \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_overview_questions = [\n",
    "    \"How do I configure OpenFn to listen for incoming SMS messages and trigger a workflow based on the message content?\",\n",
    "    \"Can you help me set up a job that automatically exports data from CommCare and imports it into a DHIS2 instance?\",\n",
    "    \"I need to create a workflow that synchronizes patient data between OpenMRS and a national health registry. What's the best approach?\",\n",
    "    \"How can I use OpenFn to transform data from a CSV file into FHIR resources and load them into a FHIR server?\",\n",
    "    \"I want to set up automated alerts based on data thresholds in my system. How can I achieve this with OpenFn?\",\n",
    "    \"We're using Salesforce and need to integrate it with our accounting system. Can you help us build a workflow to synchronize invoice data?\",\n",
    "    \"How can I use the OpenFn CLI to deploy and manage my workflows across multiple environments (dev, staging, production)?\",\n",
    "    \"I need to implement a data quality check within my workflow. How can I use JavaScript within OpenFn to validate data and handle errors?\",\n",
    "    \"How can I set up OpenFn to automatically retry failed tasks with exponential backoff?\",\n",
    "    \"I'm working with a complex data model in ActivityInfo. Can you help me write a job to efficiently extract and transform this data?\",\n",
    "    \"How do I configure OpenFn to use OAuth 2.0 for authentication with a REST API?\",\n",
    "    \"What are the best practices for handling Personally Identifiable Information (PII) within OpenFn workflows?\",\n",
    "    \"How can I use OpenFn to create a scheduled task that runs daily and generates reports based on data from multiple systems?\",\n",
    "    \"I need to integrate data from a legacy system that only supports XML. How can I transform XML data into JSON for use in my workflows?\",\n",
    "    \"Can you help me set up monitoring and logging for my OpenFn workflows?\",\n",
    "    \"How do I write a job to take data from a Google Sheet and use that data to create a new user in Keycloak?\",\n",
    "    \"How can I implement a workflow that handles data conflicts between two systems, using a defined conflict resolution strategy?\",\n",
    "    \"I need to trigger a workflow based on changes in a cloud storage bucket (e.g., AWS S3, Google Cloud Storage). How can I set this up?\",\n",
    "    \"Can you provide guidance on optimizing the performance of my OpenFn workflows, especially when dealing with large datasets?\",\n",
    "    \"How do I use OpenFn to implement a bulk data import process, breaking the data into smaller batches to avoid API rate limits?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_coding_questions = [\n",
    "    \"How do I use the each() function to iterate over an array of patients and create individual records in the destination system?\",\n",
    "    \"What's the correct syntax for using the $ operator to access nested data within the state object?\",\n",
    "    # \"How can I implement error handling using try/catch blocks within a job?\",\n",
    "    \"What's the proper way to use the dataValue() function to extract specific fields from the incoming data?\",\n",
    "    \"How do I use the get() function with the HTTP adapter to fetch data from an external API?\",\n",
    "    # \"Can you show me how to use string interpolation to dynamically construct API endpoints in a get() or post() operation?\",\n",
    "    # \"What's the syntax for using the map() function to transform an array of objects before sending them to the destination system?\",\n",
    "    \"How do I use the upsert() function with the Salesforce adapter to update or insert records based on an external ID?\",\n",
    "    \"What's the correct way to use the fn() function to execute custom JavaScript code within a job?\",\n",
    "    \"How can I use the post() function with the HTTP adapter to send transformed data to another system?\",\n",
    "    \"What's the syntax for using the create() function with the DHIS2 adapter to create new data elements?\",\n",
    "    \"How do I use the alterState() function to modify the state object between operations?\",\n",
    "    \"Can you show me how to use the field() function to map data from one schema to another?\",\n",
    "    \"What's the proper way to use the combine() function to merge data from multiple sources?\",\n",
    "    \"How do I use the sourceValue() function to reference values from the trigger source in my job?\",\n",
    "    \"What's the syntax for using the relationship() function to establish relationships between objects in Salesforce?\",\n",
    "    # \"How can I use the then() method to chain operations and handle asynchronous tasks?\",\n",
    "    \"What's the correct way to use the execute() function to run a custom SQL query in a database adapter?\",\n",
    "    \"How do I use the scrub() function to clean and standardize data before sending it to the destination system?\",\n",
    "    \"Can you show me how to use the chunk() function to process large datasets in smaller batches to avoid memory issues?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_coding_questions = [\n",
    "    \"How do I use a JavaScript Promise to handle asynchronous API calls in a workflow?\",\n",
    "    \"What's the best way to parse a complex JSON response from an external API in JavaScript?\",\n",
    "    \"How can I use JavaScript's reduce() function to aggregate data from multiple API responses?\",\n",
    "    \"Is there a way to implement retry logic for failed API requests using JavaScript?\",\n",
    "    \"How do I use JavaScript's destructuring assignment to extract specific fields from an API response?\",\n",
    "    \"What's the most efficient way to filter an array of objects based on multiple criteria in JavaScript?\",\n",
    "    \"How can I use JavaScript's map() function to transform data structures?\",\n",
    "    \"Is it possible to use JavaScript's async/await syntax for better readability of asynchronous code?\",\n",
    "    \"How do I implement error handling for database operations in JavaScript?\",\n",
    "    \"What's the best approach to dynamically construct a SQL query string in JavaScript based on input parameters?\",\n",
    "    \"How can I use JavaScript's Date object to format dates according to a specific timezone?\",\n",
    "    \"Is there a way to use JavaScript's Set object to remove duplicates from an array of objects?\",\n",
    "    \"How do I use JavaScript's fetch API to make HTTP requests to external services?\",\n",
    "    \"What's the most efficient way to merge data from multiple API responses using JavaScript objects?\",\n",
    "    \"How can I use JavaScript's regular expressions to validate and clean input data?\",\n",
    "    \"In PostgreSQL, how do I write a query to update multiple rows with different values in a single statement?\",\n",
    "    \"What's the best way to handle NULL values in a PostgreSQL query when joining tables?\",\n",
    "    \"How do I write a SQL query to pivot rows into columns dynamically based on the data received?\",\n",
    "    \"In PostgreSQL, how can I use the JSONB data type to store and query complex JSON structures?\",\n",
    "    \"What's the most efficient way to bulk insert a large number of records into a PostgreSQL database from a JavaScript array?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_test_set(adaptor_questions, platform_questions, specific_questions, \n",
    "                            workflow_overview_questions, specific_coding_questions, general_coding_questions):    \n",
    "        all_queries = []\n",
    "        all_types = []\n",
    "        \n",
    "        # Add each category to the lists\n",
    "        for questions, q_type in [\n",
    "            (adaptor_questions, 'adaptor_questions'),\n",
    "            (platform_questions, 'platform_questions'),\n",
    "            (specific_questions, 'specific_questions'),\n",
    "            (workflow_overview_questions, 'workflow_overview_questions'),\n",
    "            (specific_coding_questions, 'specific_coding_questions'),\n",
    "            (general_coding_questions, 'general_coding_questions')\n",
    "        ]:\n",
    "            all_queries.extend(questions)\n",
    "            all_types.extend([q_type] * len(questions))\n",
    "        \n",
    "        # Create the base dataframe\n",
    "        df = pd.DataFrame({\n",
    "            'question': all_queries,\n",
    "            'question_type': all_types\n",
    "        })\n",
    "        \n",
    "        # Add the label columns according to the specified rules\n",
    "        df['consult_doc_label'] = ~df['question_type'].isin(['general_coding_questions'])\n",
    "        df['consult_adaptor_docs_label'] = df['question_type'].isin(['adaptor_questions', 'specific_coding_questions'])\n",
    "        df['consult_general_docs_label'] = df['question_type'].isin(['platform_questions', \n",
    "                                                            'specific_questions', \n",
    "                                                            'workflow_overview_questions', 'specific_coding_questions'])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>consult_doc_label</th>\n",
       "      <th>consult_adaptor_docs_label</th>\n",
       "      <th>consult_general_docs_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there a way to handle pagination when pulli...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When authenticating with API tokens instead of...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to update a Contact record in Salesforc...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I fetch submissions from a specific fo...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to write data from a workflow to a Goog...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>In PostgreSQL, how do I write a query to updat...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>What's the best way to handle NULL values in a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>How do I write a SQL query to pivot rows into ...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In PostgreSQL, how can I use the JSONB data ty...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What's the most efficient way to bulk insert a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Is there a way to handle pagination when pulli...   \n",
       "1    When authenticating with API tokens instead of...   \n",
       "2    I need to update a Contact record in Salesforc...   \n",
       "3    How can I fetch submissions from a specific fo...   \n",
       "4    I want to write data from a workflow to a Goog...   \n",
       "..                                                 ...   \n",
       "105  In PostgreSQL, how do I write a query to updat...   \n",
       "106  What's the best way to handle NULL values in a...   \n",
       "107  How do I write a SQL query to pivot rows into ...   \n",
       "108  In PostgreSQL, how can I use the JSONB data ty...   \n",
       "109  What's the most efficient way to bulk insert a...   \n",
       "\n",
       "                question_type  consult_doc_label  consult_adaptor_docs_label  \\\n",
       "0           adaptor_questions               True                        True   \n",
       "1           adaptor_questions               True                        True   \n",
       "2           adaptor_questions               True                        True   \n",
       "3           adaptor_questions               True                        True   \n",
       "4           adaptor_questions               True                        True   \n",
       "..                        ...                ...                         ...   \n",
       "105  general_coding_questions              False                       False   \n",
       "106  general_coding_questions              False                       False   \n",
       "107  general_coding_questions              False                       False   \n",
       "108  general_coding_questions              False                       False   \n",
       "109  general_coding_questions              False                       False   \n",
       "\n",
       "     consult_general_docs_label  \n",
       "0                         False  \n",
       "1                         False  \n",
       "2                         False  \n",
       "3                         False  \n",
       "4                         False  \n",
       "..                          ...  \n",
       "105                       False  \n",
       "106                       False  \n",
       "107                       False  \n",
       "108                       False  \n",
       "109                       False  \n",
       "\n",
       "[110 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = create_test_set(adaptor_questions, platform_questions, specific_questions, \n",
    "                            workflow_overview_questions, specific_coding_questions, general_coding_questions)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needs_docs_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "needs_docs(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "needs_docs(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you show me how to use the chunk() function to process large datasets in smaller batches to avoid memory issues?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query': 'chunk function batch processing', 'doc_type': None}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Is there a way to handle pagination when pulling large datasets from an API? I'm running into memory issues using a standard approach.\",\n",
       " \"When authenticating with API tokens instead of a username/password, what's the best way to configure authentication in an integration with DHIS2?\",\n",
       " 'I need to update a Contact record in Salesforce. Can you provide a code snippet to update the FirstName and LastName based on data from a previous step?',\n",
       " 'How can I fetch submissions from a specific form within a certain date range in KoboToolbox? Can you provide an example?',\n",
       " 'I want to write data from a workflow to a Google Sheet. How do I specify the sheet ID and range? What permissions are needed for the service account?']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.question.to_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(text=\"If you are a super user configuring a new Google OAuth\\nclient for your OpenFn deployment, then please refer to Google's documentation\\nto\\n[configure an OAuth Client ID](https://developers.google.com/identity/protocols/oauth2/javascript-implicit-flow),\\nas well as see the standard[openid-configuration](https://accounts.google.com/.well-known/openid-configuration)\\nfor how to complete the OAuth Client Setup form on OpenFn.\\n\\n:::#### Permissions (Scopes)\\n\\nPermissions and access in an OAuth instance are defined by scopes which are\\nnamed differently by providers based on their functions within their platform.\\n\\nFor GoogleSheets, your super user will likely need to add the following scopes\\nto the Oauth Client Setup configured for Google in your OpenFn deployment. That\\nsaid, please refer to\\n[Google's documentation on Oauth scopes](https://developers.google.com/identity/protocols/oauth2/scopes)\\nfor the latest information.\\n\\n- `openid`\\n- `email`\\n- `profile`\\n- `https://www.googleapis.com/auth/spreadsheets`\", metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='ng data from CRM systems into Google Sheets for sales\\nanalysis. - Updating inventory levels in a Google Sheets spreadsheet from an\\ne-commerce platform. - Automatically populating project management data from\\ntask tracking systems into Google Sheets for reporting purposes.\\n\\nA step by step guide is found\\n[in this tutorial](https://docs.openfn.org/documentation/tutorials/http-to-googlesheets)\\nthat shows us how to get data via a REST API and push it to Google Sheet.### Helpful Resources\\n\\n- [Google Sheets API Overview](https://developers.google.com/sheets/api/guides/concepts)\\n- OpenFn Workflow Tutorial:\\n  [HTTP-to-GoogleSheets](https://docs.openfn.org/documentation/tutorials/http-to-googlesheets)', metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='- `openid`\\n- `email`\\n- `profile`\\n- `https://www.googleapis.com/auth/spreadsheets`### Integration Options\\n\\nThere are a couple of primary ways to integrate with this app, each catering to\\ndifferent use cases.#### 1. Pushing Data to OpenFn via Google App Scripts\\n\\nWith this method, data from Google Forms or Google Sheets is automatically\\npushed to an OpenFn webhook trigger workflow whenever new entries are made. This\\nreal-time approach ensures that your data processing pipelines are continuously\\nfed with the latest information.\\n\\n**Use Cases:** - Monitoring survey responses in real-time. - Triggering\\nimmediate actions based on form submissions. - Enabling timely data\\nsynchronization between Google Forms/Sheets and external systems.\\n\\nTo push data to OpenFn in real-time we will have to configure a _trigger_ on a\\nGoogle App Script as demonstrated in the example below.\\n![image](https://github.com/OpenFn/docs/assets/167166847/4680c12b-ad57-497e-9073-37e287624f42)', metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"---\\ntitle: Google Forms/Google Sheets\\n---## Google Sheets Adaptor Overview\\n\\nGoogle Sheets adaptor provides seamless integration between Google Forms, Google\\nSheets, and the OpenFn platform, enabling robust data flow management.### Authentication and Authorization\\n\\nThis adaptor requires OAuth authorization to connect with Google Sheets. This\\nauthorization can be achieved by a user or organization admin consenting to an\\nOAuth client to access resources. Users can request authorization via the\\ndefault OpenFn Google OAuth client or choose to\\n`Add new (generic) OAuth client(s)` for their account and projects.\\n\\nTo authorize Google Sheets for your OpenFn workflows, read our documentation on\\n[using OAuth credentials](/documentation/build/credentials#use-oauth2-credentials).\\n\\n:::info Google Oauth Client Setup Tips for Super Users\\n\\nSetting up your own generic OAuth client requires that you have an OAuth\\napplication set up. If you are a super user configuring a new Google OAuth\\nclient for your OpenFn deployment, then please refer to Google's documentation\\nto\\n[configure an OAuth Client ID](https://developers.google.com/identity/protocols/oauth2/javascript-implicit-flow),\\nas well as see the standard\", metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=':::#### Permissions (Scopes)\\n\\nPermissions and access in an OAuth instance are defined by scopes which are\\nnamed differently by providers based on their functions within their platform.\\nFor MS Graph, there are two types of scopes: Delegated permission and\\nApplication Scopes. Delegated permissions are used by applications that sign in\\na user and act on behalf of the signed-in user. These permissions require the\\nuser to consent and are subject to the same data access restrictions as the user\\nwithin the organization. Application scopes, on the other hand, are used by\\napplications that run without a signed-in user. They require admin consent and\\nprovide access to all data for an organization, typically used by background\\nservices or daemons. OpenFn workflows and generic OAuth setup are compatible\\nwith both the delegated and application scopes. Below is a list of commonly used\\nOAuth MS Scopes for your reference. **The scopes required will depend on your', metadata={'doc_title': 'msgraph', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text='sana-configuration-schema) for more on the required authentication parameters.\\n\\nSee platform docs on [managing credentials](/documentation/manage-projects/manage-credentials) for how to configure a credential in OpenFn. If working locally or if using a Raw JSON credential type, then your configuration will look something like this:```\\n{\\n  \"apiVersion\": \"1.0\",\\n  \"token\": \"sample-tokenyWSJdXBACMLLWMNGgADFA\"\\n}\\n```### Helpful Links\\n\\n1. [API documentation](https://developers.asana.com/docs/overview)### Implementation Examples\\n\\n1. The Wildlife Conservation Society (WCS) - KoboToolBox -> GoogleSheets -> Asana sync: [https://openfn.github.io/ConSoSci/asana/](https://openfn.github.io/ConSoSci/asana/)', metadata={'doc_title': 'asana', 'docs_type': 'adaptor_docs'}, score=None),\n",
       " SearchResult(text=\"[image](https://github.com/OpenFn/docs/assets/167166847/4680c12b-ad57-497e-9073-37e287624f42)```js\\nfunction sendToOpenFn(data) {\\n  var payload = JSON.stringify(data);\\n  var url = 'https://app.openfn.org/i/your-workorder-id';\\n  var options = {\\n    method: 'post',\\n    contentType: 'application/json',\\n    payload: payload,\\n  };\\n  var response = UrlFetchApp.fetch(url, options);\\n  Logger.log(response);\\n}\\n\\nfunction onFormSubmit(e) {\\n  var headers = Object.keys(e.namedValues); // Get column headers from form responses\\n\\n  // Create the survey form entry object dynamically\\n  var surveyFormEntry = { formId: 'surveyForm', data: {} };\\n\\n  // Iterate over each row of form responses\\n  for (var i = 1; i  0) {\\n      surveyFormEntry.data = formData;\\n      // Send the survey form entry to OpenFn and log the workorder id back on Google sheet\\n      sendToOpenFn(surveyFormEntry); // Sending data to OpenFn and getting the response\\n    }\\n  }\\n}\\n```#### 2. Pulling Data from Google Sheets using OpenFn\\n\\nAlternatively, you can pull data from Google Sheets at specific intervals or\\non-demand using a `cron` workflow in OpenFn, allowing for more controlled data\\nretrieval processes. This method is particularly useful when you need to fetch\\nhistorical data or perform periodic data updates.\\n\\nCheck out the docs on available [functions](./packages/googlesheets-docs) to\\nlearn how to configure a workflow step to use this OpenFn adaptor to\\n`getValues()` and fetch data from a target Google Sheet.\\n\\n**Use Cases:** - Aggregating data for periodic reporting or analysis. -\\nImplementing batch processing for efficiency and resource optimization.\", metadata={'doc_title': 'googlesheets', 'docs_type': 'adaptor_docs'}, score=None)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docsite_search.search(q.get(\"query\"), top_k=7, docs_type=\"adaptor_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is some relevant T/F?\n",
    "\n",
    "evaluate_retrieved_docs_system_prompt = \"\"\"\n",
    "You are an assistant for evaluating the relevance of code documentation snippets retrieved for a solutions engineer working for a workflow automation platform.\n",
    "\n",
    "You will receive a user question, and the retrieved documentation snippets. Answer whether any of the snippets contain information\n",
    "that will help the solutions engineer address the user question.\n",
    "\n",
    "Answer with True or False, followed by a short 2-3 sentence explanation on a new line.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "evaluate_retrieved_docs_user_prompt = \"\"\"\n",
    "The user question: \"{user_question}\" \n",
    "\n",
    "The retrieved docs: {retrieved_docs}\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_retrieved_docs(user_question, retrieved_docs):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\", # TODO change to cheaper model\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=evaluate_retrieved_docs_system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": evaluate_retrieved_docs_user_prompt.format(user_question=user_question, retrieved_docs=retrieved_docs)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_retrieved_docs(user_question, retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "0.6454545454545455\n",
      "0.6777777777777778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>consult_doc_label</th>\n",
       "      <th>consult_adaptor_docs_label</th>\n",
       "      <th>consult_general_docs_label</th>\n",
       "      <th>search_results_answer_basic_rag_top5</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>evaluations_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there a way to handle pagination when pulli...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text=\"Best practice is to includ...</td>\n",
       "      <td>True\\n\\nThe documentation shows multiple appro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When authenticating with API tokens instead of...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text='What data formats are     ...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to update a Contact record in Salesforc...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text='&gt; The source code for comb...</td>\n",
       "      <td>True\\n\\nThe documentation contains a relevant ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I fetch submissions from a specific fo...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text=\"Check out:\\n\\n- Kobo API d...</td>\n",
       "      <td>True\\n\\nThe documentation snippets show how to...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to write data from a workflow to a Goog...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text=\"---\\nsidebar_label: HTTP t...</td>\n",
       "      <td>False\\n\\nThe provided documentation snippets d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>In PostgreSQL, how do I write a query to updat...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text=\"[add_credential_postgres](...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>What's the best way to handle NULL values in a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text='---\\ntitle: PostgreSQL\\n--...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>How do I write a SQL query to pivot rows into ...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text=\"Then, when building your w...</td>\n",
       "      <td>False\\n\\nThe provided documentation snippets d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In PostgreSQL, how can I use the JSONB data ty...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text='---\\ntitle: PostgreSQL\\n--...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What's the most efficient way to bulk insert a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[SearchResult(text=')### Sample Job Expression...</td>\n",
       "      <td>False\\n\\nThe provided documentation snippets s...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Is there a way to handle pagination when pulli...   \n",
       "1    When authenticating with API tokens instead of...   \n",
       "2    I need to update a Contact record in Salesforc...   \n",
       "3    How can I fetch submissions from a specific fo...   \n",
       "4    I want to write data from a workflow to a Goog...   \n",
       "..                                                 ...   \n",
       "105  In PostgreSQL, how do I write a query to updat...   \n",
       "106  What's the best way to handle NULL values in a...   \n",
       "107  How do I write a SQL query to pivot rows into ...   \n",
       "108  In PostgreSQL, how can I use the JSONB data ty...   \n",
       "109  What's the most efficient way to bulk insert a...   \n",
       "\n",
       "                question_type  consult_doc_label  consult_adaptor_docs_label  \\\n",
       "0           adaptor_questions               True                        True   \n",
       "1           adaptor_questions               True                        True   \n",
       "2           adaptor_questions               True                        True   \n",
       "3           adaptor_questions               True                        True   \n",
       "4           adaptor_questions               True                        True   \n",
       "..                        ...                ...                         ...   \n",
       "105  general_coding_questions              False                       False   \n",
       "106  general_coding_questions              False                       False   \n",
       "107  general_coding_questions              False                       False   \n",
       "108  general_coding_questions              False                       False   \n",
       "109  general_coding_questions              False                       False   \n",
       "\n",
       "     consult_general_docs_label  \\\n",
       "0                         False   \n",
       "1                         False   \n",
       "2                         False   \n",
       "3                         False   \n",
       "4                         False   \n",
       "..                          ...   \n",
       "105                       False   \n",
       "106                       False   \n",
       "107                       False   \n",
       "108                       False   \n",
       "109                       False   \n",
       "\n",
       "                  search_results_answer_basic_rag_top5  \\\n",
       "0    [SearchResult(text=\"Best practice is to includ...   \n",
       "1    [SearchResult(text='What data formats are     ...   \n",
       "2    [SearchResult(text='> The source code for comb...   \n",
       "3    [SearchResult(text=\"Check out:\\n\\n- Kobo API d...   \n",
       "4    [SearchResult(text=\"---\\nsidebar_label: HTTP t...   \n",
       "..                                                 ...   \n",
       "105  [SearchResult(text=\"[add_credential_postgres](...   \n",
       "106  [SearchResult(text='---\\ntitle: PostgreSQL\\n--...   \n",
       "107  [SearchResult(text=\"Then, when building your w...   \n",
       "108  [SearchResult(text='---\\ntitle: PostgreSQL\\n--...   \n",
       "109  [SearchResult(text=')### Sample Job Expression...   \n",
       "\n",
       "                                            evaluation  evaluations_bin  \n",
       "0    True\\n\\nThe documentation shows multiple appro...             True  \n",
       "1    False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "2    True\\n\\nThe documentation contains a relevant ...             True  \n",
       "3    True\\n\\nThe documentation snippets show how to...             True  \n",
       "4    False\\n\\nThe provided documentation snippets d...            False  \n",
       "..                                                 ...              ...  \n",
       "105  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "106  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "107  False\\n\\nThe provided documentation snippets d...            False  \n",
       "108  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "109  False\\n\\nThe provided documentation snippets s...            False  \n",
       "\n",
       "[110 rows x 8 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate direct basic rag\n",
    "\n",
    "out_f = \"results_basic_rag_top5.csv\"\n",
    "result_col_name = \"search_results_answer_basic_rag_top5\"\n",
    "test_df_results_basic_rag = test_df.copy()\n",
    "# test_df_results_basic_rag = test_df_results_basic_rag[:2] \n",
    "\n",
    "retrieved_docs = []\n",
    "\n",
    "for user_question in test_df.question.to_list(): #[:2]\n",
    "    results = docsite_search.search(user_question, top_k=5, docs_type=None)\n",
    "    retrieved_docs.append(results)\n",
    "test_df_results_basic_rag[result_col_name] = retrieved_docs\n",
    "\n",
    "test_df_results_basic_rag.to_csv(out_f, index=False)\n",
    "\n",
    "evaluations = []\n",
    "evaluations_bin = []\n",
    "for q, d in zip(test_df_results_basic_rag.question.to_list(), test_df_results_basic_rag[result_col_name].to_list()):\n",
    "    evaluation = evaluate_retrieved_docs(q, d)\n",
    "    evaluations.append(evaluation)\n",
    "    if evaluation.startswith(\"True\"):\n",
    "        evaluations_bin.append(True)\n",
    "    elif evaluation.startswith(\"False\"):\n",
    "        evaluations_bin.append(False)\n",
    "    else:\n",
    "        evaluations_bin.append(None)\n",
    "test_df_results_basic_rag[\"evaluation\"] = evaluations\n",
    "test_df_results_basic_rag[\"evaluations_bin\"] = evaluations_bin\n",
    "print(test_df_results_basic_rag['evaluations_bin'].eq(True).mean())# avg useful \n",
    "print(test_df_results_basic_rag[test_df_results_basic_rag[\"question_type\"]!=\"general_coding_questions\"]['evaluations_bin'].eq(True).mean()) # avg useful but ignore genral coding\n",
    "test_df_results_basic_rag.to_csv(out_f, index=False)\n",
    "test_df_results_basic_rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'pagination memory large dataset', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'batch processing chunks', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'DHIS2', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'api token authentication', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'salesforce', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'update contact salesforce', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'kobotoolbox', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'date range filter submissions', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'google_sheets', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'service account permissions google sheets', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'twilio', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'variable substitution template', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'postgresql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'sql query parameters injection', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'openmrs', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'patient search multiple identifiers', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'commcare', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'verify case property', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'odk', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'logs success submission confirmation', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'sftp', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'error handling retry resilience', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'http adaptor patch request headers', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'http', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'rapidpro', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'trigger flow variables', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'bigquery', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'schema transformation large dataset', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'microsoft-dynamics', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'version compatibility upgrade', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'mailchimp', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'authentication failure mailchimp', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'azure storage security considerations', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'azure', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'Primero', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'data quality best practices sync', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'OpenHIM', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'workflow integration OpenHIM example', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'OpenMRS', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'retry error handling workflow', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'create new workflow', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'trigger vs step workflow difference', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'merge branch skip step workflow', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'troubleshoot error workflow', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'workflow status codes', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'timeout optimization workflow', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'workflow performance tuning', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'logging troubleshooting best practices', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'run logs debug', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'mapping error handling', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'error handling transform', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'DUPLICATE_VALUE error', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'edit source data retry', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'retry failed job', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'approval process workflow', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'approval', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'data transformation', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'language-common', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'kobo', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'auto generate job kobo form', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'login access platform credentials', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'programming knowledge requirements prerequisites', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'error message customization', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'error handling best practices', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'auto-correct workflow fix repair', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'manually run workflow test troubleshoot', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'two-way sync bidirectional synchronization', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'schedule trigger cron', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'schedule', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'error handling retry exponential backoff', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'transform function best practices', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'data transformation examples', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'webhook trigger database', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'webhook', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'environment variables credentials secrets', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'encryption security sensitive data', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'vault', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'logging level format customize', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'git workflow version control', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'github', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'unit test job testing', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'alerts notifications workflow failure execution time', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'batch processing large datasets performance', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'bulk operations', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'custom javascript libraries import require', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'external libraries dependencies', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'nested json mapping transform', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'json path expression', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'rate limiting throttling', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'CLI deploy manage workflows', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'sms trigger workflow', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'twilio', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'commcare', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'dhis2', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'job configuration export import', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'OpenMRS', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'workflow synchronization data', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'csv', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'fhir', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'alerts notifications triggers threshold', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'inbox', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'message', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'salesforce', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'invoice workflow sync', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'CLI environment management deploy workflow', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript validation error handling', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'data quality check workflow', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'retry exponential backoff', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'ActivityInfo', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'data transformation', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'oauth2 authentication rest', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'http', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'PII data security best practices', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'data protection privacy', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'schedule cron job recurring', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'job multiple sources', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'xml json transform', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'xml', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'monitoring logging metrics', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'google sheets', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'keycloak', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'conflict resolution workflow', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'data synchronization error handling', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 's3', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'gcs', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'trigger workflow storage bucket', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'performance optimization large data', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'batch operations bulk data', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'bulk data import batch', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'rate limit', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'each function array iteration', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': '$ operator state nested data', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'dataValue function extract fields', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'HTTP', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'get function HTTP request', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'salesforce', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'upsert external id', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'fn() function javascript custom code', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'HTTP', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'post function transform data', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'dhis2', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'create function data elements', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'alterState function modify state', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'field() function mapping schema', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'combine function merge data', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'sourceValue function', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'salesforce', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'relationship function', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'execute sql query', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'database', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'scrub function data cleaning standardization', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'chunk function batch processing', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript promise async', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'parse json javascript', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript reduce aggregate', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'retry logic javascript', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript destructuring', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript array filter multiple conditions', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript map transform', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'async await javascript', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'error handling database javascript', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'database', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'sql query string javascript dynamic', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'sql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript date timezone format', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript set object array deduplicate', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'http fetch javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'http', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'merge data javascript', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'javascript regex validation', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'javascript', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'postgresql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'postgresql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'null values join tables', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'sql pivot dynamic columns', 'doc_type': 'general_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'sql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'postgresql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'jsonb postgresql json', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "{'query': 'postgresql', 'doc_type': 'adaptor_docs'}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "{'query': 'bulk insert batch records', 'doc_type': None}\n",
      "INFO:DocsiteSearch:Metadata filters built\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:DocsiteSearch:Similar documents retreived: 5\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "0.6727272727272727\n",
      "0.7333333333333333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>consult_doc_label</th>\n",
       "      <th>consult_adaptor_docs_label</th>\n",
       "      <th>consult_general_docs_label</th>\n",
       "      <th>consult_doc_answer_v1</th>\n",
       "      <th>consult_adaptor_docs_answer_v1</th>\n",
       "      <th>consult_general_docs_answer_v1</th>\n",
       "      <th>search_query_answer_v1</th>\n",
       "      <th>search_results_answer_v1</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>evaluations_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there a way to handle pagination when pulli...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'pagination memory large dataset', ...</td>\n",
       "      <td>[[SearchResult(text='This typically won\\'t hap...</td>\n",
       "      <td>True\\n\\nThe documentation contains relevant in...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When authenticating with API tokens instead of...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'DHIS2', 'doc_type': 'adaptor_docs'...</td>\n",
       "      <td>[[SearchResult(text=\"# DHIS2## App Overview\\n\\...</td>\n",
       "      <td>False\\n\\nWhile the retrieved documentation con...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to update a Contact record in Salesforc...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'salesforce', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: Salesforce\\n-...</td>\n",
       "      <td>True\\n\\nThe documentation includes a code snip...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I fetch submissions from a specific fo...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'kobotoolbox', 'doc_type': 'adaptor...</td>\n",
       "      <td>[[SearchResult(text=\"---\\ntitle: Kobo Toolbox\\...</td>\n",
       "      <td>True\\n\\nThe retrieved documentation shows how ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to write data from a workflow to a Goog...</td>\n",
       "      <td>adaptor_questions</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'google_sheets', 'doc_type': 'adapt...</td>\n",
       "      <td>[[SearchResult(text='ng data from CRM systems ...</td>\n",
       "      <td>False\\n\\nWhile the documentation snippets cont...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>In PostgreSQL, how do I write a query to updat...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>What's the best way to handle NULL values in a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>True\\n\\nThe retrieved documentation contains r...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>How do I write a SQL query to pivot rows into ...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'sql pivot dynamic columns', 'doc_t...</td>\n",
       "      <td>[[SearchResult(text=\"For example, imagine that...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In PostgreSQL, how can I use the JSONB data ty...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nThe retrieved documentation snippets ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What's the most efficient way to bulk insert a...</td>\n",
       "      <td>general_coding_questions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'query': 'postgresql', 'doc_type': 'adaptor_...</td>\n",
       "      <td>[[SearchResult(text='---\\ntitle: PostgreSQL\\n-...</td>\n",
       "      <td>False\\n\\nWhile the documentation shows how to ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Is there a way to handle pagination when pulli...   \n",
       "1    When authenticating with API tokens instead of...   \n",
       "2    I need to update a Contact record in Salesforc...   \n",
       "3    How can I fetch submissions from a specific fo...   \n",
       "4    I want to write data from a workflow to a Goog...   \n",
       "..                                                 ...   \n",
       "105  In PostgreSQL, how do I write a query to updat...   \n",
       "106  What's the best way to handle NULL values in a...   \n",
       "107  How do I write a SQL query to pivot rows into ...   \n",
       "108  In PostgreSQL, how can I use the JSONB data ty...   \n",
       "109  What's the most efficient way to bulk insert a...   \n",
       "\n",
       "                question_type  consult_doc_label  consult_adaptor_docs_label  \\\n",
       "0           adaptor_questions               True                        True   \n",
       "1           adaptor_questions               True                        True   \n",
       "2           adaptor_questions               True                        True   \n",
       "3           adaptor_questions               True                        True   \n",
       "4           adaptor_questions               True                        True   \n",
       "..                        ...                ...                         ...   \n",
       "105  general_coding_questions              False                       False   \n",
       "106  general_coding_questions              False                       False   \n",
       "107  general_coding_questions              False                       False   \n",
       "108  general_coding_questions              False                       False   \n",
       "109  general_coding_questions              False                       False   \n",
       "\n",
       "     consult_general_docs_label consult_doc_answer_v1  \\\n",
       "0                         False                 False   \n",
       "1                         False                  True   \n",
       "2                         False                  True   \n",
       "3                         False                  True   \n",
       "4                         False                  True   \n",
       "..                          ...                   ...   \n",
       "105                       False                 False   \n",
       "106                       False                 False   \n",
       "107                       False                 False   \n",
       "108                       False                 False   \n",
       "109                       False                 False   \n",
       "\n",
       "     consult_adaptor_docs_answer_v1  consult_general_docs_answer_v1  \\\n",
       "0                             False                           False   \n",
       "1                             False                           False   \n",
       "2                             False                           False   \n",
       "3                             False                           False   \n",
       "4                             False                           False   \n",
       "..                              ...                             ...   \n",
       "105                           False                           False   \n",
       "106                           False                           False   \n",
       "107                           False                           False   \n",
       "108                           False                           False   \n",
       "109                           False                           False   \n",
       "\n",
       "                                search_query_answer_v1  \\\n",
       "0    [{'query': 'pagination memory large dataset', ...   \n",
       "1    [{'query': 'DHIS2', 'doc_type': 'adaptor_docs'...   \n",
       "2    [{'query': 'salesforce', 'doc_type': 'adaptor_...   \n",
       "3    [{'query': 'kobotoolbox', 'doc_type': 'adaptor...   \n",
       "4    [{'query': 'google_sheets', 'doc_type': 'adapt...   \n",
       "..                                                 ...   \n",
       "105  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "106  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "107  [{'query': 'sql pivot dynamic columns', 'doc_t...   \n",
       "108  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "109  [{'query': 'postgresql', 'doc_type': 'adaptor_...   \n",
       "\n",
       "                              search_results_answer_v1  \\\n",
       "0    [[SearchResult(text='This typically won\\'t hap...   \n",
       "1    [[SearchResult(text=\"# DHIS2## App Overview\\n\\...   \n",
       "2    [[SearchResult(text='---\\ntitle: Salesforce\\n-...   \n",
       "3    [[SearchResult(text=\"---\\ntitle: Kobo Toolbox\\...   \n",
       "4    [[SearchResult(text='ng data from CRM systems ...   \n",
       "..                                                 ...   \n",
       "105  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "106  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "107  [[SearchResult(text=\"For example, imagine that...   \n",
       "108  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "109  [[SearchResult(text='---\\ntitle: PostgreSQL\\n-...   \n",
       "\n",
       "                                            evaluation  evaluations_bin  \n",
       "0    True\\n\\nThe documentation contains relevant in...             True  \n",
       "1    False\\n\\nWhile the retrieved documentation con...            False  \n",
       "2    True\\n\\nThe documentation includes a code snip...             True  \n",
       "3    True\\n\\nThe retrieved documentation shows how ...             True  \n",
       "4    False\\n\\nWhile the documentation snippets cont...            False  \n",
       "..                                                 ...              ...  \n",
       "105  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "106  True\\n\\nThe retrieved documentation contains r...             True  \n",
       "107  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "108  False\\n\\nThe retrieved documentation snippets ...            False  \n",
       "109  False\\n\\nWhile the documentation shows how to ...            False  \n",
       "\n",
       "[110 rows x 12 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate LLM query reformulation + search with optional adaptor/general docs filter. + optionally evaluate needs rag step effect (do this by discounting the answers with a FALSE answer in needs rag to get the second result)\n",
    "out_f = \"results_v1_top5.csv\"\n",
    "\n",
    "needs_docs_answers = []\n",
    "consult_adaptor_docs = []\n",
    "consult_general_docs = []\n",
    "\n",
    "search_docs_answers = []\n",
    "search_results = []\n",
    "\n",
    "for user_question in test_df.question.to_list(): #:5\n",
    "    needs_docs_answer = needs_docs(user_question)\n",
    "    search_docs_answer = search_docs(user_question)\n",
    "\n",
    "    needs_docs_answers.append(needs_docs_answer)\n",
    "    search_docs_answers.append(search_docs_answer)\n",
    "\n",
    "    has_adaptor_docs = False\n",
    "    has_general_docs = False\n",
    "    \n",
    "    # Check each document in the results\n",
    "    for answer in search_docs_answer:\n",
    "        doc_type = answer.get(\"doc_type\")\n",
    "        \n",
    "        if doc_type == \"adaptor_documentation\":\n",
    "            has_adaptor_docs = True\n",
    "        elif doc_type == \"general_documentation\":\n",
    "            has_general_docs = True\n",
    "            \n",
    "    consult_adaptor_docs.append(has_adaptor_docs)\n",
    "    consult_general_docs.append(has_general_docs)\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for q in search_docs_answer:\n",
    "        print(q)\n",
    "        result = docsite_search.search(q.get(\"query\"), top_k=5, docs_type=q.get(\"doc_type\"))\n",
    "        retrieved_docs.append(result)\n",
    "    search_results.append(retrieved_docs)\n",
    "\n",
    "test_df_results = test_df.copy()\n",
    "# test_df_results = test_df_results[:5]\n",
    "test_df_results[\"consult_doc_answer_v1\"] = needs_docs_answers\n",
    "test_df_results[\"consult_adaptor_docs_answer_v1\"] = consult_adaptor_docs\n",
    "test_df_results[\"consult_general_docs_answer_v1\"] = consult_general_docs\n",
    "test_df_results[\"search_query_answer_v1\"] = search_docs_answers\n",
    "test_df_results[\"search_results_answer_v1\"] = search_results\n",
    "\n",
    "test_df_results.to_csv(out_f, index=False)\n",
    "\n",
    "# evaluate\n",
    "evaluations = []\n",
    "evaluations_bin = []\n",
    "for q, d in zip(test_df_results.question.to_list(), test_df_results.search_results_answer_v1.to_list()):\n",
    "    evaluation = evaluate_retrieved_docs(q, d)\n",
    "    evaluations.append(evaluation)\n",
    "    if evaluation.startswith(\"True\"):\n",
    "        evaluations_bin.append(True)\n",
    "    elif evaluation.startswith(\"False\"):\n",
    "        evaluations_bin.append(False)\n",
    "    else:\n",
    "        evaluations_bin.append(None)\n",
    "test_df_results[\"evaluation\"] = evaluations\n",
    "test_df_results[\"evaluations_bin\"] = evaluations_bin\n",
    "print(test_df_results['evaluations_bin'].eq(True).mean()) # avg useful\n",
    "print(test_df_results[test_df_results[\"question_type\"]!=\"general_coding_questions\"]['evaluations_bin'].eq(True).mean()) # avg useful but ignore genral coding\n",
    "test_df_results.to_csv(out_f, index=False)\n",
    "test_df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_results[test_df_results[\"question_type\"]!=\"general_coding_questions\"]['evaluations_bin'].eq(True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
